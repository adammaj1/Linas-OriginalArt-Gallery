#LyX 1.3 created this file. For more info see http://www.lyx.org/
\lyxformat 221
\textclass amsart
\language english
\inputencoding auto
\fontscheme pslatex
\graphics default
\paperfontsize default
\spacing single 
\papersize Default
\paperpackage a4
\use_geometry 0
\use_amsmath 1
\use_natbib 0
\use_numerical_citations 0
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\quotes_times 2
\papercolumns 1
\papersides 1
\paperpagestyle default

\layout Title

The Bernoulli Operator
\layout Author

Linas Vepstas <linas@linas.org>
\layout Date

2 January 2004 (revised 16 October 2006)
\layout Abstract

This paper reviews a raft of related ideas surrounding the Bernoulli operator.
 The Bernoulli operator is the transfer operator or Frobenius-Perron operator
 of the Bernoulli map.
 The Bernoulli map is a simple map of the unit interval onto itself, which
 has the effect of discarding the leading binary digit of the binary expansion
 of a number upon every iteration.
 This map has been well studied in the literature, and much, if not most,
 of what is presented here is well-known.
 
\layout Abstract

If there is anything new here, then perhaps it is a representation of the
 Bernoulli map eigenfunctions in terms of the Takagi curve (or Blancmange
 curve).
 In particular, it is shown that the Hurwitz-zeta function basis for the
 continuous spectrum is just a linear combination of the Takagi curves,
 and vice-versa.
 I find this curious and important somehow: the Blancmange curve has an
 explicitly fractal self-similarity given by the dyadic monoid.
 The dyadic monoid is the monoid that describes the self-similarity of the
 infinite binary tree, and is a subset of the modular group 
\begin_inset Formula $SL(2,\mathbb{Z})$
\end_inset 

.
\layout Abstract

This paper is part of a set of chapters that explore the relationship between
 the real numbers, the modular group, and fractals.
\layout Section

The Bernoulli operator
\layout Standard

THIS IS A SET OF WORKING NOTES.
 Its somewhat loosely structured, sometimes messy, and occasionally assumes
 familiarity with the authors other writings and/or a general knowledge
 of dynamical systems.
 The intro hasn't been written yet.
\layout Standard

The general layout is:
\layout Standard

-- Define the Bernoulli operator, demonstrate some of its eigenfunctions
 and eigenvalues.The presentation is a simplified variant of the material
 in 
\begin_inset LatexCommand \cite{Dri99}

\end_inset 

.
 
\layout Standard

-- Present the Hurwitz Zeta eigenfunctions
\layout Standard

-- Present the topological zeta function
\layout Standard

The Bernoulli map and Bernoulli operator appear in many forms and guises
 throughout the theory of dynamical systems.
 In order to give a proper and complete treatment of the subject, the topics
 listed below should be presented/reviewed/understood.
 
\layout Itemize

The Bernoulli process in probability theory is one of the simplest Markov
 processes.
 Understood as a Markov process, it has a number of generalizations.
 
\layout Itemize

Discuss entropy.
\layout Itemize

Discuss symbolic dynamics in two letters.
 Mention one-dimensional tiling, mention Fibonacci tiling.
 Mention Lindenmeyer systems.
 Mention subshifts of finite type.
 Mention how subshifts are solved.
 
\layout Itemize

Discuss connection to the Cantor set.
 viz, all strings in two letters.
\layout Itemize

Discuss free groups, discuss Cayley tree, discuss group presentation, and
 connect it up to this.
\layout Itemize

Discuss ergodicity, the ergodic theorem, and how its related to this.
 Discuss how the Bernoulli map is a 
\begin_inset Quotes eld
\end_inset 

bad
\begin_inset Quotes erd
\end_inset 

 example of ergodicity.
 Hypothesis: are all transfer operators that are triangular (have a polynomial
 basis) equivalent to 
\begin_inset Quotes eld
\end_inset 

bad
\begin_inset Quotes erd
\end_inset 

 ergodic sequences? Or are there good (uniformly converging) ergodic sequences
 that can result from triangular transfer operators?
\layout Itemize

Discuss wavelet transforms, point out their dyadic nature, point out their
 relationship to this mess; point out how wavelet transforms are a good
 tool for working with this kind of dyadic data.
\layout Itemize

While this paper focuses primarily on the polynomial and square-integrable
 eigenfunctions of the Bernoulli operator, there is also a class of non-differen
tiable, non-integrable eigenfunctions that can be precisely defined in terms
 of the shift operator on a one-dimensional lattice model, the Ising model
 of statistical mechanics.
 The Ising model has a natural topology that is distinctly different from
 the topology of the real number line; this alternate topology allows for
 a much, much larger set of eigenfunctions.
 This is discussed in greater detail in 
\begin_inset LatexCommand \cite{Ve-I04}

\end_inset 

.
\layout Standard

Some of the sections, including the section on orthogonality and completeness,
 are awkwardly presented.
 The topic is subtle, the notation is not elegant.
 In a certain sense, this paper illustrates all the wrong ways in which
 to present the notions of completeness with regards to a Hilbert space.
 The notions of what is fractal, and what is differentiable, and what is
 an operator, and what is an eigenvalue, is muddied as a result.
 The journey, however faulty, is still educational, though.
 To do: Riesz representation thm.
\layout Section

Introduction
\layout Standard

The method of the transfer operator was introduced by David Ruelle[need
 ref] as a powerful mechanism for studying the nature of iterated maps.
 The transfer operator, sometimes called the Frobenius-Perron operator,
 or the Ruelle-Frobenius-Perron operator, provides a means to escape the
 narrow confines of point-set topology when considering an iterated function,
 and instead explore the function using wildly different topologies.
 In its most concrete form, it is a linear operator acting on a Banach space
 of functions.
 However, the structure and the properties of the operator depend very much
 on which space of functions one considers.
 In the following, the spaces of polynomial functions of the real numbers,
 as well as the space of square-integrable functions will be considered.
 More broadly, one may also consider the transfer operator acting on other
 spaces, such as the 
\begin_inset Formula $p$
\end_inset 

-adic numbers, endowed with unusual topologies: in particular, with the
 so called 
\begin_inset Quotes eld
\end_inset 

product topology
\begin_inset Quotes erd
\end_inset 

 whose basis are the cylinder sets.
 In this topology, the transfer operator can be made to resemble the lattice
 models of theoretical physics, such as the Ising model, thus offering additiona
l means of gaining insight.
\begin_inset LatexCommand \cite{May91}

\end_inset 

 
\layout Standard

It is easiest to begin with the concrete definition.
 Consider a function 
\begin_inset Formula $g:[0,1]\to[0,1]$
\end_inset 

, that is, a function mapping the unit interval of the real number line
 to itself.
 Upon iteration, the function may have fixed points or orbits of points.
 These orbits may be attractors or repellors, or may be neutral saddle points.
 The action of 
\begin_inset Formula $g$
\end_inset 

 may be ergodic or chaotic, strong-mixing or merely topologically mixing.
 In any case, the language used to discuss 
\begin_inset Formula $g$
\end_inset 

 is inherently based on either the point-set topology of the unit interval,
 or the 
\begin_inset Quotes eld
\end_inset 

natural
\begin_inset Quotes erd
\end_inset 

 topology on the unit interval, the topology of open sets.
 
\layout Standard

A shift in perspective may be gained not by considering how 
\begin_inset Formula $g$
\end_inset 

 acts on points or open sets, but instead by considering how 
\begin_inset Formula $g$
\end_inset 

 acts on distributions on the unit interval.
 Intuitively, one might consider a dusting of points on the unit interval,
 with a local density given by 
\begin_inset Formula $\rho(x)$
\end_inset 

 at point 
\begin_inset Formula $x\in[0,1]$
\end_inset 

, and then consider how this dusting or density evolves upon iteration by
 
\begin_inset Formula $g$
\end_inset 

.
 This verbal description may be given form as 
\begin_inset Formula \begin{equation}
\rho^{\prime}(x)=\int_{0}^{1}\,\delta\left(x-g(y)\right)\rho(y)\; dy\label{eq:transfer-dirac}\end{equation}

\end_inset 

 where 
\begin_inset Formula $\rho^{\prime}(x)$
\end_inset 

 is the new density at point 
\begin_inset Formula $x$
\end_inset 

 and 
\begin_inset Formula $\delta$
\end_inset 

 is the Dirac delta function.
 
\layout Standard

In this viewpoint, 
\begin_inset Formula $g$
\end_inset 

 becomes an operator that maps densities 
\begin_inset Formula $\rho$
\end_inset 

 to other densities 
\begin_inset Formula $\rho^{\prime}$
\end_inset 

, or notationally, 
\begin_inset Formula \begin{equation}
\mathcal{L}_{g}\rho=\rho^{\prime}\label{eq:}\end{equation}

\end_inset 

 The operator 
\begin_inset Formula $\mathcal{L}_{g}$
\end_inset 

 is called the transfer operator or the Ruelle-Frobenius-Perron operator.
 It is not hard to see that it is a linear operator, in that 
\begin_inset Formula \begin{equation}
\mathcal{L}_{g}(a\rho_{1}+b\rho_{2})=a\mathcal{L}_{g}\rho_{1}+b\mathcal{L}_{g}\rho_{2}\label{eq:}\end{equation}

\end_inset 

 for constants 
\begin_inset Formula $a,b$
\end_inset 

 and densities 
\begin_inset Formula $\rho_{1},\rho_{2}$
\end_inset 

.
 
\layout Standard

When the function 
\begin_inset Formula $g$
\end_inset 

 is differentiable, and doesn't have a vanishing derivative, the integral
 formulation of the transfer operator above can be rephrased in a more convenien
t form, as 
\begin_inset Formula \begin{equation}
\left[\mathcal{L}_{g}\rho\right]\left(x\right)=\sum_{y:x=g(y)}\frac{\rho(y)}{\left|dg(y)/dy\right|}\label{eq: transfer-jacobi}\end{equation}

\end_inset 

 where the sum is presumed to extend over at most a countable number of
 points.
 If these conditions do not hold, a transfer operator can still be defined,
 although more care must be taken in its definition.
\layout Standard

One generalization should be immediately apparent: although the word 
\begin_inset Quotes eld
\end_inset 

density
\begin_inset Quotes erd
\end_inset 

 implies that 
\begin_inset Formula $\rho$
\end_inset 

 is a smooth map from the unit interval to the non-negative reals, no such
 requirement need to be enforced: 
\begin_inset Formula $\rho$
\end_inset 

 may be a map from the unit interval to any ring 
\begin_inset Formula $R$
\end_inset 

, and it need not be smooth, differentiable or even continuous.
 This generalization gives a very rich structure to 
\begin_inset Formula $\mathcal{L}_{g}$
\end_inset 

: the precise form of 
\begin_inset Formula $\mathcal{L}_{g}$
\end_inset 

 will take will depend very strongly on 
\begin_inset Formula $R$
\end_inset 

, whether its the reals 
\begin_inset Formula $\mathbb{R}$
\end_inset 

, the complex numbers 
\begin_inset Formula $\mathbb{C}$
\end_inset 

, or some other field or ring.
 It will also depend strongly on whether one restricts oneself to smooth
 functions, continuous functions, square-integrable functions, or some other
 function space.
 An adequate study requires reference to the specific topology that the
 function space is endowed with; many different topologies may be considered.
 That is, in general, one must consider 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

 to be an operator acting on a topological space endowed with multiplication
 and addition, that is, a topological ring.
 
\layout Standard

The structure of 
\begin_inset Formula $\mathcal{L}_{g}$
\end_inset 

 also depends on the topology applied to the unit interval.
 Besides the natural topology on the real number line, the unit interval
 can be given several other topologies.
 The most important of these is the Cantor set topology, or the 
\begin_inset Formula $p$
\end_inset 

-adic topology.
 Here, one considers the unit interval 
\begin_inset Formula $[0,1]$
\end_inset 

 to consist of the set of strings 
\begin_inset Formula \begin{equation}
\Omega=\left\{ \sigma=(\sigma_{0},\sigma_{1},\sigma_{2},\ldots)\,:\,\sigma_{k}\in\left\{ 0,1,\ldots,p-1\right\} \,,\, x=\sum_{k=0}^{\infty}\sigma_{k}p^{-(k+1)}\;,\, x\in[0,1]\right\} \label{eq:}\end{equation}

\end_inset 

 Intuitively, this set is simply the set of all the digits of a base-
\begin_inset Formula $p$
\end_inset 

 expansion of the real numbers 
\begin_inset Formula $x\in[0,1]$
\end_inset 

.
 The connection with physics comes from the realization that this set can
 be understood to be the collection of all field configurations of a one-dimensi
onal, one-sided lattice, where each lattice location can take on one of
 
\begin_inset Formula $p$
\end_inset 

 values.
 Such lattices are commonly given the product topology, where the open sets
 are the cylinder sets consisting of substrings of sequences of letters.
 The topology also has a natural measure, derived from the length of letter
 sequences.
 Aside from the 
\begin_inset Formula $p$
\end_inset 

-adic expansion above, one also has the continued fraction expansion, where
 one considers the sequence of integers making up the continued fraction
 
\begin_inset Formula \begin{equation}
x=[0;\sigma_{1},\sigma_{2},\sigma_{3},\ldots]=\frac{1}{\sigma_{1}+\frac{1}{\sigma_{2}+\frac{1}{\sigma_{3}+\ldots}}}\label{eq:}\end{equation}

\end_inset 

 where each 
\begin_inset Formula $\sigma_{k}$
\end_inset 

 is a positive integer; the entire sequence again be given a product topology,
 although the measure is constructed differently.
\layout Standard

In most of what follows, the topological rings that will be considered will
 be very concrete: these will be the Banach spaces of polynomial functions,
 and of square-integrable functions.
 Some exploration of the lattice-model topology is explored in 
\begin_inset LatexCommand \cite{Ve-L06}

\end_inset 

.
 Since 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

 is a linear operator, the primary focus of study is to characterize it
 along traditional lines: find its representations, find the eigenvectors
 and eigenspaces associated with each representation, discuss any symmetries
 and pertinent isomorphisms these spaces might have.
 
\layout Standard

The eigenfunctions of 
\begin_inset Formula $\mathcal{L}_{g}$
\end_inset 

 are called Ruelle resonances [need citation], elaborate.
\layout Section

The Transfer Operator of the Bernoulli Map
\layout Standard

The Bernoulli map is an exactly solvable example of deterministic chaos.
 A presentation of this map, its associated transfer operator and its solution
 in terms of polynomial eigenfunctions is given by Driebe
\begin_inset LatexCommand \cite{Dri99}

\end_inset 

.
 This and the next section recaps those results using a simplified development
 and simpler tools.
 The simplified development enables the discussion of more complex scenarios,
 given in later sections.
 
\layout Standard

The Bernoulli map is given by 
\begin_inset Formula \begin{equation}
b(x)=2x-\left\lfloor 2x\right\rfloor \label{eq:}\end{equation}

\end_inset 

 where 
\begin_inset Formula $\left\lfloor x\right\rfloor $
\end_inset 

 denotes the integer part of 
\begin_inset Formula $x$
\end_inset 

.
 The map can be intuitively thought of as popping the leading digit off
 of the binary or 2-adic expansion of 
\begin_inset Formula $x.$
\end_inset 

 This map has a positive Lyapunov exponent and is highly chaotic, as, in
 a certain sense, one can say that the digits of the binary expansion of
 some 'arbitrary' number are unpredictable, and that the orbits of two close-by
 numbers will eventually become 'uncorellated' (after suitably defining
 what we mean by 'arbitrary' and 'unpredictable').
 Closely related is the 
\begin_inset Formula $p$
\end_inset 

-adic map, given by 
\begin_inset Formula \begin{equation}
a(x)=px-\left\lfloor px\right\rfloor \label{eq:}\end{equation}

\end_inset 

 for 
\begin_inset Formula $p$
\end_inset 

 an integer.
 As above, this map has the effect of popping off the leading digit of the
 base-
\begin_inset Formula $p$
\end_inset 

 expansion of 
\begin_inset Formula $x$
\end_inset 

.
 In the same vein, one may also consider the Gauss map 
\begin_inset Formula \begin{equation}
g(x)=\frac{1}{x}-\left\lfloor \frac{1}{x}\right\rfloor \label{eq:}\end{equation}

\end_inset 

 which has the effect of lopping of the leading digit of the continued fraction
 expansion of 
\begin_inset Formula $x$
\end_inset 

.
 What these three maps have in common is that they deal with different (and,
 in a certain sense, inequivalent) representations of the continuum of real
 numbers.
 Each map is chaotic, but in a different way.
\layout Standard

The Ruelle-Frobenius-Perron operator or transfer operator of the Bernoulli
 map is given by
\begin_inset Formula \begin{equation}
\left[\mathcal{L}_{B}f\right](x)=\frac{1}{2}\left[f\left(\frac{x}{2}\right)+f\left(\frac{x+1}{2}\right)\right]\label{eq:Bernoulli operator}\end{equation}

\end_inset 

 which follows directly from equation 
\begin_inset LatexCommand \ref{eq: transfer-jacobi}

\end_inset 

.
 Similarly, the transfer operator for the general 
\begin_inset Formula $p$
\end_inset 

-adic map is 
\begin_inset Formula \begin{equation}
\left[\mathcal{L}_{p}f\right](x)=\frac{1}{p}\sum_{k=0}^{p-1}f\left(\frac{x+k}{p}\right)\label{eq:}\end{equation}

\end_inset 

 while that for the Gauss map is 
\begin_inset Formula \begin{equation}
\left[\mathcal{L}_{G}f\right](x)=\sum_{k=1}^{\infty}\;\frac{1}{(x+k)^{2}}\; f\left(\frac{1}{1+x}\right)\label{eq:}\end{equation}

\end_inset 

 This last is known as the Gauss-Kuzmin-Wirsing operator [give ref].
 It is not studied further here, although it has bearing on some results,
 due to its relationship to the representation of the real numbers.
 
\layout Standard

As indicated in the introduction a critical issue and a point of confusion
 is that the operator 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

 is not well-defined without also specifying the function space on which
 it acts, and without also specifying the topology to be used on the unit
 interval.
 In particular, the spectrum of eigenvalues and eigenvectors for 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

 can vary from being discrete, to being continuous, depending on the the
 function space and the topology.
 The simplest case assumes the natural topology of the reals on the unit
 interval, and takes as the function space the set of orthogonal polynomials
 on the unit interval.
 In this case, the eigenfunctions may be shown to be the Bernoulli polynomials
 
\begin_inset Formula $B_{n}(x)$
\end_inset 

, associated with the eigenvalues 
\begin_inset Formula $2^{-n}$
\end_inset 

.
 That is, one finds that 
\begin_inset Formula \begin{equation}
\left[\mathcal{L}_{B}B_{n}\right](x)=\frac{1}{2^{n}}B_{n}(x)\label{eq:}\end{equation}

\end_inset 

where the first few 
\begin_inset Formula $B_{n}(x)$
\end_inset 

 are 
\begin_inset Formula \begin{eqnarray}
B_{0}(x) & = & 1\nonumber \\
B_{1}(x) & = & x-\frac{1`}{2}\nonumber \\
B_{2}(x) & = & x^{2}-x+\frac{1}{6}\nonumber \\
B_{3}(x) & = & x^{3}-\frac{3x^{2}}{2}+\frac{x}{2}\label{eq:}\end{eqnarray}

\end_inset 

 and so on.
 Perhaps the easiest proof that these are the eigenfunctions may be obtained
 by considering the generating function for the Bernoulli polynomials: 
\begin_inset Formula \begin{equation}
G(x,t)=\frac{te^{xt}}{e^{t}-1}=\sum_{n=0}^{\infty}B_{n}(x)\,\frac{t^{n}}{n!}\label{eq:}\end{equation}

\end_inset 

 It is then straight-forward to verify that 
\begin_inset Formula \begin{equation}
\left[\mathcal{L}_{B}G\right](x,t)=\frac{1}{2}\left[G\left(\frac{x}{2},t\right)+G\left(\frac{x+1}{2},t\right)\right]=G\left(x,\frac{t}{2}\right)\label{eq:}\end{equation}

\end_inset 

 or equivalently, by applying the linearity of 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

, that
\begin_inset Formula \begin{equation}
\left[\mathcal{L}_{B}G\right](x,t)=\sum_{n=0}^{\infty}\frac{t^{n}}{n!}\left[\mathcal{L}_{B}B_{n}\right](x)=\sum_{n=0}^{\infty}\left(\frac{t}{2}\right)^{n}\frac{B_{n}(x)}{n!}\label{eq:}\end{equation}

\end_inset 

 and then equating the coefficients of the powers of 
\begin_inset Formula $t$
\end_inset 

.
 This derivation follows for the general 
\begin_inset Formula $p$
\end_inset 

-adic case:
\layout Theorem

The eigenvalues of the 
\begin_inset Formula $p$
\end_inset 

-adic transfer operator are the Bernoulli polynomials, and are associated
 with the eigenvalues 
\begin_inset Formula $p^{-n}$
\end_inset 

.
 That is, one has
\begin_inset Formula \begin{equation}
\left[\mathcal{L}_{p}B_{n}\right](x)=\frac{1}{p^{n}}B_{n}(x)\label{eq:}\end{equation}

\end_inset 

 
\layout Proof

The proof proceeds as in the above 2-adic case, and hinges on the well-known
 factorization 
\begin_inset Formula \[
w^{p}-1=\left(w-1\right)\left(w^{p-1}+w^{p-2}+\cdots+1\right)\]

\end_inset 

 Here, take 
\begin_inset Formula $w^{p}-1=e^{t}-1$
\end_inset 

, so that 
\begin_inset Formula \begin{eqnarray*}
\left[\mathcal{L}_{p}G\right](x,t) & = & \frac{1}{p}\sum_{k=0}^{p-1}G\left(\frac{x+k}{p},t\right)\\
 & = & \frac{1}{p}\frac{t}{e^{t}-1}\,\sum_{k=0}^{p-1}\exp\left(\frac{(x+k)t}{p}\right)\\
 & = & \frac{t}{p}\;\frac{\left(1+e^{t/p}+e^{2t/p}+\cdots+e^{(p-1)t/p}\right)}{e^{t}-1}\\
 & = & G\left(x,\frac{t}{p}\right)\end{eqnarray*}

\end_inset 

 Then, equating coefficients of powers of 
\begin_inset Formula $t$
\end_inset 

 of the generating function, one obtains the desired result.
 
\layout Standard

The above theorem is a fancy restatement of an old and well-known result
 on the Bernoulli polynomials, namely, the so-called 
\begin_inset Quotes eld
\end_inset 

multiplication theorem
\begin_inset Quotes erd
\end_inset 

 given by Joseph Ludwig Raabe in 1851.
 This is usually given in the much more prosaic form of 
\begin_inset Formula \begin{equation}
B_{n}(px)=p^{n-1}\sum_{k=0}^{p-1}B_{n}\left(x+\frac{k}{p}\right)\label{eq:}\end{equation}

\end_inset 

 but amounts to the same thing.
 What the language of the transfer operator provides is an abstraction that
 allows the multiplication theorem to be examined in a broader fashion.
 In particular, multiplication theorems exist not only for the Bernoulli
 polynomials, but more broadly, including the Gamma function and the Hurwitz
 zeta function.
 These will be re-discovered in later sections.
 
\layout Standard

The following two sections provide an alternate and more abstract and labored
 derivation of the above result.
 The goal of the abstraction is to develop the machinery needed to explore
 the Bernoulli operator in more general topological settings.
 Rather than starting with the Hilbert space of orthogonal polynomials on
 the unit interval, the next section defines a Banach space on monomials,
 and its dual.
 The subsequent section then exposes the matrix elements of the Bernoulli
 operator in this space.
\layout Section

The Polynomial Representation 
\layout Standard

The polynomial eigenvectors of the Bernoulli operator can be derived in
 several ways.
 One seemingly natural approach is to start with a Hilbert space of orthogonal
 polynomials on the unit interval.
 This approach is taken by Driebe
\begin_inset LatexCommand \cite{Dri99}

\end_inset 

, who starts with the Legendre polynomials, rescaled to the unit interval,
 and obtains the resulting Bernoulli polynomials.
 This allows for a derivation of the right eigenvectors, but is then discovered
 to generate difficulties when considering the left eigenvectors.
 This arises because the the Bernoulli operator is not invertible in this
 Hilbert space: it is quite singular, in that 
\begin_inset Formula $\left[\mathcal{L}_{B}f\right]=0$
\end_inset 

 whenever 
\begin_inset Formula $f$
\end_inset 

 is skew about 1/2, that is, whenever 
\begin_inset Formula $f(y)=-f(1/2+y)$
\end_inset 

.
 A set of left eigenvectors can be found, but these are not polynomials
 or even ordinary real-valued functions; rather, they are generalized functions,
 expressed as derivatives of the Dirac delta function.
 From this exercise, one concludes that the starting assumption of envisioning
 the transfer operator acting on a Hilbert space does not provide any particular
 benefit or insight, and thus can be dispensed with.
 
\layout Standard

Generalized functions can be loosely defined as linear functionals from
 the space of functions to the reals.
 In this sense, generalized functions belong to the dual space of a function
 space.
 Given that the left eigenvectors of the Bernoulli operator can be shown
 to belong to such a dual space, it then makes sense to start in this way.
 Thus, for the following, an infinite-dimensional vector space will be construct
ed, with the basis elements being the monomials.
 The generalized functions appear naturally as elements of the dual space.
 The transfer operator then has a direct representation in this space.
 In principle, this infinite-dimensional vector space may be taken to be
 a Banach space; however, the development below does not make any particular
 use of the norm that Banach spaces are equipped with.
 
\layout Standard

This is perhaps a critical point, and deserves being belabored.
 In this section, the vocabulary for discussing an infinite-dimensional
 vector space will be developed.
 The sums appearing herein range formally over the countable infinity.
 However, the language of this section avoids questions of the convergence
 of these sums; the question of convergence can be deferred to later sections,
 when one is manipulating actual operators, and the sums take a concrete
 form.
 Thus, for the development of this section, a discussion of the vector norm,
 a discussion of a metric on the vector space, and a discussion of the topology
 of the vector space can be completely avoided.
 The manipulations are completely algebraic in nature.
 Curiously, this will continue to be the case when these algebraic manipulations
 are applied to the Bernoulli operator.
 That is, the resulting concrete sums will turn out to range over only a
 finite number of non-zero terms, and so the question of convergence will
 not come up.
 As a result, the foundations do not require a notion of norm, metric or
 topology on this vector space.
 When the exceptions to this rule crop up, they will be discussed explicitly.
\layout Standard

XXX the above is false, as there are plenty of places below where it is
 assumed that an infinite number of elts are non-zero.
 Again, need to refine the algebraic vs.
 topological discussion to handle these subtleties XXX.
\layout Standard

Consider an infinite-dimensional vector space 
\begin_inset Formula $V$
\end_inset 

 with a countable ordered set of linearly independent basis vectors 
\begin_inset Formula $e_{k}$
\end_inset 

 labelled by the natural numbers 
\begin_inset Formula $k$
\end_inset 

.
 A general element 
\begin_inset Formula $v\in V$
\end_inset 

 may be written as 
\begin_inset Formula $v=\sum_{k=0}^{\infty}a_{k}e_{k}$
\end_inset 

.
 The dual space 
\begin_inset Formula $V^{*}$
\end_inset 

 is the set of all linear functionals 
\begin_inset Formula $L:V\to\mathbb{R}$
\end_inset 

.
 General elements of the dual space may be written as linear combinations
 of the basis elements 
\begin_inset Formula $e_{k}^{*}$
\end_inset 

, which are the maps such that 
\begin_inset Formula $e_{j}^{*}(e_{k})=\delta_{jk}$
\end_inset 

.
 
\layout Standard

For the space 
\begin_inset Formula $\mathcal{P}$
\end_inset 

 of real-analytic functions on the unit interval, the basis elements may
 be taken to be the monomials 
\begin_inset Formula $e_{k}=x^{k}$
\end_inset 

, so that a general real-analytic function 
\begin_inset Formula $f\in\mathcal{P}$
\end_inset 

 is written as 
\begin_inset Formula \begin{equation}
f(x)=\sum_{n=0}^{\infty}a_{n}x^{n}\label{eq:taylor-f}\end{equation}

\end_inset 

 This may be trivially interpreted as nothing more than the Taylor's series
 for a real-valued function expanded at 
\begin_inset Formula $x=0$
\end_inset 

.
 The correct formalism for discussing the dual space is a bit trickier.
 There is a strong historical desire to represent the linear operators of
 the dual space by means of integrals, that is, to represent the linear
 functional 
\begin_inset Formula $L:\mathcal{P}\to\mathbb{R}$
\end_inset 

 by an integral 
\begin_inset Formula \begin{equation}
\int_{0}^{1}l(x)\, f(x)\, dx\label{eq:}\end{equation}

\end_inset 

 where 
\begin_inset Formula $f\in\mathcal{P}$
\end_inset 

 and 
\begin_inset Formula $l$
\end_inset 

 is some 
\begin_inset Quotes eld
\end_inset 

generalized function
\begin_inset Quotes erd
\end_inset 

.
 Momentarily succumbing to this desire, one finds that the  dual vectors
 may be written as 
\begin_inset Formula \begin{equation}
e_{k}^{*}=\frac{(-1)^{k}}{k!}\delta^{(k)}(x)\label{eq:}\end{equation}

\end_inset 

 where 
\begin_inset Formula $\delta(x)$
\end_inset 

 is the Dirac delta function.
 Thus, one has as the duality relation 
\begin_inset Formula \begin{equation}
e_{j}^{*}\left(e_{k}\right)=\int_{0}^{1}x^{k}\;\frac{(-1)^{j}}{j!}\delta^{(j)}(x)\, dx=\int_{0}^{1}\,\delta(x)\,\frac{d^{j}x^{k}}{dx^{j}}\, dx=\delta_{jk}\label{eq:orthogonality}\end{equation}

\end_inset 

 after integration by parts.
 Generalized functions are less than an ideal mechanism for representing
 elements of the dual space, but it is a mostly workable and consistent
 mechanism; its drawbacks in this particular context will be discussed in
 great detail below.
 The relation 
\begin_inset LatexCommand \ref{eq:orthogonality}

\end_inset 

 above demonstrates orthogonality; one may also show completeness: 
\begin_inset Formula \begin{equation}
\sum_{n=0}^{\infty}e_{n}\otimes e_{n}^{*}=\sum_{n=0}^{\infty}x^{n}\,\frac{(-1)^{n}}{n!}\delta^{(n)}(x)=\delta(x-y)\label{eq:}\end{equation}

\end_inset 

 
\layout Standard

The remainder of the text switches to the quantum mechanical bra-ket notation,
 writing 
\begin_inset Formula $\left|n\right\rangle =e_{n}$
\end_inset 

 and 
\begin_inset Formula $\left\langle n\right|=e_{n}^{*}$
\end_inset 

.
 The orthogonality condition becomes 
\begin_inset Formula $\left\langle n\right|\left.m\right\rangle =\delta_{nm}$
\end_inset 

 while for completeness one writes 
\begin_inset Formula \[
\mathbb{I}=\sum_{n=0}^{\infty}e_{n}\otimes e_{n}^{*}=\sum_{n=0}^{\infty}\left|n\right\rangle \left\langle n\right|\]

\end_inset 

 The re-introduction of the coordinate 
\begin_inset Formula $x$
\end_inset 

 is done by writing 
\begin_inset Formula $\left\langle x\right|\left.m\right\rangle =x^{m}$
\end_inset 

, while for the transpose one writes 
\begin_inset Formula $\left\langle n\right|\left.x\right\rangle =(-1)^{n}\delta^{(n)}(x)/n!$
\end_inset 

.
 The advantage of the bra-ket notation over the use of the 
\begin_inset Formula $e_{k}$
\end_inset 

 is that it can be used to make clear when one is discussing the coordinate
 representation, involving 
\begin_inset Formula $x$
\end_inset 

 or 
\begin_inset Formula $\delta(x)$
\end_inset 

, and when one is discussing the vector space elements in the abstract,
 without reference to the coordinate representation.
 To fully articulate this notation, one may write the Taylor's series as
 
\begin_inset Formula \begin{eqnarray}
f(x) & = & \left\langle x|f\right\rangle \nonumber \\
 & = & \sum_{n=0}^{\infty}\left\langle x|n\right\rangle \left\langle n|f\right\rangle \nonumber \\
 & = & \sum_{n=0}^{\infty}x^{n}\left\langle n|f\right\rangle \nonumber \\
 & = & \sum_{n=0}^{\infty}x^{n}\int dy\left\langle n|y\right\rangle \left\langle y|f\right\rangle \nonumber \\
 & = & \sum_{n=0}^{\infty}x^{n}\int dy(-)^{n}\frac{\delta^{(n)}(y)}{n!}f(y)\nonumber \\
 & = & \sum_{n=0}^{\infty}x^{n}\frac{f^{(n)}(0)}{n!}\label{eq:Taylors bra-kets}\end{eqnarray}

\end_inset 

 
\layout Standard

This notation allows the matrix elements 
\begin_inset Formula $U_{mn}=\left\langle m\left|\mathcal{L}\right|n\right\rangle $
\end_inset 

 of the transfer operator to be articulated as well.
 Starting with the classical notation of equation 
\begin_inset LatexCommand \ref{eq:taylor-f}

\end_inset 

, one has 
\begin_inset Formula \begin{equation}
\left[\mathcal{L}f\right](x)=\sum_{m=0}^{\infty}b_{m}x^{m}=\sum_{m=0}^{\infty}x^{m}\sum_{n=0}^{\infty}U_{mn}a_{n}\label{eq:}\end{equation}

\end_inset 

 or, equivalently 
\begin_inset Formula \begin{equation}
\left[\mathcal{L}f\right](x)=\left\langle x\left|\mathcal{L}f\right.\right\rangle =\sum_{m=0}^{\infty}\left\langle x\left|m\right.\right\rangle \left\langle m\left|\mathcal{L}f\right.\right\rangle =\sum_{m=0}^{\infty}\sum_{n=0}^{\infty}\left\langle x\left|m\right.\right\rangle \left\langle m\left|\mathcal{L}\right|n\right\rangle \left\langle n|f\right\rangle \label{eq:}\end{equation}

\end_inset 

where 
\begin_inset Formula $\left\langle n|f\right\rangle =a_{n}$
\end_inset 

 and 
\begin_inset Formula $\left\langle n\left|\mathcal{L}f\right.\right\rangle =b_{n}$
\end_inset 

.
 Equating each power of 
\begin_inset Formula $x^{m}$
\end_inset 

 one finds, in the classical notation, that 
\begin_inset Formula \begin{equation}
\left.\frac{1}{m!}\;\frac{d^{m}\left[\mathcal{L}f\right](x)}{dx^{m}}\right|_{x=0}=\sum_{n=0}^{\infty}U_{mn}\left.\frac{1}{n!}\;\frac{d^{n}f(x)}{dx^{n}}\right|_{x=0}\label{eq:}\end{equation}

\end_inset 

 where 
\begin_inset Formula \begin{equation}
\left\langle n|f\right\rangle =a_{n}=\left.\frac{1}{n!}\;\frac{d^{n}f(x)}{dx^{n}}\right|_{x=0}\label{eq:}\end{equation}

\end_inset 

 and so on.
 The formulation of the transfer operator given in equation 
\begin_inset LatexCommand \ref{eq:transfer-dirac}

\end_inset 

 fits in this framework as well; it is merely the spatial representation
 of the matrix elements: 
\begin_inset Formula \begin{eqnarray}
\delta\left(x-g(y)\right)=\mathcal{L}(x,y) & = & \left\langle x\left|\mathcal{L}\right|y\right\rangle \nonumber \\
 & = & \sum_{m=0}^{\infty}\sum_{n=0}^{\infty}\left\langle x|m\right\rangle \left\langle m\left|\mathcal{L}\right|n\right\rangle \left\langle n|y\right\rangle \label{eq:}\end{eqnarray}

\end_inset 


\layout Standard

Armed with this notation, the following section explores the matrix elements
 of the Bernoulli operator in this polynomial basis.
 
\layout Section

The Bernoulli Operator in the Polynomial Basis
\layout Standard

This section reviews the structure of the Bernoulli operator 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

 in the polynomial basis developed above.
 The matrix elements in the monomial basis are given by
\layout Standard


\begin_inset Formula \begin{equation}
\left[\mathcal{L}_{B}\right]_{mk}\equiv U_{mk}\equiv\left\langle m\right|U\left|k\right\rangle =\frac{\delta_{mk}}{2^{m}}+\left(\begin{array}{c}
k\\
m\end{array}\right)\frac{\Theta_{mk}}{2^{k+1}}\label{eq:U_mn}\end{equation}

\end_inset 

 where 
\begin_inset Formula $\left(\begin{array}{c}
m\\
k\end{array}\right)=\frac{m!}{k!(m-k)!}$
\end_inset 

 is the binomial coefficient, and 
\begin_inset Formula \begin{equation}
\Theta_{mk}=\left\{ \begin{array}{c}
0\;\textrm{ if }\; k\leq m\\
1\;\textrm{ if }\; k>m\end{array}\right.\label{eq:}\end{equation}

\end_inset 

 is a traceless, pure upper-triangular matrix.
 These matrix elements are easily obtained by direct substitution, that
 is, by contemplating the coefficient to the 
\begin_inset Formula $x^{m}$
\end_inset 

 term in the expansion of 
\begin_inset Formula \begin{equation}
\left[\mathcal{L}_{B}x^{k}\right]=\frac{1}{2}\left[\left(\frac{x}{2}\right)^{k}+\left(\frac{1+x}{2}\right)^{k}\right]\label{eq:}\end{equation}

\end_inset 

 Both the diagonal elements, and the reason for the appearance of the binomial
 coefficients should be immediately clear.
 Note that the evaluation of these matrix elements does not require the
 evaluation of any sums with an infinite number of non-zero terms.
\layout Standard

The eigenvalues may be promptly read off the diagonal; these eigenvalues
 are 
\begin_inset Formula $\lambda_{n}=2^{-n}$
\end_inset 

.
 Because the matrix is upper-triangular, it is easily solvable for both
 the left and right eigenvectors, which agree perfectly with those given
 by Driebe
\begin_inset LatexCommand \cite{Dri99}

\end_inset 

.
 Visually, the upper-left of this matrix looks like 
\begin_inset Formula \begin{equation}
U_{mk}=\left[\begin{array}{cccccc}
1 & \frac{1}{4} & \frac{1}{8} & \frac{1}{16} & \frac{1}{32} & ...\\
0 & \frac{1}{2} & \frac{1}{4} & \frac{3}{16} & \frac{1}{8}\\
0 & 0 & \frac{1}{4} & \frac{3}{16} & \frac{3}{16}\\
0 & 0 & 0 & \frac{1}{8} & \frac{1}{8}\\
0 & 0 & 0 & 0 & \frac{1}{16}\\
... &  &  &  &  & ...\end{array}\right]\label{eq:}\end{equation}

\end_inset 

 The right and left eigenvectors are developed in the following sections.
\layout Subsection

Right eigenvectors
\layout Standard

The right eigenvectors are denoted by 
\begin_inset Formula $\left|B_{n}\right\rangle $
\end_inset 

 and have the vector components 
\begin_inset Formula \begin{equation}
\left\langle k\left|B_{n}\right.\right\rangle =\left(\begin{array}{c}
n\\
k\end{array}\right)\left(1-\Theta_{n,k}\right)B_{n-k}=\left\{ \begin{array}{ccc}
\left(\begin{array}{c}
n\\
k\end{array}\right)B_{n-k} & \;\textrm{ for }\; & k\leq n\\
0 & \;\textrm{ for }\; & k>n\end{array}\right.\label{eq:B-right matrix elts}\end{equation}

\end_inset 

 where 
\begin_inset Formula $B_{k}$
\end_inset 

 are the Bernoulli numbers.
 Note that only a finite number of these vector components are non-vanishing.
\layout Theorem

The 
\begin_inset Formula $\left|B_{n}\right\rangle $
\end_inset 

 are eigenvectors of 
\begin_inset Formula $U_{mk}$
\end_inset 

, associated with eigenvalues 
\begin_inset Formula $2^{-n}$
\end_inset 

.
 That is, 
\begin_inset Formula \begin{equation}
\mathcal{L}_{B}\left|B_{n}\right\rangle =\frac{1}{2^{n}}\left|B_{n}\right\rangle \label{eq:}\end{equation}

\end_inset 

 
\layout Proof

This may be verified in the monomial basis through brute-force multiplication
 of the vector into the matrix: 
\begin_inset Formula \begin{eqnarray*}
\sum_{k=0}^{\infty}\left\langle m\right|U\left|k\right\rangle \left\langle k\left|B_{n}\right.\right\rangle  & = & \sum_{k=m}^{n}\left[\frac{\delta_{mk}}{2^{m}}+\left(\begin{array}{c}
k\\
m\end{array}\right)\frac{\Theta_{mk}}{2^{k+1}}\right]\left(\begin{array}{c}
n\\
k\end{array}\right)B_{n-k}\\
 & = & \frac{1}{2^{m}}\left(\begin{array}{c}
n\\
m\end{array}\right)B_{n-m}+...non-trivial-taylor-expn\\
 & = & \frac{1}{2^{n}}\left\langle m\left|B_{n}\right.\right\rangle \end{eqnarray*}

\end_inset 

 XXX fill in details.
 Note that this proof does not require the evaluation of any sums with an
 infinite number of non-zero elements; all sums are algebraically finite.
\layout Standard

The right eigenvectors can be given a representation in coordinate space,
 and these are found to be the Bernoulli polynomials discussed previously:
\layout Standard


\begin_inset Formula \begin{equation}
\sum_{k=0}^{\infty}\left\langle x|k\right\rangle \left\langle k\left|B_{n}\right.\right\rangle =\sum_{k=0}^{n}\left(\begin{array}{c}
n\\
k\end{array}\right)B_{n-k}x^{k}=B_{n}(x)\label{eq:}\end{equation}

\end_inset 

 These were previously shown to be eigenvectors, and so this is consistent
 with the above.
 Thus, at this level, one may conclude that the coordinate representation
 and the matrix representation for this transfer operator are consistent.
 
\layout Subsection

Left Eigenvectors
\layout Standard

This matrix expression for 
\begin_inset Formula $U_{mn}$
\end_inset 

 also admits left eigenvectors which can be given an explicit representation.
 Letting the left eigenvectors be denoted by 
\begin_inset Formula $\left\langle \tilde{B}_{n}\right|$
\end_inset 

, they have, for 
\begin_inset Formula $n>0$
\end_inset 

, the components 
\begin_inset Formula \begin{equation}
\left\langle \left.\tilde{B}_{n}\right|k\right\rangle =\left(\begin{array}{c}
k\\
n-1\end{array}\right)\frac{1}{n}\left(\delta_{nk}+\Theta_{nk}\right)\label{eq:B-left matrix elts}\end{equation}

\end_inset 

 The zeroth left eigenvector is a special-case; it has components 
\begin_inset Formula $\left\langle \left.\tilde{B}_{0}\right|k\right\rangle =1/(k+1)$
\end_inset 

.
 Unlike the right eigenvectors, the left eigenvectors all have an infinite
 number of non-zero components.
 They share the same eigenvalue spectrum with the right eigenvectors, so
 that 
\begin_inset Formula \begin{equation}
\sum_{k=0}^{\infty}\left\langle \left.\tilde{B}_{n}\right|m\right\rangle U_{mk}=\frac{1}{2^{n}}\left\langle \left.\tilde{B}_{n}\right|k\right\rangle \label{eq:}\end{equation}

\end_inset 

 which may again be demonstrated by a brute-force evaluation of the sum.
 Despite the fact that an infinite number of the left eigenvector components
 are non-vanishing, this sum will only contain a finite number of non-zero
 terms, and thus its finiteness is guaranteed on algebraic grounds.
\layout Standard

One may write down a coordinate-space representation for the left eigenvectors,
 by contracting them against the dual-space basis elements 
\begin_inset Formula $\left\langle k|x\right\rangle $
\end_inset 

.
 This leads directly to the generalized functions: 
\begin_inset Formula \begin{eqnarray}
\left\langle \tilde{B}_{n}|x\right\rangle  & = & \sum_{k=0}^{\infty}\left\langle \left.\tilde{B}_{n}\right|k\right\rangle \left\langle k|x\right\rangle \nonumber \\
 & = & \sum_{k=0}^{\infty}\left\langle \left.\tilde{B}_{n}\right|k\right\rangle (-)^{k}\frac{\delta^{(k)}(x)}{k!}\nonumber \\
 & = & \frac{1}{n}\sum_{k=n}^{\infty}\left(\begin{array}{c}
k\\
n-1\end{array}\right)(-)^{k}\frac{\delta^{(k)}(x)}{k!}\nonumber \\
 & = & \frac{(-)^{n+1}}{n!}\left[\delta^{(n-1)}(1-x)-\delta^{(n-1)}(x)\right]\label{eq:left bernoulli}\end{eqnarray}

\end_inset 

 for the 
\begin_inset Formula $n>0$
\end_inset 

 case.
 The 
\begin_inset Formula $n=0$
\end_inset 

 left eigenvector is best understood by integrating it over some arbitrary
 function 
\begin_inset Formula $f(x)$
\end_inset 

: 
\begin_inset Formula \begin{eqnarray}
\int_{0}^{1}dx\,\left\langle \left.\tilde{B}_{0}\right|x\right\rangle f(x) & = & \int_{0}^{1}dx\, f(x)\sum_{k=0}^{\infty}(-)^{k}\frac{\delta^{(k)}(x)}{(k+1)!}\nonumber \\
 & = & \sum_{k=0}^{\infty}\frac{f^{(k)}(0)}{(k+1)!}\nonumber \\
 & = & \sum_{k=0}^{\infty}\frac{f^{(k)}(0)}{k!}\int_{0}^{1}x^{k}dx\nonumber \\
 & = & \int_{0}^{1}f(x)dx\nonumber \\
 & = & \left\langle \left.\tilde{B}_{0}\right|f\right\rangle \label{eq:}\end{eqnarray}

\end_inset 

 The other left eigenvectors can also be made more concrete by looking at
 how they act on some function 
\begin_inset Formula $f(x)$
\end_inset 

; this may be written as 
\begin_inset Formula \begin{equation}
\left\langle \left.\tilde{B}_{n}\right|f\right\rangle =\frac{1}{n!}\left[f^{(n-1)}(1)-f^{(n-1)}(0)\right]\label{eq:B-tilde acting on f}\end{equation}

\end_inset 


\layout Subsection

Duality
\layout Standard

That the left eigenvectors are dual to the Bernoulli polynomials, which
 can be verified in either the matrix-element basis, or the coordinate-space
 representation.
\layout Theorem

The left and right eigenvectors are dual, in that
\begin_inset Formula \begin{equation}
\left\langle \left.\tilde{B}_{n}\right|B_{m}\right\rangle =\sum_{k=0}^{\infty}\left\langle \left.\tilde{B}_{n}\right|k\right\rangle \left\langle k\left|B_{m}\right.\right\rangle =\delta_{nm}\label{eq:}\end{equation}

\end_inset 

 and furthermore, the duality is algebraic, in that no infinite sums need
 be performed to demonstrate duality.
\layout Proof

Consider first the 
\begin_inset Formula $n=0$
\end_inset 

 case.
 One has 
\begin_inset Formula \begin{eqnarray*}
\left\langle \left.\tilde{B}_{0}\right|B_{m}\right\rangle  & = & \sum_{k=0}^{\infty}\left\langle \left.\tilde{B}_{0}\right|k\right\rangle \left\langle k\left|B_{m}\right.\right\rangle \\
 & = & \sum_{k=0}^{m}\frac{1}{k+1}\left(\begin{array}{c}
m\\
k\end{array}\right)B_{m-k}\\
 & = & \frac{1}{m+1}\sum_{j=0}^{m}\left(\begin{array}{c}
m+1\\
j\end{array}\right)B_{j}\\
 & = & \delta_{m0}\end{eqnarray*}

\end_inset 

 where the substitution 
\begin_inset Formula $j=m-k$
\end_inset 

 was made to obtain the last sum.
 The vanishing of the last sum is a well-known identity on the Bernoulli
 numbers.
 The 
\begin_inset Formula $n\ne0$
\end_inset 

 case requires more work, but ends similarly: 
\begin_inset Formula \begin{eqnarray*}
\left\langle \left.\tilde{B}_{n}\right|B_{m}\right\rangle  & = & \sum_{k=0}^{\infty}\left\langle \left.\tilde{B}_{n}\right|k\right\rangle \left\langle k\left|B_{m}\right.\right\rangle \\
 & = & \sum_{k=0}^{m}\left(\begin{array}{c}
k\\
n-1\end{array}\right)\frac{1}{n}\left(\delta_{nk}+\Theta_{nk}\right)\left(\begin{array}{c}
m\\
k\end{array}\right)B_{m-k}\\
 & = & \frac{1}{n}\left(\begin{array}{c}
n\\
n-1\end{array}\right)\left(\begin{array}{c}
m\\
n\end{array}\right)B_{m-n}+\sum_{k=n+1}^{m}\left(\begin{array}{c}
k\\
n-1\end{array}\right)\frac{1}{n}\left(\begin{array}{c}
m\\
k\end{array}\right)B_{m-k}\\
 & = & \begin{cases}
0 & \mbox{ for }n>m\\
B_{0} & \mbox{ for }n=m\\
\left(\begin{array}{c}
m\\
n\end{array}\right)B_{m-n}+\frac{m!}{n!(m-n+1)!}\sum_{k=n+1}^{m}\left(\begin{array}{c}
m-n+1\\
m-k\end{array}\right)B_{m-k} & \mbox{ for }n<m\end{cases}\end{eqnarray*}

\end_inset 

 The last case can be shown to vanish, by again making the substitution
 
\begin_inset Formula $j=m-k$
\end_inset 

, to get 
\begin_inset Formula \begin{eqnarray*}
\left(\begin{array}{c}
m\\
n\end{array}\right)B_{m-n} & + & \frac{m!}{n!(m-n+1)!}\sum_{j=0}^{m-n-1}\left(\begin{array}{c}
m-n+1\\
j\end{array}\right)B_{j}\\
 & = & \frac{m!}{n!(m-n+1)!}\sum_{j=0}^{m-n}\left(\begin{array}{c}
m-n+1\\
j\end{array}\right)B_{j}=0\end{eqnarray*}

\end_inset 

 thus concluding the proof.
 Notice that this proof does not require the evaluation of any infinite
 sums: all sums are performed over a finite number of terms.
\layout Standard

This proof of duality may also be conducted in coordinate space, where it
 takes the form 
\begin_inset Formula \begin{eqnarray}
\left\langle \left.\tilde{B}_{n}\right|B_{m}\right\rangle  & = & \int_{0}^{1}\left\langle \left.\tilde{B}_{n}\right|x\right\rangle \left\langle x\left|B_{m}\right.\right\rangle \, dx\nonumber \\
 & = & \int_{0}^{1}\frac{(-1)^{n+1}}{n!}\left[\delta^{(n-1)}(1-x)-\delta^{(n-1)}(x)\right]B_{m}(x)\, dx\nonumber \\
 & = & \begin{cases}
0 & \mbox{ for }m<n-1\\
\frac{m!}{n!(m-n+1)!}\left[B_{m-n+1}(1)-B_{m-n+1}(0)\right] & \mbox{ for }m\ge n-1\end{cases}\nonumber \\
 & = & \delta_{mn}\label{eq:}\end{eqnarray}

\end_inset 

 and so, as with most of the previous results, one may be lulled into a
 sense of complacency about the equivalence of the coordinate-space and
 monomial-vector-space representations.
 This complacency is ill-founded, as demonstrated below.
\layout Subsection

Completeness
\layout Standard

Given this duality, the operator 
\begin_inset Formula \begin{equation}
\mathbb{I}_{B}=\sum_{n=0}^{\infty}\left|B_{n}\right\rangle \left\langle \tilde{B}_{n}\right|\label{eq:Bern-ident}\end{equation}

\end_inset 

 can then be recognized as a projection operator.
 In fact, it is complete in the monomial basis, in that 
\begin_inset Formula \begin{equation}
\left\langle j\left|\mathbb{I}_{B}\right|k\right\rangle =\left\langle j\right|\sum_{n=0}^{\infty}\left|B_{n}\right\rangle \left\langle \tilde{B}_{n}\right|\left|k\right\rangle =\delta_{jk}\label{eq:B-complete}\end{equation}

\end_inset 

 can be shown, using essentially the same operations as in the proof above.
 Also, as before, this demonstration involves sums with only a finite number
 of terms, and so completeness may be taken as an algebraic property.
 That is, 
\begin_inset Formula $\mathbb{I}_{B}$
\end_inset 

 may be taken to be the identity operator on the vector space of polynomials.
\layout Standard

Curiously, this identity operator 
\begin_inset Formula $\mathbb{I}_{B}$
\end_inset 

 expanded in the Bernoulli basis as in formula 
\begin_inset LatexCommand \ref{eq:Bern-ident}

\end_inset 

 is the Euler-Maclaurin summation formula in disguise.
 This may be seen by expanding
\layout Standard


\begin_inset Formula \begin{eqnarray}
f(x) & = & \left\langle x|f\right\rangle \nonumber \\
 & = & \sum_{m=0}^{\infty}B_{m}(x)\left\langle \left.\tilde{B}_{m}\right|f\right\rangle \label{eq:B-maclaurin}\\
 & = & \int_{0}^{1}f(y)\, dy+\sum_{m=0}^{\infty}\frac{B_{m}(x)}{m!}\left[f^{(m-1)}(1)-f^{(m-1)}(0)\right]\nonumber \end{eqnarray}

\end_inset 

 This may be compared to the 
\begin_inset Formula $n=1$
\end_inset 

 case of the traditional Euler-Maclaurin summation formula, 
\begin_inset Formula \begin{equation}
\frac{1}{n}\sum_{k=0}^{n-1}f\left(\frac{k+x}{n}\right)=\int_{0}^{1}f(y)\, dy+\sum_{m=0}^{\infty}\frac{B_{m}(x)}{n^{m}m!}\left[f^{(m-1)}(1)-f^{(m-1)}(0)\right]\label{eq:}\end{equation}

\end_inset 

 By combining equations 
\begin_inset LatexCommand \ref{eq:Bern-ident}

\end_inset 


\begin_inset LatexCommand \ref{eq:B-complete}

\end_inset 

 and 
\begin_inset LatexCommand \ref{eq:B-maclaurin}

\end_inset 

 one is sorely tempted to deduce orthogonality over coordinate space.
 That is, one wants to deduce that 
\begin_inset Formula \begin{equation}
\sum_{n=0}^{\infty}\left\langle x\left|B_{n}\right.\right\rangle \left\langle \left.\tilde{B}_{n}\right|y\right\rangle =\delta(x-y)\label{eq: Bernoulli-faulty}\end{equation}

\end_inset 

 which superficially seems to be entirely reasonable.
 The problem with equation 
\begin_inset LatexCommand \ref{eq: Bernoulli-faulty}

\end_inset 

 is that it is misleading, as will be expanded upon in the following sections.
 As long as the functions 
\begin_inset Formula $f(x)$
\end_inset 

 are in all cases understood to be polynomials, then there is no harm in
 using equation 
\begin_inset LatexCommand \ref{eq: Bernoulli-faulty}

\end_inset 

.
 This follows in part because 
\begin_inset Formula $\mathbb{I}_{B}$
\end_inset 

 really is the identity operator on the space of polynomials: there are
 no polynomials (other than 
\begin_inset Formula $f(x)=0$
\end_inset 

) that are in the kernel of 
\begin_inset Formula $\mathbb{I}_{B}$
\end_inset 

.
 However, one may consider larger function spaces than those consisting
 of polynomials; on these spaces, 
\begin_inset Formula $\mathbb{I}_{B}$
\end_inset 

 will be found to have a large and non-trivial kernel, while by contrast,
 the Dirac delta function 
\begin_inset Formula $\delta(x-y)$
\end_inset 

 does not have a non-trivial kernel on these same spaces.
 This will be delved into in the next section.
\layout Subsection

The Bernoulli operator in diagonal form; the Koopman operator
\layout Standard

From the above manipulations, one may deduce that, in the polynomial representat
ion, the Frobenius-Perron operator of the Bernoulli map is 
\begin_inset Formula \begin{equation}
\mathcal{L}_{B}=\sum_{n=0}^{\infty}\left|B_{n}\right\rangle \lambda_{n}\left\langle \tilde{B}_{n}\right|\label{eq:}\end{equation}

\end_inset 

 We can make use of this diagonal form to easily compute formal expressions
 involving 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

.
 Thus, for a function 
\begin_inset Formula $f(x)$
\end_inset 

 that is expressible as a polynomial series in 
\begin_inset Formula $x$
\end_inset 

, one may write the operator 
\begin_inset Formula \begin{equation}
f\left(\mathcal{L}_{B}\right)=\sum_{n=0}^{\infty}\left|B_{n}\right\rangle f\left(\lambda_{n}\right)\left\langle \tilde{B}_{n}\right|\label{eq:}\end{equation}

\end_inset 

 whose matrix elements can be explicitly demonstrated in the monomial basis:
 
\begin_inset Formula \begin{equation}
\left\langle j\left|f\left(\mathcal{L}_{B}\right)\right|k\right\rangle =\sum_{j\leq n\leq k}\left(\begin{array}{c}
n\\
j\end{array}\right)\left(\begin{array}{c}
k\\
n-1\end{array}\right)\frac{B_{n-j}}{n}f\left(2^{-n}\right)\label{eq:}\end{equation}

\end_inset 

 As in previous cases, note that the summation involves only a finite number
 of terms, and is thus manifestly finite (provided that 
\begin_inset Formula $f$
\end_inset 

 is finite).
 As curious example, one may write, 
\begin_inset Formula $\mathcal{L}_{B}=\exp(-H_{B})$
\end_inset 

 so that 
\begin_inset Formula $H_{B}=-\log\mathcal{L}_{B}$
\end_inset 

 has matrix elements 
\begin_inset Formula \begin{equation}
\left\langle j\left|H_{B}\right|k\right\rangle =\frac{\log(2)}{k+1}\left(\begin{array}{c}
k+1\\
j\end{array}\right)\sum_{m=0}^{k-j}\left(\begin{array}{c}
k-j+1\\
m\end{array}\right)(j+m)\, B_{m}\label{eq:}\end{equation}

\end_inset 


\layout Standard

None of the eigenvalues 
\begin_inset Formula $\lambda_{n}$
\end_inset 

 are zero.
 In the previous section, it was shown that the kernel of 
\begin_inset Formula $\mathbb{I}_{B}$
\end_inset 

 is trivial.
 Thus, 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

 is invertible.
 This inverse is known as the 
\emph on 
Koopman operator
\emph default 
, and is denoted by 
\begin_inset Formula $\mathcal{K}_{B}$
\end_inset 

: 
\begin_inset Formula \begin{equation}
\mathcal{K}_{B}=\sum_{n=0}^{\infty}\left|B_{n}\right\rangle \frac{1}{\lambda_{n}}\left\langle \tilde{B}_{n}\right|\label{eq:}\end{equation}

\end_inset 

 By duality and completeness, one has that the Koopman operator is both
 a left and a right inverse, 
\begin_inset Formula \begin{equation}
\mathcal{L}_{B}\mathcal{K}_{B}=\mathcal{K}_{B}\mathcal{L}_{B}=\mathbb{I}_{B}\label{eq:}\end{equation}

\end_inset 

 in the polynomial representation.
 In this representation, one may honestly write 
\begin_inset Formula $\mathcal{K}_{B}=\mathcal{L}_{B}^{-1}$
\end_inset 

.
 This will not at all be the case when one considers the Bernoulli operator
 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

 acting on other function spaces: it will be seen to have a large and non-trivia
l kernel, and so it will not be invertible (as 
\begin_inset Quotes eld
\end_inset 

half
\begin_inset Quotes erd
\end_inset 

 of its eigenvlaues will be seen to be zero).
\layout Section

Topology, Completeness and Orthogonality
\layout Standard

The notions of completeness and orthogonality are treated above without
 any appeal to topology.
 That is, they are handled with what is essentially an algebraic approach,
 where all sums are essentially finite and well defined because all sums
 involve only a finite number of non-zero terms.
 This was possible in part by construction, and in part by luck: the Bernoulli
 operator was a solvable, upper-triangular matrix in the infinite vector
 space whose basis elements are the monomials.
 A sum over monomials, where only a finite number of terms are non-zero,
 is a polynomial.
 To go beyond this, to get to more general functions, such as, for example,
 a sum over monomials with an infinite number of non-zero elements, requires
 the introduction of a topology on the infinite vector space, so that limits
 of Cauchy sequences can be defined and discussed.
 There are several ways to provide a topology; the straightforward way is
 to provide the space with a metric topology.
 A metric topology endows the infinite vector space with a norm, so that
 the length of a vector can be given, and the distance between vectors defined
 as the length of the vector difference.
\layout Standard

With this in mind, the question then turns to 
\begin_inset Quotes eld
\end_inset 

what are the interesting topologies?
\begin_inset Quotes erd
\end_inset 

.
 Before this question is asked in earnest, it is worth illustrating why,
 exactly, it is an important question, and why the role of topologies needs
 to be addressed.
 Some of the difficulties of sticking to a purely algebraic approach are
 illustrated in this section.
 
\layout Standard

The operator 
\begin_inset Formula $\mathbb{I}_{B}=\sum_{n=0}^{\infty}\left|B_{n}\right\rangle \left\langle \tilde{B}_{n}\right|$
\end_inset 

 was shown to be complete on the space of polynomials.
 By this, it is meant that there is no polynomial that lies in the kernel
 of 
\begin_inset Formula $\mathbb{I}_{B}$
\end_inset 

, other than the trivial polynomial 
\begin_inset Formula $p(x)=0$
\end_inset 

.
 Equivalently, there does not exist any polynomial for which the identity
 
\begin_inset Formula $p(x)=-p(x+1/2)$
\end_inset 

 holds: more broadly, there is no such thing as a periodic polynomial.
 A function which obeys 
\begin_inset Formula $f(x)=-f(x+1/2)$
\end_inset 

 is of necessity periodic, of which 
\begin_inset Formula $f(x)=\sin(2\pi x)$
\end_inset 

 is a canonical example.
 More generally, sine and cosine waves which have an odd number of periods
 in the unit interval are all in the kernel of the Bernoulli operator, when
 that operator is taken in the coordinate-space.
 This may be seen very easily simply by direct substitution into equation
 
\begin_inset LatexCommand \ref{eq:Bernoulli operator}

\end_inset 

, which promptly yields 
\begin_inset Formula \begin{equation}
\mathcal{L}_{B}^{\mbox{Coordinate}}\sin2\pi(2k+1)x=\frac{1}{2}\left[\sin\pi(2k+1)x+\sin\pi(2k+1)(x+1)\right]=0\label{eq:}\end{equation}

\end_inset 

 for integers 
\begin_inset Formula $k$
\end_inset 

; likewise for the cosine.
 Not so for waves with an even number of periods: 
\begin_inset Formula \begin{equation}
\mathcal{L}_{B}^{\mbox{Coordinate}}\sin4\pi kx=\frac{1}{2}\left[\sin2\pi kx+\sin2\pi k(x+1)\right]=\sin2\pi kx\label{eq:}\end{equation}

\end_inset 

Here, the superscript 
\begin_inset Quotes eld
\end_inset 

Coordinate
\begin_inset Quotes erd
\end_inset 

 is introduced to distinguish the operator in the coordinate basis, as given
 in equation 
\begin_inset LatexCommand \ref{eq:Bernoulli operator}

\end_inset 

, from the same operator, but in the monomial basis:
\begin_inset Formula \begin{equation}
\mathcal{L}_{B}^{\mbox{Monomial}}=\sum_{m,n=0}^{\infty}\left|m\right\rangle U_{mn}\left\langle n\right|\label{eq:}\end{equation}

\end_inset 

 where 
\begin_inset Formula $U_{mn}$
\end_inset 

 is the upper-triangular matrix of equation 
\begin_inset LatexCommand \ref{eq:U_mn}

\end_inset 

.
 Similarly, for the Bernoulli basis: 
\begin_inset Formula \begin{equation}
\mathcal{L}_{B}^{\mbox{Bernoulli}}=\sum_{n=0}^{\infty}\left|B_{n}\right\rangle \frac{1}{2^{n}}\left\langle \tilde{B}_{n}\right|\label{eq:}\end{equation}

\end_inset 

 Via the theorems in the previous section, it was established (XXX but perhaps
 not very clearly? XXX) that 
\begin_inset Formula $\mathcal{L}_{B}^{\mbox{Bernoulli}}=\mathcal{L}_{B}^{\mbox{Monomial}}$
\end_inset 

 when considered as operators acting on the space of polynomials.
 However, these are not at all equivalent if one considers them as operators
 acting sines and cosines.
 In particular, consider 
\begin_inset Formula $\mathbb{I}_{B}$
\end_inset 

 acting on 
\begin_inset Formula $f(x)=\exp2\pi ikx$
\end_inset 

:
\begin_inset Formula \begin{eqnarray}
\left[\mathbb{I}_{B}f\right](x) & = & \left\langle x\left|\mathbb{I}_{B}\right|f\right\rangle \nonumber \\
 &  & \sum_{n=0}^{\infty}\left\langle x\left|B_{n}\right.\right\rangle \left\langle \left.\tilde{B}_{n}\right|f\right\rangle \\
 & = & \sum_{n=0}^{\infty}\left\langle x\left|B_{n}\right.\right\rangle \frac{1}{n!}\left[f^{(n-1)}(1)-f^{(n-1)}(0)\right]\nonumber \\
 & = & \sum_{n=0}^{\infty}\left\langle x\left|B_{n}\right.\right\rangle \frac{1}{n!}\left[(2\pi ik)^{n-1}e^{2\pi ik}-(2\pi ik)^{n-1}\right]\nonumber \\
 & = & 0\label{eq:}\end{eqnarray}

\end_inset 

 where the second step makes use of equation 
\begin_inset LatexCommand \ref{eq:B-tilde acting on f}

\end_inset 

.
 This is remarkable, as any periodic wave constructed from sines and cosines
 seems to be in the kernel.
 By contrast, 
\begin_inset Formula $\mathbb{I}_{M}$
\end_inset 

 does not behave this way: 
\begin_inset Formula \begin{eqnarray}
\left[\mathbb{I}_{M}f\right](x) & = & \left\langle x\left|\mathbb{I}_{M}\right|f\right\rangle \nonumber \\
 &  & \sum_{n=0}^{\infty}\left\langle x\left|n\right.\right\rangle \left\langle \left.n\right|f\right\rangle \\
 & = & \sum_{n=0}^{\infty}\left\langle x\left|n\right.\right\rangle \frac{1}{n!}f^{(n)}(0)\nonumber \\
 & = & \sum_{n=0}^{\infty}\left\langle x\left|n\right.\right\rangle \frac{1}{n!}(2\pi ik)^{n}\nonumber \\
 & = & \sum_{n=0}^{\infty}x^{n}\frac{1}{n!}(2\pi ik)^{n}\label{eq:}\\
 & = & \exp2\pi ikx\end{eqnarray}

\end_inset 

 which is just a re-derivation of equation 
\begin_inset LatexCommand \ref{eq:Taylors bra-kets}

\end_inset 

.
 Thus, 
\begin_inset Formula $\mathbb{I}_{B}$
\end_inset 

 and 
\begin_inset Formula $\mathbb{I}_{M}$
\end_inset 

 are inequivalent when acting on sine functions; so 
\begin_inset Formula $\mathcal{L}_{B}^{\mbox{Monomial}}$
\end_inset 

 and 
\begin_inset Formula $\mathcal{L}_{B}^{\mbox{Bernoulli}}$
\end_inset 

 are inequivalent as well.
\layout Standard

Since the sine function is the limit of a polynomial sequence, it seems
 strange or somehow contradictory that 
\begin_inset Formula $\mathcal{L}_{B}^{\mbox{Bernoulli}}$
\end_inset 

 has only a trivial kernel on the space of polynomials, while utterly and
 completely killing all sine functions.
 In order to define limits, or more precisely, in order to define 
\begin_inset Formula $\sin2\pi x$
\end_inset 

 as the limit of a sequence of polynomials, one must define the manner in
 which a polynomial sequence can converge to a function, and, for that,
 one must have a topology.
\layout Standard

Can this conundrum be escaped without appealing to topology? Since 
\begin_inset Formula $\mathbb{I}_{B}$
\end_inset 

 seems to be somehow incomplete when considering sine functions, perhaps,
 one might think, this lack of completeness is due to the form of the left
 eigenstates given in equation 
\begin_inset LatexCommand \ref{eq:left bernoulli}

\end_inset 

.
 One might make a guess that perhaps a more truly complete set of states
 can be found by considering 
\begin_inset Formula \begin{equation}
\tilde{S}_{n}(x)=\frac{(-)^{n+1}}{n!}\left[\delta^{(n-1)}(1-x)+\delta^{(n-1)}(x)\right]\label{eq:}\end{equation}

\end_inset 

 so that sums and differences of the duals 
\begin_inset Formula $\tilde{B}_{m}(x)$
\end_inset 

 and 
\begin_inset Formula $\tilde{S}_{n}(x)$
\end_inset 

 can be used to regain the duals to the monomials 
\begin_inset Formula $\left\langle n|y\right\rangle =(-)^{n}\delta^{(n)}(y)/n!$
\end_inset 

.
 
\layout Theorem

The duals to 
\begin_inset Formula $\tilde{S}_{n}(x)$
\end_inset 

 are given by 
\begin_inset Formula $S_{n}(x)=nE_{n}(x)/2$
\end_inset 

 where the 
\begin_inset Formula $E_{n}(x)$
\end_inset 

 are the Euler polynomials.
 
\layout Proof

Consider the generating function for the Euler polynomials 
\begin_inset Formula \begin{equation}
G_{E}(x,t)=\frac{2e^{xt}}{1+e^{t}}=\sum_{n=0}^{\infty}E_{n}(x)\frac{t^{n}}{n!}\label{eq:}\end{equation}

\end_inset 

 Then one has, by taking the left hand side, 
\begin_inset Formula \begin{equation}
\int_{0}^{1}\tilde{S}_{n}(x)\,\frac{2e^{xt}}{1+e^{t}}\, dx=\frac{2}{n}\frac{t^{n-1}}{(n-1)!}\label{eq:}\end{equation}

\end_inset 

 and, performing the same operation on the right hand side, 
\begin_inset Formula \begin{equation}
\int_{0}^{1}\tilde{S}_{n}(x)\sum_{k=0}^{\infty}E_{k}(x)\frac{t^{k}}{k!}\; dx=\sum_{k=0}^{\infty}\frac{t^{k}}{k!}\int_{0}^{1}\tilde{S}_{n}(x)\, E_{k}(x)\, dx\label{eq:}\end{equation}

\end_inset 

 Then, equating the two sides, one has demonstrated duality of these states:
 
\begin_inset Formula \begin{equation}
\int_{0}^{1}\tilde{S}_{n}(x)\, E_{k}(x)\, dx=\frac{2\delta_{k,n-1}}{n}\label{eq:}\end{equation}

\end_inset 

 which completes the proof.
\layout Standard

As with the Bernoulli polynomials, one can, to a limit extent, make a restricted
 completeness statement in coordinate space.
 That is, if one decomposes a function 
\begin_inset Formula $f(y)=\sum_{k=1}^{\infty}a_{k}S_{k}(y)$
\end_inset 

 then one easily finds 
\begin_inset Formula \begin{equation}
\int_{0}^{1}\sum_{n=1}^{\infty}S_{n}(x)\tilde{S}_{n}(y)\, f(y)\, dy=f(x)\label{eq:}\end{equation}

\end_inset 

 from which one wants to conclude, once again incorrectly, or at least misleadin
gly, that 
\begin_inset Formula \begin{equation}
\sum_{n=1}^{\infty}S_{n}(x)\tilde{S}_{n}(y)=\delta(x-y)\label{eq:}\end{equation}

\end_inset 

 by repeating the same concerns and issues that lead to equation 
\begin_inset LatexCommand \ref{eq: Bernoulli-faulty}

\end_inset 

.
 The fault is the assumption that arbitrary, non-polynomial 
\begin_inset Formula $f(y)$
\end_inset 

 can be decomposed in the fashion given.
 In fact, the operator 
\begin_inset Formula $\mathbb{I}_{S}=\sum_{n=0}^{\infty}\left|S_{n}\right\rangle \left\langle \tilde{S}_{n}\right|$
\end_inset 

 has a large kernel: this time, all functions that are evenly periodic are
 in the kernel.
 That is, any function for which one has 
\begin_inset Formula $f(y)=f(y+1/2)$
\end_inset 

 lies in the kernel of 
\begin_inset Formula $\mathbb{I}_{S}$
\end_inset 

.
\layout Standard

One might hope that one can remedy the above situation by taking the sum
 
\begin_inset Formula $\mathbb{I}_{C}=\mathbb{I}_{B}+\mathbb{I}_{S}$
\end_inset 

 with one operator projecting out the even periodic functions, and the other
 the odd periodic functions, and that somehow, between the two of them,
 making a whole.
 However, one immediately runs into a problem with the basis functions.
 The 
\begin_inset Formula $\tilde{S}_{n}(x)$
\end_inset 

 are not orthogonal to the 
\begin_inset Formula $B_{n}(x)$
\end_inset 

, and vice-versa.
 This is easily seen by considering the the generating function for the
 Bernoulli polynomials: 
\begin_inset Formula \begin{equation}
G_{B}(x,t)=\frac{te^{xt}}{e^{t}-1}=\sum_{n=0}^{\infty}B_{n}(x)\frac{t^{n}}{n!}\label{eq:}\end{equation}

\end_inset 

Integrating, one gets 
\begin_inset Formula \begin{equation}
\int_{0}^{1}\tilde{S}_{n}(x)G_{B}(x,t)\, dx=\frac{t^{n}}{n!}\frac{e^{t}+1}{e^{t}-1}=\sum_{k=0}^{\infty}\frac{t^{k}}{k!}\int_{0}^{1}\tilde{S}_{n}(x)\, B_{k}(x)\, dx\label{eq:}\end{equation}

\end_inset 

 from which one deduces that 
\begin_inset Formula $\int_{0}^{1}\tilde{S}_{n}(x)\, B_{k}(x)\, dx=0$
\end_inset 

 only for 
\begin_inset Formula $k<n-1$
\end_inset 

, and similarly for 
\begin_inset Formula $\int_{0}^{1}\tilde{B}_{n}(x)\, S_{k}(x)\, dx$
\end_inset 

.
 
\layout Standard

More generally, any such attempt to patch up the situation is doomed to
 fail.
 This is a well-known theorem of functional analysis, which states that
 the space 
\begin_inset Formula $\mathbb{R}^{\omega}$
\end_inset 

does not have a countable algebraic basis.
 That is, while one may be able to write down a countable number of linearly
 independent vectors 
\begin_inset Formula $e_{k}$
\end_inset 

 with 
\begin_inset Formula $k\in\mathbb{N}_{0}$
\end_inset 

, the linear combinations 
\begin_inset Formula $\sum_{k}a_{k}e_{k}$
\end_inset 

 with only a finite number of 
\begin_inset Formula $a_{k}\in\mathbb{R}$
\end_inset 

 being non-zero fail to span all of 
\begin_inset Formula $\mathbb{R}^{\omega}$
\end_inset 

.
 In particular, the vector 
\begin_inset Formula $v=\sum_{k}e_{k}$
\end_inset 

 cannot be expressed as the sum over only a finite number of 
\begin_inset Formula $e_{k}$
\end_inset 

.
 XXX Does this theorem have a name? Need to reference a ref for this.
 XXXX 
\layout Subsection

Hermitian Operators: Here be Dragons
\layout Standard

There are other conceptual difficulties tied to infinite vector spaces when
 these spaces are not given certain additional structure.
 This section reviews the difficulty of defining the idea of a Hermitian
 or self-adjoint operator on unadorned infinite vector spaces, a difficulty
 that goes away when the space is a Hilbert space.
 When an infinite-dimensional vector space is endowed with an inner product,
 making it into a Hilbert space, then one may define the idea of a Hermitian
 operator 
\begin_inset Formula $H$
\end_inset 

 as an operator that is equal to its (complex-conjugate) transpose: 
\begin_inset Formula $H=H^{\dagger}$
\end_inset 

.
 For Hermitian operators, there always exists a basis in which 
\begin_inset Formula $H$
\end_inset 

 is diagonal, and all of the diagonal matrix elements are real.
 All Hermitian operators may be brought to the diagonal form by applying
 an orthogonal transformation 
\begin_inset Formula $O$
\end_inset 

, where an operator 
\begin_inset Formula $O$
\end_inset 

 is orthogonal when its inverse is equal to its transpose: 
\begin_inset Formula $O^{-1}=O^{T}$
\end_inset 

, or equivalently, when 
\begin_inset Formula $O^{T}O=OO^{T}=1$
\end_inset 

.
 The usual concepts of a Hermitian operator are violated in the infinite
 vector space of polynomials, as demonstrated in this section.
 XXXX This statement needs clarification.
 We have 
\begin_inset Formula $\mathcal{L}_{B}^{\mbox{Bernoulli}}=\mathcal{L}_{B}^{\mbox{Monomial}}$
\end_inset 

when acting on the space of polynomials, per previous results, which should
 be strengthened.
 Thus, if 
\begin_inset Formula $\mathcal{L}_{B}^{\mbox{Bernoulli}}$
\end_inset 

 is Hermitian, then 
\begin_inset Formula $\mathcal{L}_{B}^{\mbox{Monomial}}$
\end_inset 

 should be as well, as demonstrated below.
 Right? XXX well No.
 Fix up this mess.
 XXXX.
\layout Standard

It was seen above that the monomials form a complete set of basis states
 that can be used to represent polynomials.
 The operator 
\begin_inset Formula $\mathbb{I}_{M}=\sum_{n=0}^{\infty}\left|n\right\rangle \left\langle n\right|$
\end_inset 

 can be called the identity operator over the space of polynomials; it has
 no kernel in that space.
 Here, the subscript 
\begin_inset Formula $M$
\end_inset 

 is used to remind us that the identity operator is built from the monomial
 states.
 Thus, for the Bernoulli operator 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

, one may confidently write 
\begin_inset Formula $\mathcal{L}_{B}=\mathbb{I}_{M}\mathcal{L}_{B}\mathbb{I}_{M}$
\end_inset 

 which expands to 
\begin_inset Formula \begin{equation}
\mathcal{L}_{B}^{\mbox{Monomial}}=\sum_{m=0}^{\infty}\sum_{n=0}^{\infty}\left|m\right\rangle \left\langle m\left|\mathcal{L}_{B}\right|n\right\rangle \left\langle n\right|\label{eq:}\end{equation}

\end_inset 

 As seen above, the matrix elements 
\begin_inset Formula $U_{mn}\equiv\left\langle m\left|\mathcal{L}_{B}\right|n\right\rangle $
\end_inset 

 are upper-triangular, and so one wants to say that the operator 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

 is clearly not Hermitian.
 That is, the matrix elements 
\begin_inset Formula $H_{mn}$
\end_inset 

 of Hermitian operators 
\begin_inset Formula $H$
\end_inset 

 are expected to obey the identity 
\begin_inset Formula $H_{mn}=H_{nm}^{*}$
\end_inset 

 where the superscript 
\begin_inset Formula $*$
\end_inset 

 denotes complex conjugation.
 Here, the matrix elements 
\begin_inset Formula $U_{mn}$
\end_inset 

are real, but clearly one has 
\begin_inset Formula $U_{mn}\ne U_{nm}$
\end_inset 

.
 Thus, in this basis it seems that 
\begin_inset Formula $\mathcal{L}_{B}^{\mbox{Monomial}}$
\end_inset 

 cannot be Hermitian.
 
\layout Standard

In previous sections, it was shown that the operator 
\begin_inset Formula $\mathbb{I}_{B}=\sum_{n=0}^{\infty}\left|B_{n}\right\rangle \left\langle \tilde{B}_{n}\right|$
\end_inset 

 is also the identity operator on the space of polynomials.
 Using the same trick to write 
\begin_inset Formula \begin{equation}
\mathcal{L}_{B}^{\mbox{Bernoulli}}=\mathbb{I}_{B}\mathcal{L}_{B}\mathbb{I}_{B}=\sum_{m=0}^{\infty}\sum_{n=0}^{\infty}\left|B_{m}\right\rangle \left\langle \tilde{B}_{m}\left|\mathcal{L}_{B}\right|B_{n}\right\rangle \left\langle \tilde{B}_{n}\right|\label{eq:}\end{equation}

\end_inset 

 one finds that 
\begin_inset Formula $\mathcal{L}_{B}^{\mbox{Bernoulli}}$
\end_inset 

 is an operator which has matrix elements that are non-zero only on the
 diagonal: 
\begin_inset Formula $\left\langle B_{m}\left|U_{B}\right|\tilde{B}_{n}\right\rangle =\delta_{mn}\lambda_{n}$
\end_inset 

.
 Matrices that are diagonal and have only real entries on the diagonal are
 clearly Hermitian, so this implies that 
\begin_inset Formula $\mathcal{L}_{B}^{\mbox{Bernoulli}}$
\end_inset 

 is Hermitian.
 So which is it? Is 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

 Hermitian or not? It appears that the Hermiticity of 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

 depends very much on which of the two polynomial bases are used.
\layout Standard

Consider now the change of basis from the monomial basis to the Bernoulli
 basis.
 Explicitly, this change of basis is 
\begin_inset Formula \begin{eqnarray}
\mathcal{L}_{B}^{\mbox{Bernoulli}} & = & \sum_{j,k=0}^{\infty}\left|B_{j}\right\rangle \delta_{jk}\lambda_{k}\left\langle \tilde{B}_{k}\right|\nonumber \\
 & = & \sum_{j,k,m,n=0}^{\infty}\left|B_{j}\right\rangle \left\langle \left.\tilde{B}_{j}\right|m\right\rangle \left\langle m\left|\mathcal{L}_{B}^{\mbox{Monomial}}\right|n\right\rangle \left\langle n\left|B_{k}\right.\right\rangle \left\langle \tilde{B}_{k}\right|\nonumber \\
 & = & \tilde{B}\,\mathcal{L}_{B}^{\mbox{Monomial}}\, B\label{eq:}\end{eqnarray}

\end_inset 

 where the operators 
\begin_inset Formula $\tilde{B}$
\end_inset 

 and 
\begin_inset Formula $B$
\end_inset 

 show the change of basis: 
\begin_inset Formula \begin{equation}
\tilde{B}=\sum_{j,m=0}^{\infty}\left|B_{j}\right\rangle \left\langle \left.\tilde{B}_{j}\right|m\right\rangle \left\langle m\right|\label{eq:}\end{equation}

\end_inset 

 and 
\begin_inset Formula \begin{equation}
B=\sum_{k,n=0}^{\infty}\left|n\right\rangle \left\langle n\left|B_{k}\right.\right\rangle \left\langle \tilde{B}_{k}\right|\label{eq:}\end{equation}

\end_inset 

 It is not hard to work out that 
\begin_inset Formula $\tilde{B}B=\mathbb{I}_{B}$
\end_inset 

 and that 
\begin_inset Formula $B\tilde{B}=\mathbb{I}_{M}$
\end_inset 

 so that, in a certain sense 
\begin_inset Formula $\tilde{B}$
\end_inset 

 is both a left- and right-inverse of 
\begin_inset Formula $B$
\end_inset 

; one may confidently write 
\begin_inset Formula $B^{-1}=\tilde{B}$
\end_inset 

 as a two-sided inverse.
 Is there any sense in which 
\begin_inset Formula $B$
\end_inset 

 and 
\begin_inset Formula $\tilde{B}$
\end_inset 

 are orthogonal operators? It seems not, which may be deduced from the matrix
 elements.
 If 
\begin_inset Formula $O$
\end_inset 

 is orthogonal, then one has 
\begin_inset Formula $O^{-1}=O^{T}$
\end_inset 

.
 The matrix elements obey 
\begin_inset Formula $\left[O^{-1}\right]_{nk}=\left[O^{T}\right]_{nk}=\left[O\right]_{kn}$
\end_inset 

.
 By contrast, a quick review of equations 
\begin_inset LatexCommand \ref{eq:B-right matrix elts}

\end_inset 

 and 
\begin_inset LatexCommand \ref{eq:B-left matrix elts}

\end_inset 

 makes it clear that 
\begin_inset Formula $\left\langle \left.\tilde{B}_{k}\right|n\right\rangle \ne\left\langle n\left|B_{k}\right.\right\rangle $
\end_inset 

, and so 
\begin_inset Formula $B$
\end_inset 

 is not an orthogonal operator.
 This is perhaps not a surprise: orthogonal operators normally preserve
 the Hermitian-ness of an operator, whereas 
\begin_inset Formula $B$
\end_inset 

 clearly breaks it.
 
\layout Section

The Fourier Representation 
\layout Standard

The Koopman operator of the Bernoulli Map has the property of taking a function
 and making two copies of it.
 That is, 
\begin_inset Formula \begin{eqnarray}
\left[\mathcal{K}_{B}f\right](y) & = & \int_{0}^{1}\delta\left(x-b(y)\right)f(x)\, dx\nonumber \\
 & = & f(b(y))\nonumber \\
 & = & f(2y)\theta(1-2y)+f(2y-1)\theta(2y-1)\label{eq:}\end{eqnarray}

\end_inset 

 where 
\begin_inset Formula $\theta(x)$
\end_inset 

 is the step function, identically zero for 
\begin_inset Formula $x<0$
\end_inset 

 and identically one for 
\begin_inset Formula $x>0$
\end_inset 

.
 The Koopman operator for the Bernoulli map is not faithfully representable
 in the polynomial basis; this can be seen in two ways.
 First, it introduces a discontinuity at 
\begin_inset Formula $x=1/2$
\end_inset 

 which the polynomials cannot move beyond; the radius of the circle of converge
 is limited by this singularity.
 Secondly, it takes a function and more-or-less makes it periodic; again,
 the polynomials cannot cope directly with this.
 Thus, one is motivated to explore the Fourier representation, if only to
 express the Koopman operator.
 
\layout Standard

It is easy to find an explicit form for this operator in the Fourier basis.
 Writing 
\begin_inset Formula \begin{equation}
f(x)=\sum_{n}a_{n}\cos2\pi nx\;+b_{n}\sin2\pi nx\label{eq:}\end{equation}

\end_inset 

 then 
\begin_inset Formula \begin{equation}
\left[\mathcal{K}_{B}f\right](x)=\sum_{n}a_{n}\cos4\pi nx\;+b_{n}\sin4\pi nx\label{eq:}\end{equation}

\end_inset 

 or, in Dirac notation, 
\begin_inset Formula $\left\langle em\left|\mathcal{K}_{B}\right|en\right\rangle =\delta_{2m,n}$
\end_inset 

.
 This is a very singular operator in this basis.
 Visually, it has the distinctive appearance of 
\begin_inset Formula \begin{equation}
\mathcal{K}_{B}=\left[\begin{array}{cccccc}
1 & 0 & 0 & 0 & 0 & ...\\
0 & 0 & 0 & 0 & ...\\
0 & 1 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & ...\\
...\end{array}\right]\label{eq:}\end{equation}

\end_inset 

 where every other row consists of zeros.
 In this same basis, 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

is equally remarkable: it is literally the transpose: that is 
\begin_inset Formula $\mathcal{K}_{B}=\mathcal{L}_{B}^{T}$
\end_inset 

 in this basis, and so 
\begin_inset Formula \begin{equation}
\mathcal{L}_{B}=\left[\begin{array}{cccccc}
1 & 0 & 0 & 0 & 0 & ...\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 1\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & ...\\
...\end{array}\right]\label{eq:}\end{equation}

\end_inset 

 In this representation, it is easily seen that 
\begin_inset Formula $\mathcal{L}_{B}\mathcal{K}_{B}=1$
\end_inset 

 but 
\begin_inset Formula $\mathcal{K}_{B}\mathcal{L}_{B}\neq1$
\end_inset 

, just as in the coordinate-space representation.
 It is very instructive to verify that the Bernoulli polynomials are still
 eigenfunctions in this representation.
 For 
\begin_inset Formula $n\neq0$
\end_inset 

, one has 
\begin_inset Formula \begin{equation}
\int_{0}^{1}B_{1}(x)\,\sin(2\pi nx)\, dx=\frac{-1}{\pi n}\label{eq:}\end{equation}

\end_inset 

 and it is straightforward to visually verify that 
\begin_inset Formula $U_{B}B_{1}=\frac{1}{2}B_{1}$
\end_inset 

.
 By working with the generator for the Bernoulli polynomials, 
\begin_inset Formula \begin{equation}
\frac{te^{xt}}{e^{t}-1}=\sum_{n=0}^{\infty}B_{n}(x)\,\frac{t^{n}}{n!}\label{eq:}\end{equation}

\end_inset 

 one can immediately find, for 
\begin_inset Formula $m\neq0$
\end_inset 

, the Fourier components 
\begin_inset Formula \begin{equation}
\int_{0}^{1}B_{n}(x)\,\cos(2\pi mx)\, dx=\left\{ \begin{array}{cc}
0 & \;\textrm{ for n odd }\\
\left(-\right)^{1+n/2}n!/\left(2\pi m\right)^{n} & \;\textrm{ for n even }\end{array}\right.\label{eq:}\end{equation}

\end_inset 

 and 
\begin_inset Formula \begin{equation}
\int_{0}^{1}B_{n}(x)\,\sin(2\pi mx)\, dx=\left\{ \begin{array}{cc}
0 & \;\textrm{ for n even }\\
\left(-\right)^{(n+1)/2}n!/\left(2\pi m\right)^{n} & \;\textrm{ for n odd }\end{array}\right.\label{eq:}\end{equation}

\end_inset 

 Applying the Fourier-representation 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

 to these to these vector components makes it immediately clear how the
 eigenvalue of 
\begin_inset Formula $1/2^{n}$
\end_inset 

 is associated with the eigenvector 
\begin_inset Formula $B_{n}$
\end_inset 

.
 
\layout Section

The Hurwitz zeta eigenfunctions
\layout Standard

The Fourier representation also makes it clear that any vector with vector
 components 
\begin_inset Formula $a_{n}=1/n^{s}$
\end_inset 

 will be an eigenvector of 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

 associated with the eigenvalue 
\begin_inset Formula $\lambda=1/2^{s}$
\end_inset 

.
 In coordinate space, one may write these eigenfunctions as 
\begin_inset Formula \begin{equation}
\beta(x;s)=2\Gamma(s+1)\sum_{n=1}^{\infty}\frac{\exp(2\pi inx)}{\left(2\pi n\right)^{s}}\label{eq:}\end{equation}

\end_inset 

 which transform as 
\begin_inset Formula $\mathcal{L}_{B}\beta(x;s)=2^{-s}\beta(x;s)$
\end_inset 

.
 Given the nature of summation, this series is strictly convergent for any
 complex-valued 
\begin_inset Formula $s$
\end_inset 

 with 
\begin_inset Formula $\Re s>1$
\end_inset 

.
 This series recreates the Bernoulli polynomials for integer values of 
\begin_inset Formula $n$
\end_inset 

, so for example, 
\begin_inset Formula $\Re\beta(x;2)=B_{2}(x)$
\end_inset 

 and 
\begin_inset Formula $\Im\beta(x;3)=B_{3}(x)$
\end_inset 

 and generally 
\begin_inset Formula $\Re\left[\left(-i\right)^{n}\beta(x;n)\right]=-B_{n}(x)$
\end_inset 

.
 Equivalently, the Fourier series for the Bernoulli polynomials can be written
 as 
\begin_inset Formula \begin{eqnarray}
B_{n}(x) & = & -\Gamma(n+1)\sum_{k=1}^{\infty}\frac{\exp(2\pi ikx)+\exp(2\pi ik(1-x))}{\left(2\pi ik\right)^{n}}\label{eq:}\\
 & = & \frac{-(-i)^{n}}{2}\left(\beta(x;n)+\beta(1-x;n)\right)\nonumber \end{eqnarray}

\end_inset 

See, for example 
\begin_inset LatexCommand \cite[Thm. 12.19]{Apo76}

\end_inset 

.
 It turns out that these eigenfunctions are essentially a form of the Hurwitz
 zeta function 
\begin_inset Formula \begin{equation}
\zeta(s,x)=\sum_{n=0}^{\infty}\frac{1}{\left(n+x\right)^{s}}\label{eq:}\end{equation}

\end_inset 

 and that, in fact, the Hurwitz zeta itself is an eigenfunction, with eigenvalue
 
\begin_inset Formula $2^{s-1}$
\end_inset 

.
 One may confirm this by following a very old-fashioned recipe for obtaining
 the functional relation for a zeta-like sum.
 Start by expressing the gamma function as 
\begin_inset Formula \begin{equation}
\int_{0}^{\infty}dy\, e^{-2\pi ny}y^{s-1}=\frac{\Gamma(s)}{(2\pi n)^{s}}\label{eq:}\end{equation}

\end_inset 

 Substituting into the expression for 
\begin_inset Formula $\beta$
\end_inset 

 and performing the sum, one may write 
\begin_inset Formula \begin{equation}
\beta(x;s)=2s\int_{0}^{\infty}dy\,\frac{y^{s-1}}{\exp\left(-2\pi i(x+iy)\right)-1}\label{eq:}\end{equation}

\end_inset 

 Then, following a traditional trick 
\begin_inset LatexCommand \cite[pp 13 ff]{Edw74}

\end_inset 

, re-write this as a contour integral 
\begin_inset Formula \begin{equation}
\beta(x;s)=\frac{-is}{\sin\pi s}\oint\frac{(-y)^{s}}{\exp\left(-2\pi i(x+iy)\right)-1}\;\frac{dy}{y}\label{eq:}\end{equation}

\end_inset 

 where the contour is taken to extend from 
\begin_inset Formula $+\infty+i\epsilon$
\end_inset 

, running just above the positive real axis, to the origin, circling the
 origin in a clockwise fashion, and returning to 
\begin_inset Formula $+\infty-i\epsilon$
\end_inset 

 just under the real axis.
 The contour essentially encloses the cut of the logarithm in the expression
 
\begin_inset Formula $(-y)^{s}=\exp s\,\log(-y)$
\end_inset 

.
 The old fashioned recipe calls for closing the contour at infinity (in
 a counter-clockwise direction) and then taking the dubious step of asserting
 Cauchy's Theorem to equate the integral around the cut to the sum of the
 poles, where we note that we have a pole whenever 
\begin_inset Formula $x+iy=n$
\end_inset 

 for some integer 
\begin_inset Formula $n$
\end_inset 

.
 By doing this we get the formal summation 
\begin_inset Formula \begin{equation}
\frac{i\sin\pi s}{s}\beta(x;s)=\exp\left(\frac{i\pi s}{2}\right)\sum_{n=-\infty}^{\infty}(n-x)^{s-1}\label{eq:}\end{equation}

\end_inset 

 This is a 
\begin_inset Quotes eld
\end_inset 

formal sum
\begin_inset Quotes erd
\end_inset 

, since the preceding steps required taking 
\begin_inset Formula $\Re s>1$
\end_inset 

 whereas now one needs to take 
\begin_inset Formula $\Re s<0$
\end_inset 

.
 This is a bit of jiggery-pokery that is common for this type of presentation;
 a different set of tools is required to do better.
 So we proceed, ignoring these difficulties.
 Re-write this sum as 
\begin_inset Formula \begin{equation}
\frac{i\sin\pi s}{s}\beta(x;s)=\exp\left(\frac{i\pi s}{2}\right)\left[\sum_{n=0}^{\infty}(n+(1-x))^{s-1}+e^{-i\pi(s-1)}\sum_{n=0}^{\infty}(n+x)^{s-1}\right]\label{eq:}\end{equation}

\end_inset 

 where we were mindful to rotate counter-clockwise for 
\begin_inset Formula $n<0$
\end_inset 

 when replacing 
\begin_inset Formula $(-)^{n}$
\end_inset 

 by 
\begin_inset Formula $e^{-i\pi n}$
\end_inset 

 instead of the sloppy and incorrect 
\begin_inset Formula $e^{i\pi n}$
\end_inset 

.
 Recognizing the sums as the Hurwitz Zeta, this then gives the desired result:
 
\begin_inset Formula \begin{equation}
\beta(x;s)=\frac{is}{\sin\pi s}\left[e^{-i\pi s/2}\zeta(1-s,x)-e^{i\pi s/2}\zeta(1-s,1-x)\right]\label{eq:}\end{equation}

\end_inset 

 It is straightforward to invert this and solve for 
\begin_inset Formula $\zeta$
\end_inset 

; one gets 
\begin_inset Formula \begin{equation}
\zeta(1-s,x)=\frac{1}{2s}\left[e^{-i\pi s/2}\beta(x;s)+e^{i\pi s/2}\beta(1-x;s)\right]\label{eq:}\end{equation}

\end_inset 

 thus proving the assertion that the Hurwitz zeta is an eigenfunction of
 the Bernoulli operator, with eigenvalue 
\begin_inset Formula $2^{s-1}$
\end_inset 

.
 To verify the correctness of the above steps, expand the exponentials in
 terms of their real and imaginary parts, to find that 
\layout Standard


\begin_inset Formula \begin{equation}
\zeta(z,x)=\frac{2\Gamma(1-z)}{\left(2\pi\right)^{1-z}}\left[\sin\left(\frac{\pi z}{2}\right)\sum_{n=1}^{\infty}\frac{\cos(2\pi nx)}{n^{1-z}}+\cos\left(\frac{\pi z}{2}\right)\sum_{n=1}^{\infty}\frac{\sin(2\pi nx)}{n^{1-z}}\right]\label{eq:}\end{equation}

\end_inset 

 which agrees with standard textbook presentations of the Hurwitz zeta;
 see for example 
\begin_inset LatexCommand \cite[Thm 12.6]{Apo76}

\end_inset 

.
\layout Subsection

Visualizing the Hurwitz zeta eigenfunctions
\layout Standard

Perhaps one surprising aspect of this result is that the Hurwitz zeta eigenfunct
ions appear to be smooth, since one is conditioned to expect that the only
 continuous-spectrum eigenfunctions of a Frobenius-Perron operator are fractal.
 Thus, it is worthwhile to take a few minutes to get acquainted with the
 shape and nature of the zeta.
 This section shows a number of graphs, and discusses the analytic structure
 of the eigenvectors.
 We'll see that the eigenfunctions are 
\begin_inset Formula $C^{\infty}$
\end_inset 

 in 
\begin_inset Formula $x$
\end_inset 

 for almost all 
\begin_inset Formula $x$
\end_inset 

: everywhere except at the endpoints 
\begin_inset Formula $x=0,1$
\end_inset 

.
 Thus, these are not 
\begin_inset Quotes eld
\end_inset 

physical
\begin_inset Quotes erd
\end_inset 

 eigenstates, if 
\begin_inset Formula $C^{\infty}$
\end_inset 

 for all 
\begin_inset Formula $x$
\end_inset 

 is placed as a demand for being 
\begin_inset Quotes eld
\end_inset 

physical
\begin_inset Quotes erd
\end_inset 

.
 We also find that there are eigenfunctions that have eigenvalues greater
 than one; these, while quite smooth and differentiable, are not square-integrab
le: they are divergent at 
\begin_inset Formula $x=0,1$
\end_inset 

.
 However, in all other respects, the eigenfunctions are analytically well-behave
d, even if a bit 
\begin_inset Quotes eld
\end_inset 

lumpy
\begin_inset Quotes erd
\end_inset 

 and uneven.
 
\layout Standard

There is a countably infinite degeneracy of eigenfunctions for a give eigenvalue.
 We can see this by writing 
\begin_inset Formula $s=\sigma+i\tau$
\end_inset 

 in terms of its real and imaginary components.
 Then the eigenvalue is 
\begin_inset Formula $\lambda=2^{-s}=2^{-\sigma}\exp(-i\tau\ln2)$
\end_inset 

 and it belongs to a family of eigenvectors with 
\begin_inset Formula $\tau'=\tau+2\pi n/\ln2$
\end_inset 

 for 
\begin_inset Formula $n\in\mathbb{Z}$
\end_inset 

.
 The next five figures show some of these, graphed in various ways.
\layout Standard


\begin_inset Float figure
wide false
collapsed true

\layout Caption

Real Part of Symmetric Beta
\layout Standard


\begin_inset Graphics
	filename zeta-2.2345-04.png
	width 100text%

\end_inset 


\layout Standard

This figure illustrates a family of eigenvectors 
\begin_inset Formula $\beta(x;s)$
\end_inset 

 having the same eigenvalue.
 To be precise, the illustration shows 
\begin_inset Formula \[
\Re\left[\beta(x;s)+\beta(-x;s)\right]/2\left|\Gamma(1+s)\right|\]

\end_inset 

 for values of 
\begin_inset Formula $s=\sigma+2\pi in/\ln2$
\end_inset 

 for 
\begin_inset Formula $\sigma=2.345$
\end_inset 

 and n=0,1,2,3,4.
 Note that 
\begin_inset Formula $\left|\Gamma(1+s)\right|$
\end_inset 

gets very small as n gets large, and so the normalization brings them to
 visually comparable values.
 Other real values of 
\begin_inset Formula $\sigma$
\end_inset 

 can be understood by recalling that 
\begin_inset Formula $\beta$
\end_inset 

 essentially interpolates between Bernoulli polynomials at integer values
 of 
\begin_inset Formula $\sigma$
\end_inset 

.
 In short, they'll all look more or less like this.
 
\layout Standard

XXX redo this graph to show 
\begin_inset Formula $\Re\left[\beta(x;s)+\beta(1-x;s)\right]/2\left|\Gamma(1+s)\right|$
\end_inset 

 instead XXX
\end_inset 


\layout Standard


\begin_inset Float figure
wide false
collapsed true

\layout Caption

Magnitude of Symmetric Beta
\layout Standard


\begin_inset Graphics
	filename zeta-abs-2.2345-04.png
	width 100text%

\end_inset 


\layout Standard

This figure illustrates a family of eigenvectors 
\begin_inset Formula $\beta(x;s)$
\end_inset 

 having the same eigenvalue.
 To be precise, the illustration shows 
\begin_inset Formula \[
\left|\beta(x;s)+\beta(-x;s)\right|/2\left|\Gamma(1+s)\right|\]

\end_inset 

 for values of 
\begin_inset Formula $s=\sigma+2\pi in/\ln2$
\end_inset 

 for 
\begin_inset Formula $\sigma=2.345$
\end_inset 

 and n=0,1,2,3,4.
 Note that 
\begin_inset Formula $\left|\Gamma(1+s)\right|$
\end_inset 

gets very small as n gets large, and so the normalization brings them to
 visually comparable values.
 
\layout Standard

XXX redo this graph to show 
\begin_inset Formula $\left|\beta(x;s)+\beta(1-x;s)\right|/2\left|\Gamma(1+s)\right|$
\end_inset 

 instead XXX
\end_inset 


\layout Standard


\begin_inset Float figure
wide false
collapsed true

\layout Caption

Real part of Beta
\layout Standard


\begin_inset Graphics
	filename zeta-exp-2.2345-04.png
	width 100text%

\end_inset 


\layout Standard

This figure illustrates a family of eigenvectors 
\begin_inset Formula $\beta(x;s)$
\end_inset 

 having the same eigenvalue.
 To be precise, the illustration shows 
\begin_inset Formula \[
\Re\beta(x;s)/2\left|\Gamma(1+s)\right|\]

\end_inset 

 for values of 
\begin_inset Formula $s=\sigma+2\pi in/\ln2$
\end_inset 

 for 
\begin_inset Formula $\sigma=2.345$
\end_inset 

 and n=0,1,2,3,4.
 Although these curves clearly look very lumpy, they are 
\begin_inset Formula $C^{\infty}$
\end_inset 

 for all 
\begin_inset Formula $x\in(0,1)$
\end_inset 

 but not at the endpoints 
\begin_inset Formula $x=0,1$
\end_inset 

.
 At the endpoints, the derivative becomes divergent after just a few derivatives
, where the curves are behaving essentially as 
\begin_inset Formula $x^{s-1}$
\end_inset 

 and 
\begin_inset Formula $(1-x)^{s-1}$
\end_inset 

.
 It is impossible to see this pending divergence in the graphs above, because,
 to the naked eye, a graph of, for example, 
\begin_inset Formula $x^{1.5}$
\end_inset 

 is nearly indistinguishable from a graph of 
\begin_inset Formula $x^{2}$
\end_inset 

.
 
\layout Standard

Although these curves appear to be sine-wave-like, it is perhaps more correct
 to think of them as being Bernoulli-polynomial-like.
 That is, to better understand what eigenvectors near some arbitrary value
 of 
\begin_inset Formula $s$
\end_inset 

 look like, its useful to think of what the polynomial 
\begin_inset Formula $B_{\left\lfloor \Re s\right\rfloor }(x)$
\end_inset 

 looks like.
 Recall, however, that, of course, 
\begin_inset Formula $B_{k}(x)$
\end_inset 

 for 
\begin_inset Formula $k\geq3$
\end_inset 

 is very sine-wave like! Note also that the first curve shown above, for
 
\begin_inset Formula $n=0$
\end_inset 

, generally resembles 
\begin_inset Formula $B_{2}(x)$
\end_inset 

 which is a parabola.
\end_inset 


\layout Standard


\begin_inset Float figure
wide false
collapsed true

\layout Caption

Magnitude of Beta 
\layout Standard


\begin_inset Graphics
	filename zeta-emag-2.2345-04.png
	width 100text%

\end_inset 


\layout Standard

This figure illustrates a family of eigenvectors 
\begin_inset Formula $\beta(x;s)$
\end_inset 

 having the same eigenvalue.
 To be precise, the illustration shows 
\begin_inset Formula \[
\left|\beta(x;s)\right|/2\left|\Gamma(1+s)\right|\]

\end_inset 

 for values of 
\begin_inset Formula $s=\sigma+2\pi in/\ln2$
\end_inset 

 for 
\begin_inset Formula $\sigma=2.345$
\end_inset 

 and n=0,1,2,3,4.
 Note that curiously, these functions seem to be smoother for larger x and
 seem to have vanishing ripples as x approaches zero.
 Curiously, the ripples seem to have a period of oscillation of approximately
 
\begin_inset Formula $nx$
\end_inset 

 in the interval 
\begin_inset Formula $x\in\left[1/2,1\right]$
\end_inset 

, and then repeating again between 1, 1/2, 1/4, 1/8, ...
 We'll gain some insight into these ripples in a later section, where we
 will analyze a sawtooth map having the same oscillatory behavior, for which
 the Hurwitz Zeta also plays a role as an eigenvector.
 That is, there is a certain sense in which the above curves are self-similar,
 with the curve in the interval 
\begin_inset Formula $x\in\left[2^{-k-1},2^{-k}\right]$
\end_inset 

 reprises the curve in the interval 
\begin_inset Formula $x\in\left[1/2,1\right]$
\end_inset 


\end_inset 


\layout Standard


\begin_inset Float figure
wide false
collapsed true

\layout Caption

Argument of Beta
\layout Standard


\begin_inset Graphics
	filename zeta-arg-2.2345-04.png
	width 100text%

\end_inset 


\layout Standard

This figure illustrates a family of eigenvectors 
\begin_inset Formula $\beta(x;s)$
\end_inset 

 having the same eigenvalue.
 To be precise, the illustration shows 
\begin_inset Formula \[
\arg\beta(x;s)/2\left|\Gamma(1+s)\right|\]

\end_inset 

 for values of 
\begin_inset Formula $s=\sigma+2\pi in/\ln2$
\end_inset 

 for 
\begin_inset Formula $\sigma=2.345$
\end_inset 

 and n=0,1,2,3,4.
 
\end_inset 

 
\layout Standard

There are eigenfunctions with eigenvalues greater than one, essentially
 because the Hurwitz zeta can be analytically continued to everywhere on
 the complex plane except for a simple pole at 
\begin_inset Formula $z=1$
\end_inset 

.
 Examining these eigenfunctions, one quickly discovers that these are not
 square-integrable: they have singularities located at 
\begin_inset Formula $x=0,1$
\end_inset 

.
 That is, for for 
\begin_inset Formula $\Re z>0$
\end_inset 

, the Hurwitz zeta 
\begin_inset Formula $\zeta(z,x)$
\end_inset 

 has a clear singularity 
\begin_inset Formula $x^{-z}$
\end_inset 

 at 
\begin_inset Formula $x=0$
\end_inset 

.
 We remove this explicitly, and write 
\begin_inset Formula \begin{eqnarray}
\frac{\sin\pi s}{is}\beta(x;s) & = & \frac{e^{-i\pi s/2}}{x^{1-s}}-\frac{e^{i\pi s/2}}{(1-x)^{1-s}}+\nonumber \\
 &  & e^{-i\pi s/2}\left(\zeta(1-s,x)-x^{s-1}\right)-e^{i\pi s/2}\left(\zeta(1-s,1-x)-(1-x)^{s-1}\right)\label{eq:}\end{eqnarray}

\end_inset 

 The first part of the equation above encapsulates the singularities at
 
\begin_inset Formula $x=0,1$
\end_inset 

 that occur when working with eigenvalues 
\begin_inset Formula $\left|\lambda\right|=\left|2^{-s}\right|>1$
\end_inset 

, that is, with 
\begin_inset Formula $\Re s<0$
\end_inset 

.
 The remaining term is well-behaved and is shown in figure 
\begin_inset LatexCommand \ref{cap:The-non-singular-part}

\end_inset 

.
 
\layout Standard


\begin_inset Float figure
wide false
collapsed true

\layout Caption


\begin_inset LatexCommand \label{cap:The-non-singular-part}

\end_inset 

The Non-Singular Part of the Divergent Eigenfunctions
\layout Standard


\begin_inset Graphics
	filename zeta-diverge.png
	width 100text%

\end_inset 


\layout Standard

This figure shows 
\begin_inset Formula \[
\eta_{even}(x;\sigma)=\frac{\cos\pi\sigma/2}{\sigma}\left[\beta(x;\sigma)+\beta(1-x;\sigma)\right]-x^{\sigma-1}-(1-x)^{\sigma-1}\]

\end_inset 

 and 
\begin_inset Formula \[
\eta_{odd}(x;\sigma)=\frac{\sin\pi\sigma/2}{\sigma}\left[\beta(x;\sigma)-\beta(1-x;\sigma)\right]-x^{\sigma-1}+(1-x)^{\sigma-1}\]

\end_inset 

 for a value of 
\begin_inset Formula $\sigma=-3.3$
\end_inset 

, corresponding to an eigenvalue of 
\begin_inset Formula $9.85=2^{3.3}$
\end_inset 

.
 Except for the singularity, we see that the finite part of these eigenfunctions
 is very well behaved.
 
\end_inset 

 
\layout Standard

Note that when 
\begin_inset Formula $\left|\lambda\right|=\left|2^{-s}\right|<1/2$
\end_inset 

, that is, when 
\begin_inset Formula $\Re s>1$
\end_inset 

, there is no singularity, and 
\begin_inset Formula $\beta(x;s)$
\end_inset 

 is finite on the entire interval 
\begin_inset Formula $x\in[0,1]$
\end_inset 

 including the endpoints.
 For 
\begin_inset Formula $1/2<\Re s\leq1$
\end_inset 

 there is a bit of funny-business at the endpoints, that is, there is a
 weak divergence there, but the function overall remains square-integrable.
 Things break loose after that, with the exception of 
\begin_inset Formula $s=0$
\end_inset 

, where we have 
\begin_inset Formula $\beta(x;0)=-1$
\end_inset 

, a constant independent of 
\begin_inset Formula $x$
\end_inset 

.
 This essentially follows from the nature of differentiation on the Bernoulli
 polynomials, which we'll see below.
 Note, however, that for 
\begin_inset Formula $s$
\end_inset 

 near zero, the function 
\begin_inset Formula $\beta(x;s)$
\end_inset 

 has severe ringing artifacts in 
\begin_inset Formula $x$
\end_inset 

, suffering from a variation of Gibbs Phenomenon.
 
\layout Standard


\begin_inset Float figure
wide false
collapsed true

\layout Caption

Ringing
\layout Standard


\begin_inset Graphics
	filename zeta-gibbs.png
	width 100text%

\end_inset 


\layout Standard

This figure shows ringing/Gibbs phenomenon as 
\begin_inset Formula $s$
\end_inset 

 approaches zero.
 In the limit of 
\begin_inset Formula $s=0$
\end_inset 

, we expect the real part of 
\begin_inset Formula $\beta$
\end_inset 

 to approach the trivial eigenfunction 
\begin_inset Formula $\lim_{s\rightarrow0^{+}}\Re\beta(x;s)=-B_{0}(x)=-1$
\end_inset 

.
 As this graph shows, the function is indeed trying very desperately to
 get flat, with not much success.
 The ringing occurs only at 
\begin_inset Formula $s=0$
\end_inset 

; there is no problem with convergence near larger integers, where 
\begin_inset Formula $\lim_{s\rightarrow n}\Re(-i)^{s}\beta(x;s)=-B_{n}(x)$
\end_inset 

 converges very smoothly and cleanly.
\end_inset 


\layout Standard

We conclude by noting that 
\begin_inset Formula $\beta$
\end_inset 

 is 
\begin_inset Formula $C^{\infty}$
\end_inset 

 for 
\begin_inset Formula $x\in(0,1)$
\end_inset 

 but not at the endpoints 
\begin_inset Formula $x=0,1$
\end_inset 

.
 This can be easily seen by writing the derivative 
\begin_inset Formula \begin{equation}
\frac{d}{dx}\beta(x;s)=2\pi i\beta(x;s-1)\label{eq:}\end{equation}

\end_inset 

 and so even if we start with 
\begin_inset Formula $\Re s>1$
\end_inset 

, each derivative carries us one step closer into the danger zone.
 
\layout Section

The Kernel
\layout Standard

What is the kernel of 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

? It is the set of functions that have only odd Fourier terms.
 
\layout Standard

That is, for any integer 
\begin_inset Formula $k\in\mathbb{Z}$
\end_inset 

 we have 
\begin_inset Formula $\mathcal{L}_{B}\cos2\pi(2k+1)x=0$
\end_inset 

 and so we write 
\begin_inset Formula $\cos2\pi(2k+1)x\in K\left[\mathcal{L}_{B}\right]$
\end_inset 

 and likewise 
\begin_inset Formula $\sin2\pi(2k+1)x\in K\left[\mathcal{L}_{B}\right]$
\end_inset 

.
 
\layout Standard

This implies that 'half' of all square-integrable functions are in the kernel.
 This is a huge space.
 The quotient space of the implied isomorphism thus has the Bernoulli polynomial
s as the representative elements.
 This is I think the correct way to relate coordinate space to the Hilbert
 space, is by means of the quotient space generated by the kernel of the
 time-evolution operator.
 
\layout Section

The Continuous Fractal Spectrum
\layout Standard

An alternate set of eigenvectors with a continuous spectrum are given by
 
\begin_inset Formula \begin{equation}
\phi_{z,k}(x)=\sum_{n=0}^{\infty}z^{n}\exp\left(2\pi i\;2^{n}\left(2k+1\right)x\right)\label{eq:}\end{equation}

\end_inset 

 and have eigenvalue 
\begin_inset Formula $z$
\end_inset 

: that is 
\begin_inset Formula $[U_{B}\phi_{z,k}](x)=z\phi_{z,k}(x)$
\end_inset 

.
 Again, for a given fixed eigenvalue, they have a countably infinite degeneracy,
 labelled by the parameter 
\begin_inset Formula $k\in\mathbb{Z}$
\end_inset 

.
 These eigenfunctions are fractal, as can be readily seen from the graph[xxx
 need figure].
 Since they are a generalization of the Takagi-Landsberg curve, they have
 an 
\begin_inset Formula $SL(2,\mathbb{Z})$
\end_inset 

 symmetry, as discussed in an earlier chapter.
 As is typical for Takagi-type curves, these eigenfunctions are differentiable
 a finite number of times before they become differentiable nowhere.
 For example, for 
\begin_inset Formula $1/2<\left|z\right|<1$
\end_inset 

, these are continuous with respect to 
\begin_inset Formula $x$
\end_inset 

 but nowhere differentiable.
 For 
\begin_inset Formula $1/2^{m+1}<\left|z\right|<1/2^{m}$
\end_inset 

, these are everywhere 
\begin_inset Formula $m$
\end_inset 

 times differentiable with respect to 
\begin_inset Formula $x$
\end_inset 

, but nowhere 
\begin_inset Formula $m+1$
\end_inset 

 times differentiable.
 
\layout Standard

We can express these in terms of the Hurwitz Zeta eigenfunctions by considering
 the sum 
\begin_inset Formula \begin{eqnarray}
\sum_{k=0}^{\infty}z^{\ln_{2}(2k+1)}\phi_{z,k}(x) & = & \sum_{n=1}^{\infty}z^{\ln_{2}n}\exp\left(2\pi inx\right)\nonumber \\
 & = & \sum_{n=1}^{\infty}n^{\ln_{2}z}\exp\left(2\pi inx\right)\label{eq:}\end{eqnarray}

\end_inset 

 Thus, we see that we should equate 
\begin_inset Formula $s=-\ln_{2}z$
\end_inset 

 so that the eigenvalues are 
\begin_inset Formula $z=2^{-s}$
\end_inset 

.
 Multiplying by the appropriate factors, we get the desired relationship
 
\begin_inset Formula \begin{equation}
\beta(x;s)=2\Gamma(s+1)\left(2\pi\right)^{-s}\sum_{k=0}^{\infty}\left(2k+1\right)^{-s}\phi_{z,k}(x)\label{eq:}\end{equation}

\end_inset 

 That is, the Hurwitz zeta eigenfunctions are expressible as a linear combinatio
n of the fractal eigenfunctions.
 Essentially, either set of eigenfunctions can be used to form a set of
 basis states for the Bernoulli map transfer operator.
 The Hurwitz zeta eigenfunctions span a larger space than the fractal eigenfunct
ions, as the Hurwitz zeta is well-defined for eigenvalues with 
\begin_inset Formula $\left|z\right|>1$
\end_inset 

, whereas the fractal eigenfunctions are not.
 Of course, as we saw above, the Hurwitz zeta eigenfunctions are not square-inte
grable when 
\begin_inset Formula $\left|z\right|>1$
\end_inset 

, which invalidates their consideration for most 
\begin_inset Quotes eld
\end_inset 

physical
\begin_inset Quotes erd
\end_inset 

 uses.
 Note also that through careful work, the fractal eigenfunctions can probably
 be extended to 
\begin_inset Formula $\left|z\right|>1$
\end_inset 

 continuous-nowhere functions by considering their transformation properties
 under 
\begin_inset Formula $SL(2,\mathbb{Z})$
\end_inset 

.
 This would be in analogy to the exploration of the derivative of the Takagi
 curve, which, as we saw in an earlier chapter, can be defined as the Cantor
 polynomial, built out of the digits of the binary expansion of 
\begin_inset Formula $x$
\end_inset 

.
 
\layout Standard

We can explicitly demonstrate the change of basis by defining 
\begin_inset Formula \begin{equation}
\beta_{n}(x;s)\equiv\beta(x;s+2\pi ni/\ln2)\label{eq:}\end{equation}

\end_inset 

 which all share the same eigenvalue: 
\begin_inset Formula $U_{B}\beta_{n}=2^{-s}\beta_{n}$
\end_inset 

.
 We can then restrict 
\begin_inset Formula $s$
\end_inset 

 to a principle domain 
\begin_inset Formula $-\pi<\Im s\,\ln2=\arg\, z<\pi$
\end_inset 

 .
 The change of basis can now be written explicitly as 
\begin_inset Formula \begin{equation}
\beta_{n}(x;s)=\sum_{k=0}^{\infty}F_{nk}\phi_{z,k}(x)\label{eq:}\end{equation}

\end_inset 

 where the matrix elements are 
\begin_inset Formula \begin{equation}
F_{nk}=2\Gamma\left(s+1+\frac{2\pi ni}{\ln2}\right)\left(2\pi(2k+1)\right)^{-s}\exp\left[-2n\pi i\frac{\ln\pi(2k+1)}{\ln2}\right]\label{eq:}\end{equation}

\end_inset 

 Presumably 
\begin_inset Formula $F$
\end_inset 

 is invertible; either set of eigenstates span the space.
 Given that one set of basis functions are clearly fractal, while the other
 is clearly analytic, it would be interesting to describe the space of functions
 spanned by these basis states.
 That is, given an arbitrary sequence 
\begin_inset Formula $\{ b_{n}|b_{n}\in\mathbb{C}\}$
\end_inset 

, describe the nature of the functions 
\begin_inset Formula \begin{equation}
\sum_{n=0}^{\infty}b_{n}\beta_{n}(x;s)\label{eq:}\end{equation}

\end_inset 

 considered as functions of 
\begin_inset Formula $x$
\end_inset 

 or alternately of 
\begin_inset Formula $s$
\end_inset 

.
 
\layout Standard

The modular group symmetries of the fractal eigenfunctions do not seem to
 provide any interesting insight into the zeta, since they do not mix or
 permute eigenstates.
 For example, applying the generator 
\begin_inset Formula $g$
\end_inset 

 on the fractal eigenstates gives
\begin_inset Formula \begin{equation}
g\phi_{zk}(x)=\phi_{zk}\left(\frac{x}{2}\right)=\exp((2k+1)\pi ix)+z\phi_{zk}(x)\label{eq:}\end{equation}

\end_inset 

 and so one might hope that since the zetas are a linear combination of
 the fractal eigenfunctions, one might get some new insight.
 However, doing this gives the sum 
\layout Standard


\begin_inset Formula \begin{equation}
g\beta(x;s)=\beta\left(\frac{x}{2};s\right)=\frac{2\Gamma(s+1)}{(2\pi)^{s}}\sum_{k=0}^{\infty}\frac{\exp2\pi ix(2k+1)}{(2k+1)^{s}}+2^{-s}\beta(x;s)\label{eq:}\end{equation}

\end_inset 

 as a symmetry, but the evaluation of the sum in the middle yields 
\begin_inset Formula $\beta(x/2;s)-2^{-s}\beta(x;s)$
\end_inset 

 and so one gets a trivial relationship and no insight in particular.
\layout Section

Modular Group Symmetry and the Takagi Representation 
\layout Standard

Yet another way to understand the solution of the Bernoulli Map is to note
 that it can be written as 
\begin_inset Formula \begin{equation}
\left[\mathcal{L}_{B}f\right](x)=\frac{1}{2}\left(f(g_{D}(x))+f(r_{D}g_{D}r_{D}(x)\right)\label{eq: bern modular}\end{equation}

\end_inset 

 where 
\begin_inset Formula $g_{D}(x)=x/2$
\end_inset 

 and 
\begin_inset Formula $r_{D}(x)=1-x$
\end_inset 

 are the generators of the dyadic representation of the Modular group 
\begin_inset Formula $SL(2,\mathbb{Z})$
\end_inset 

.
 Thus, we expect that we should be able to build eigenfunctions out of any
 functions 
\begin_inset Formula $f(x)$
\end_inset 

 that posses a Modular group symmetry.
\layout Standard

As we saw in a previous chapter, the Takagi curve fits this bill.
 Start with the triangle wave/tent map: 
\begin_inset Formula \begin{equation}
\tau(x)=\left\{ \begin{array}{ccc}
2(x-\left\lfloor x\right\rfloor ) & \;\textrm{ when }\; & 0\leq x-\left\lfloor x\right\rfloor \leq1/2\\
2(1-x+\left\lfloor x\right\rfloor ) & \;\textrm{ when }\; & 1/2\leq x-\left\lfloor x\right\rfloor \leq1\end{array}\right.\label{eq:}\end{equation}

\end_inset 

 which has the property that it doubles under iteration: 
\begin_inset Formula $\tau^{k}(x)=\tau(2^{k-1}x)$
\end_inset 

.
 The iterated tent map behaves sort-of like a shift state, in that 
\begin_inset Formula \begin{equation}
\left[\mathcal{L}_{B}\tau^{k}\right](x)=\tau^{k-1}(x)\label{eq:}\end{equation}

\end_inset 

 although it does not terminate properly for a shift state: 
\begin_inset Formula \begin{equation}
\left[\mathcal{L}_{B}\tau\right](x)=\frac{1}{2}\label{eq:}\end{equation}

\end_inset 

 (a true shift state would vanish on the final iteration).
 Thus we see that the Takagi curve transforms as 
\begin_inset Formula \begin{equation}
\left[\mathcal{L}_{B}t_{w}\right](x)=\frac{1}{2}+wt_{w}(x)\label{eq:}\end{equation}

\end_inset 

 under the Bernoulli operator.
 We can use this to build an eigenfunction 
\begin_inset Formula \begin{equation}
b_{w}(x)=\frac{-1}{2(1-w)}+t_{w}(x)\label{eq:}\end{equation}

\end_inset 

 so that 
\begin_inset Formula $\mathcal{L}_{B}b_{w}=wb_{w}$
\end_inset 

.
 On closer examination, we can see that this eigenvalue has the same countable
 degeneracy we've seen previously.
 That is, we can replace 
\begin_inset Formula $x$
\end_inset 

 on the right-hand-side of the equations above by 
\begin_inset Formula $(2j+1)x$
\end_inset 

 for 
\begin_inset Formula $j\in\mathbb{N}$
\end_inset 

 to obtain the set of eigenfunctions
\begin_inset Formula \begin{equation}
b_{w,j}(x)=\frac{-1}{2(1-w)}+t_{w}((2j+1)x)\label{eq:}\end{equation}

\end_inset 

 all sharing the eigenvalue 
\begin_inset Formula $w.$
\end_inset 

 It follows immediately that, just as above, we should be able to express
 the Hurwitz Zeta eigenfunctions 
\begin_inset Formula $\beta_{n}(x;s)$
\end_inset 

 for fixed 
\begin_inset Formula $s=-\ln_{2}w$
\end_inset 

 as a linear combination of the 
\begin_inset Formula $b_{w,j}$
\end_inset 

 if the 
\begin_inset Formula $b_{w,j}$
\end_inset 

 are a complete set of eigenstates for the eigenvalue 
\begin_inset Formula $w$
\end_inset 

.
 One special case is immediately apparent: the Takagi Curve 
\begin_inset Formula $t_{1/4}(x)$
\end_inset 

 is a parabola, corresponding to the Bernoulli polynomial 
\begin_inset Formula $B_{2}(x)$
\end_inset 

.
\layout Standard

XXX finish me; give the explicit expressions between 
\begin_inset Formula $b_{w,j}$
\end_inset 

 and 
\begin_inset Formula $\beta_{n}$
\end_inset 

.
\layout Standard

We can also obtain other eigenvectors by starting with the Takagi curves
 that transform under the higher-dimensional representations of 
\begin_inset Formula $SL(2,\mathbb{Z})$
\end_inset 

.
 The equation 
\begin_inset LatexCommand \ref{eq: bern modular}

\end_inset 

 implies that any curve that transforms under a linear representation of
 the Modular Group can be used to build an eigenfunction of the Bernoulli
 Map.
 We exclude the non-linear representations, since we don't know how to build
 a vector space out of a non-linear operator.
 Thus, if 
\begin_inset Formula $t_{w}^{(n)}((2j+1)x)$
\end_inset 

 is a Takagi curve that transforms under an 
\begin_inset Formula $n$
\end_inset 

-dimensional representation, then 
\begin_inset Formula $\mathcal{L}_{B}$
\end_inset 

 is represented by 
\begin_inset Formula $(g_{n}+r_{n}g_{n}r_{n})/2$
\end_inset 

 where 
\begin_inset Formula $r_{n},g_{n}\in GL(n,\mathbb{R})$
\end_inset 

 are the generators of that 
\begin_inset Formula $n$
\end_inset 

-dimensional representation of 
\begin_inset Formula $SL(2,\mathbb{Z})$
\end_inset 

.
 Solving the eigenvalue equation then amounts to diagonalizing the (finite-dimen
sional) matrix 
\begin_inset Formula $(g_{n}+r_{n}g_{n}r_{n})/2$
\end_inset 

.
\layout Standard

For a fixed eigenvalue 
\begin_inset Formula $w$
\end_inset 

, we expect a countably infinite degeneracy built out of the curves 
\begin_inset Formula $t_{w}^{(n)}((2j+1)x)$
\end_inset 

.
 Again, if these form a complete set of eigenstates, then we expect to be
 able to write out 
\begin_inset Formula $\beta_{n}(x;s)$
\end_inset 

 as a linear combination of these, and v.v.
\layout Standard

XXX finish me ..
 Give explicit expression for the higher-dim reps and in particular,the
 matrix from 
\begin_inset Formula $b_{w,j}$
\end_inset 

 and 
\begin_inset Formula $\beta_{n}$
\end_inset 

 in explicit detail.
 Also develop the lower-dim reps (move in opposite direction).
 This material should probably go into the chapter on the Takagi Curves,
 instead of here,right ...
 ???
\layout Standard

Note this is a ladder of isomorphisms between the different dimensional
 reps.
 Note also the ability to choose different set of basis funcs for the higher-dim
 Takagi Curves, viz, any polynomial of degree 
\begin_inset Formula $n-2$
\end_inset 

.
\layout Section

The Topological Zeta
\layout Standard

XXX ToDo: explain topo zeta gets this has this name, and why this concept
 is important.
 XXX
\layout Standard

The topological zeta of the Bernoulli operator can be computed very easily
 in the polynomial basis because we know the eigenvalues and these form
 a simple series.
 We'll define the Bernoulli topological zeta as
\begin_inset Formula \begin{equation}
\zeta_{B}(t)\equiv\frac{1}{\det\left[\mathbb{I}-t\mathcal{L}_{B}\right]}\label{eq:}\end{equation}

\end_inset 

 We start by noting its inverse: 
\begin_inset Formula \begin{eqnarray}
\det\left[\mathbb{I}-t\mathcal{L}_{B}\right] & = & \prod_{n=0}^{\infty}\left(1-t\;2^{-n}\right)\nonumber \\
 & = & 1-t\sum_{j=0}^{\infty}2^{-j}+t^{2}\sum_{j=0}^{\infty}2^{-j}\sum_{\begin{array}{c}
k=0\\
k\neq j\end{array}}^{\infty}2^{-k}-t^{3}...\nonumber \\
 & = & 1-2t+\frac{8}{3}t^{2}-\frac{16}{7}t^{3}+\frac{128}{105}t^{4}-...\label{eq:}\end{eqnarray}

\end_inset 

 Successive terms of this series are hard to compute, and it would be interestin
g to know what the generating function for this series is.
 The series appears to have a circle of convergence of radius one.
 The zeta can be computed directly by working with its logarithm: 
\begin_inset Formula \begin{eqnarray}
\log\zeta_{B}(t) & = & \log\prod_{n=0}^{\infty}\left(1-t\;2^{-n}\right)^{-1}\nonumber \\
 & = & -\sum_{n=0}^{\infty}\log\left(1-t\;2^{-n}\right)\nonumber \\
 & = & \sum_{k=1}^{\infty}\frac{t^{k}}{k}\;\frac{2^{k}}{2^{k}-1}\nonumber \\
 & = & -\log(1-t)+\sum_{k=1}^{\infty}\frac{t^{k}}{k}\;\frac{1}{2^{k}-1}\label{eq:}\end{eqnarray}

\end_inset 

 Thus we have 
\begin_inset Formula $\textrm{Tr}\mathcal{L}_{B}^{k}=2^{k}/(2^{k}-1)$
\end_inset 

.
 Of some curiosity is the proximity of the Erdos-Borwein constant: 
\begin_inset Formula \begin{eqnarray}
1.6066... & = & \sum_{n=1}^{\infty}\frac{1}{2^{n}-1}\nonumber \\
 & = & \sum_{n=1}^{\infty}\frac{d(n)}{2^{n}}\label{eq:}\end{eqnarray}

\end_inset 

 which marks the first appearance of a classical number-theoretic function
 in the proceedings so far: 
\begin_inset Formula $d(n)$
\end_inset 

 is the number of divisors of 
\begin_inset Formula $n$
\end_inset 

.
 This arises from the Lambert series
\begin_inset Formula \begin{equation}
\sum_{n=1}^{\infty}d(n)x^{n}=\sum_{n=1}^{\infty}\frac{x^{n}}{1-x^{n}}\label{eq:}\end{equation}

\end_inset 

The sum 
\begin_inset Formula \begin{equation}
E(t)=\sum_{k=1}^{\infty}\frac{t^{k}}{1-2^{-k}}\label{eq:}\end{equation}

\end_inset 

 can be re-summed as a Lambert series, namely, 
\begin_inset Formula \begin{equation}
E(t)=\sum_{k=1}^{\infty}b_{k}2^{-k}\label{eq:}\end{equation}

\end_inset 

 where 
\begin_inset Formula \begin{equation}
b_{k}=\sum_{n|k}(2t)^{n}\label{eq:}\end{equation}

\end_inset 

 The analytic/meromorphic structure of this zeta is not clear; its dull
 within the unit disk, and its not quite obvious what the continuation is
 outside of the disk.
 XXX ToDo: get the full analytic structure.
\layout Section

Curiosities
\layout Standard

We list here some intriguing forms that suggest further relationships.
\layout Standard

The Pochhammer symbol 
\begin_inset Formula $(a)_{n}=\Gamma(a+n)/\Gamma(n)$
\end_inset 

 obeys a 
\emph on 
dimidiation formula
\emph default 
 that is reminiscent of the Bernoulli map:
\layout Standard


\begin_inset Formula \begin{eqnarray*}
(a)_{2n} & = & 2^{2n}\left(\frac{a}{2}\right)_{n}\left(\frac{a+1}{2}\right)_{n}\\
(a)_{2n+1} & = & 2^{2n+1}\left(\frac{a}{2}\right)_{n}\left(\frac{a+1}{2}\right)_{n}\end{eqnarray*}

\end_inset 


\layout Section

Conclusions
\layout Standard

Apologies for the format of this paper.
 It's a veritable candy store of goodies; there are all these yummy toys
 to play with, which one first?
\begin_inset LatexCommand \BibTeX[plain]{/home/linas/linas/fractal/paper/fractal}

\end_inset 


\the_end
