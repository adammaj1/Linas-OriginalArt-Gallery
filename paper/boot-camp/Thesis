
A combination of increasing compute power and commercial utility will
attract new attention to the field of corpus linguistics, and lead an
invasion of computational linguistics principles. Acess to compute
resources allows large data-driven experiments to be performed using large
collections of linguistic data, while commercial utility introduces a 
pragmatic engineering ethos where workable methods and practical results 
are more important than theoretical debates.  The corpus provides the 
fertile ground on which this can take place.

Half a century of computational linguistics has revealed that
building AI/NLP systems by hand is a difficult and seemingly 
insurmountable task. However, the process of hand-building seems understood
well enough that parts of it can be automated -- that is, grammars can
be learned by machine-learning systems, and semantic content can be 
automatically discerned.  The availability of a large corpus appears to
be crucial for this task.  In this sense, the distinction of corpus
linguistics and computational linguistics is blurred[1,2].  

The use of experimental techniques to discover new linguistic facts 
hidden in large corpora have provided many new discoveries. However,
some[3,8] suggest that there are even stronger ways to discover linguistic
and semantic structure: "linguistics by embodiment".  For humans, it is
easier to learn something by doing it, as opposed to just reading about
it.  Similarly, the embodiment hypothesis is that a software agent can
more easily learn semantic communication by having a virtual, 3D body
allowing linguistic behaviours to be correlated with visual and motor 
senses. This raises important theoretical and philosophical questions 
about "concepts" and "meaning": Is meaning confined to the evidence
attainable from a corpus, or is there a sublime aspect of meaning that
is accessible only by doing, feeling, sensing? In humans, internal
feelings and sensations are not directly accessible to outside observers,
and so such debates can be rendered moot by lack of evidence. The conceit
of corpus linguistics, that meaning is negotiated between speakers[4,5], 
and is located only in the discourse[6], has reigned unchallenged.  But
what happens when one has access to a robotic linguistic actor, whose 
internal states can be probed and examined? This possiblity even turns 
corpus linguistics on its head: does it make sense to collect and analyze
a corpus of humans talking to automated bankers and pizza ordering
systems?[7] Surely, the fact that the software programming for the pizza
ordering system is available seems to obviate the need to sample its
utterances. And yet, a sampling of the confusions and digressions that
occur during chat between humans and robotic systems can be commericially
valuable: the efficiency and accuracy of these systems can be improved
by studying how communications breaks down.

The goal of the rest of this screed is to sketch not only how linguistic
analysis is performed automatically, but how deeply layered and 
abstract it has become. Two points emerge: collocation and corpus are
key elements, and that forward progress requires clever observation of 
subtle linguistic phenomena by human linguists.

Collocation from a Mathematical Point of View
----------------------------------------------
The starting point for automated linguistic learning is collocation.
One of the simplest analysis one can perform is to discover important 
bigrams by means of mutual information[9]. While a simple count of
probabilities might show which word pairs (or N-grams) occur frequently, 
mutual information gives a different view: it uncovers idiomatic
expressions and "set phrases": collections of words that are used 
together in a relatively invariant fashion. Examples of high mutual
information word pairs are 'Johns Hopkins' and 'jet propulsion', both
scoring an MI of about 15 in one dataset[10].  The most striking 
result from looking over lists of high-mutual-information word pairs
is that most are the names of conceptual entities.

Mutual information between word pairs provides the starting point for
a class of parsers such as the Minimum Spanning Tree Parser[11]. These
parsers are typically dependency parsers that discover dependency
relations using only "unsupervised" training. The key word here is 
"unsupervised": rather than providing the learning algorithm with 
linguist-annotated datasets, the parser "learns" merely by observing
text "au naturel", without annotations.  This is in contrast to many
other parsers built on statistical techniques, which require an
annotated corpus as input.  By eschewing annotation, it stays true 
to one of the core ideals of corpus linguistics: "trust the text".

Sometimes it is imagined that statistical techniques stop here: anyone
who has played with (or read about) Markov chains built from N-grams
know that such systems are adept at generating quasi-grammatical nonsense
text.  But this does not imply that machine learning techniques cannot
advance further: it merely means that a single level of analysis has
only limited power, and must be aided by further layers providing
additional structure and constraints.  So, for example, although a
naive Markov chain might allow nonsense word combinations, a higher
order structure discriminator would note that certain
frequently-generated patterns are rarely or never observed in an
actual corpus, and prohibit their generation (alternatively, balk at 
creating a parse when presented with such ungrammatical input).

Another important point is that the observation of structures in
corpora can be used to validate "mentalistic" theories of grammar.
The whole point of traditional, hand-built parsers was to demonstrate
that one could model certain high-frequency word occurances with a
relatively small set of rules -- that these rules can differentiate
between grammatical and ungrammatical text. Thus, for example, high 
mutual information word pairs help validate the "mentalistic" notion
of a "dependency parser".  But this is a two way street: theories 
based on practical experience can serve as a guide for the kinds of
structures and patterns that one might be able to automaitcally mine,
using unsupervised techniques, from a text corpus. Thus, for example, 
the "Meaning-Text Theory" developed by Igor Mel'cuk and others[12, 13]
provides a description of "set phrases", deep semantic structures, and 
of the interesting notion of "lexical functions". Perhaps one might be
able to discover these by automated means, and conversely, use these 
automatically discovered structures to perform more accurate parsing
and even analysis of semantic content.

Such attempts at automated, unsupervised learning of deeper structure is
already being done, albeit without any theoretical baggage in tow.
Prominent examples include Dekang Lin's work on the automatic discovery
of synonymous phrases ("Discovery of Inference Rules from Text" [14]), 
and Poon & Domingos extension of these ideas to the automated discovery
of synonymous relations of higher arity ("Unsupervised Semantic Parsing"[15]).
The earlier work by Lin applies a fairly straight-forward statistical
analysis to discover co-occuring phrases and thus deduce their synonymy.
However, it is important to note that Lin's analysis is
"straight-forward" only because a certain amount of heavy lifting was
already performed by parsing the input corpus.  That is, rather than
applying statistical techniques on raw N-grams, the text is first
analized with a dependency parser; it is the output of the parser that
is subject to statistical analysis.  This provides an example of the
kind of "layered" approach that seems to be necessary to tease out the
deeper structures in text -- it is not possible to define a single,
simple statistical measure that will magically reveal hidden structure
in text. Indeed, the complexity of techniques can quickly escalate:
although the work of Poon & Domingos can be seen as a kind of
generalization of that of Lin, it employs a considerably more dense and
intellectually challenging mathematical framework to do so.
Nonetheless, it takes as its core the idea of "trust the text" --
there are no ad-hoc English-language-specific tweaks in the code, and,
indeed, almost no linguistic theory whatsoever, beyond the core
requirement that dependency parsing be done first.

Although the above tries to paint a continuous chain of analysis,
starting with a corpus, passing through mutual information to a
dependency parser, from which further relationships are teased out,
there are certanly other paths.  Phrase-structure-driven parsers can
also be learned by statistical techniques, although the *unsupervised*
discovery of such grammars is more mathematically challenging (i.e.
requiring a deeper understanding of topics in computer science). That
"large scale knowledge extraction" is possible from such grammars is
demonstrated by Micheal and Valiant[16].

A major blot on the above arguments is that the nature of the driving
force behind the results is the urge to discover some practical, usable,
functional technique for automatically discovering and manipulating
"knowledge". This is a laudable goal, for doing so will presumably shed
light on the nature of "meaning" and "concepts". Yet, in the mad rush
towards automated knowledge extraction and manipulation, finer and more 
subtle linguistic phenomena are left on the wayside. The overall attack
is still brute-force: the knowledge to be extracted is often no deeper
than an analysis of the sentence "Aristotle is a man", and often
requires little more linguistic analysis than that needed to parse 
that sentence. The genres frequently analyzed are financial or biomedical 
texts; fine literature and peotry are roundly ignored.  I only hope that 
perhaps some of these techniques can be refocused on more subtle phenomena.

The trend towards ever-more complex analysis seems inexorable. But
this does not mean that every simple, fast, easy way of analyzing text 
has been discovered. Recent work from Google and NEC labs titled
"Polynomial Semantic Indexing"[17] demonstrates how a fairly simple, 
shallow mathematical technique, which assumes no syntactic structure in
the text whatsoever, can none-the-less discover considerable amounts of
semantic content. While the technique is hardly any deeper than that of
computing co-occurance probabilities or mutual nformation, it is notable
in that it requires a miniscule amount of CPU time, as compared to the
act of parsing text.

Conclusion
----------


conclude:
connectionism



[1] http://mailman.uib.no/public/corpora/2008-August/007102.html
[2] http://mailman.uib.no/public/corpora/2008-August/007112.html
[3] Francisco J. Varela, Evan Thompson, Eleanor Rosch,
    "The embodied mind:  Cognitive science and human experience"
    MIT Press, Cambridge, MA, USA. 1991. 
[4] http://mailman.uib.no/public/corpora/2008-August/007115.html
[5] http://mailman.uib.no/public/corpora/2008-August/007089.html
[6] http://mailman.uib.no/public/corpora/2008-August/007161.html
[7] http://mailman.uib.no/public/corpora/2008-August/007166.html
[8] Rolf Pfeifer, Christian Scheier, "Understanding Intelligence", 
    MIT Press, October 1999
[9] Deniz Yuret, ''Discovery of Linguistic Relations Using
    Lexical Attraction'' (1998) PhD Thesis
[10] Linas Vepstas, "Linas' collection of NLP data", 2008 
     http://gnucash.org/linas/nlp/
[11] R. McDonald, F. Pereira, etal, "Minimum-Spanning Tree Parser"
     2005
     http://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.html
[12] @article{Mel'cuk1987,
   author = {Igor A. Mel'cuk and Alain Polguere},
   title = {A Formal Lexicon in Meaning-Text Theory},
   journal = {Computational Linguistics},
   year = 1987,
   volume = {13},
   pages = {261-275},
}
[13] @proceedings{Steele1990,
   title = {Meaning-Text Theory: Linguistics, Lexicography, and Implications},
   year = 1990,
   editor = {James Steele},
   publisher = {University of Ottowa Press},
}
[14] @inproceedings{Lin2001,
   author = {Lin, Dekang and Pantel, Patrick},
   title = {DIRT: Discovery of Inference Rules from Text},
   booktitle = {Proceedings of the Seventh ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD'01)},
   pages = {323--328},
   year = 2001,
   publisher = {ACM Press},
   doi = {10.1145/502512.502559},
}
[15] @inproceedings{Poon2009,
   author = {Poon, Hoifung and Domingos, Pedro},
   title = {Unsupervised Semantic Parsing},
   booktitle = {Proceedings of the 2009 Conference on Empirical Methods
in Natural Language Processing},
   pages = {1--10},
   year = 2009,
   address = {Singapore},
   month = {August},
   publisher = {Association for Computational Linguistics},
   url = {http://www.aclweb.org/anthology/D/D09/D09-1001},
}
[16] Loizos Michael and Leslie G. Valiant, "A First Experimental 
     Demonstration of Massive Knowledge Infusion" Proc. 11th
     International Conference on Principles of Knowledge Representation 
     and Reasoning, Sept. 16-20, 2008, Sydney, Australia, 378-389.
[17] Bing Bai, Jason Weston, et al, "Polynomial Semantic Indexing"
     ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 22
     Proceedings of the 2009 Conference

