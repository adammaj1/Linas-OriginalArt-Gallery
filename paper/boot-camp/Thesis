
A combination of increasing compute power and commercial utility will
attract new attention to the field of corpus linguistics, and lead an
invasion of computational linguistics principles. Acess to compute
resources allows large data-driven experiments to be performed using large
collections of linguistic data, while commercial utility introduces a 
pragmatic engineering ethos where workable methods and practical results 
are more important than theoretical debates.  The corpus provides the 
fertile ground on which this can take place.

Half a century of computational linguistics has revealed that
building AI/NLP systems by hand is a difficult and seemingly 
insurmountable task. However, the process of hand-building seems understood
well enough that parts of it can be automated -- that is, grammars can
be learned by machine-learning systems, and semantic content can be 
automatically discerned.  The availability of a large corpus appears to
be crucial for this task.  In this sense, the distinction of corpus
linguistics and computational linguistics is blurred[1,2].  

The use of experimental techniques to discover new linguistic facts 
hidden in large corpora have provided many new discoveries. However,
some[3,8] suggest that there are even stronger ways to discover linguistic
and semantic structure: "linguistics by embodiment".  For humans, it is
easier to learn something by doing it, as opposed to just reading about
it.  Similarly, the embodiment hypothesis is that a software agent can
more easily learn semantic communication by having a virtual, 3D body
allowing linguistic behaviours to be correlated with visual and motor 
senses. This raises important theoretical and philosophical questions 
about "concepts" and "meaning": Is meaning confined to the evidence
attainable from a corpus, or is there a sublime aspect of meaning that
is accessible only by doing, feeling, sensing? In humans, internal
feelings and sensations are not directly accessible to outside observers,
and so such debates can be rendered moot by lack of evidence. The conceit
of corpus linguistics, that meaning is negotiated between speakers[4,5], 
and is located only in the discourse[6], has reigned unchallenged.  But
what happens when one has access to a robotic linguistic actor, whose 
internal states can be probed and examined? This possiblity even turns 
corpus linguistics on its head: does it make sense to collect and analyze
a corpus of humans talking to automated bankers and pizza ordering
systems?[7] Surely, the fact that the software programming for the pizza
ordering system is available seems to obviate the need to sample its
utterances. And yet, a sampling of the confusions and digressions that
occur during chat between humans and robotic systems can be commericially
valuable: the efficiency and accuracy of these systems can be improved
by studying how communications breaks down.

The goal of the rest of this screed is to sketch not only how linguistic
analysis is performed automatically, but how deeply layered and 
abstract it has become. Two points emerge: collocation and corpus are
key elements, and that forward progress requires clever observation of 
subtle linguistic phenomena by human linguists.

Collocation from a Mathematical Point of View
----------------------------------------------
The starting point for automated linguistic learning is collocation.
One of the simplest analysis one can perform is to discover important 
bigrams by means of mutual information[9]. While a simple count of
probabilities might show which word pairs (or N-grams) occur frequently, 
mutual information gives a different view: it uncovers idiomatic
expressions and "set phrases": collections of words that are used 
together in a relatively invariant fashion. Examples of high mutual
information word pairs are 'Johns Hopkins' and 'jet propulsion', both
scoring an MI of about 15 in one dataset[10].  The most striking 
result from looking over lists of high-mutual-information word pairs
is that most are the names of conceptual entities.

Mutual information between word pairs provides the starting point for
a class of parsers such as the Minimum Spanning Tree Parser[11]. These
parsers are typically dependency parsers that discover dependency
relations using only "unsupervised" training. The key word here is 
"unsupervised": rather than providing the learning algorithm with 
linguist-annotated datasets, the parser "learns" merely by observing
text "au naturel", without annotations.  This is in contrast to many
other parsers built on statistical techniques, which require an
annotated corpus as input.  By eschewing annotation, it stays true 
to one of the core ideals of corpus linguistics: "trust the text".

Sometimes it is imagined that statistical techniques stop here: anyone
who has played with (or read about) Markov chains built from N-grams
know that such systems are adept at generating quasi-grammatical nonsense
text.  But this does not imply that machine learning techniques cannot
advance further: it merely means that a single level of analysis has
only limited power, and must be aided by further layers providing
additional structure and constraints.  So, for example, although a
naive Markov chain might allow nonsense word combinations, a higher
order structure discriminator would note that certain
frequently-generated patterns are rarely or never observed in an
actual corpus, and prohibit their generation (alternatively, balk at 
creting a parse when presented with such ungrammatical input).

What sort of stcutere might one expect to find mentalistic theoryeis
provie

unsupervised

set phrases MTT

conclude:
connectionism


So what does it mean to automatically discover new 

[1] http://mailman.uib.no/public/corpora/2008-August/007102.html
[2] http://mailman.uib.no/public/corpora/2008-August/007112.html
[3] Francisco J. Varela, Evan Thompson, Eleanor Rosch,
    "The embodied mind:  Cognitive science and human experience"
    MIT Press, Cambridge, MA, USA. 1991. 
[4] http://mailman.uib.no/public/corpora/2008-August/007115.html
[5] http://mailman.uib.no/public/corpora/2008-August/007089.html
[6] http://mailman.uib.no/public/corpora/2008-August/007161.html
[7] http://mailman.uib.no/public/corpora/2008-August/007166.html
[8] Rolf Pfeifer, Christian Scheier, "Understanding Intelligence", 
    MIT Press, October 1999
[9] Deniz Yuret, ''Discovery of Linguistic Relations Using
    Lexical Attraction'' (1998) PhD Thesis
[10] Linas Vepstas, "Linas' collection of NLP data", 2008 
     http://gnucash.org/linas/nlp/
[11] R. McDonald, F. Pereira, etal, "Minimum-Spanning Tree Parser"
     2005
     http://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.html
