
                 Structure in Linguistics
                 ------------------------
                 Linas Vepstas December2009

A combination of increasing compute power and commercial utility will
attract new attention to the field of corpus linguistics, and lead an
invasion of computational linguistics principles. Access to compute
resources allows large data-driven experiments to be performed using large
collections of linguistic data, while commercial utility introduces a 
pragmatic engineering ethos where workable methods and practical results 
are more important than theoretical debates.  The corpus provides the 
fertile ground on which this growth of interest can take place.

Half a century of computational linguistics has revealed that
building AI/NLP systems by hand is a difficult and seemingly 
insurmountable task. However, the process of hand-building seems understood
well enough that parts of it can be automated -- that is, grammars can
be learned by machine-learning systems, and semantic content can be 
automatically discerned.  The availability of a large corpus appears to
be crucial for this task.  In this sense, the distinction of corpus
linguistics and computational linguistics is now blurred[1,2,18,19].  

The use of experimental techniques to discover new linguistic facts 
hidden in large corpora have provided many new discoveries. However,
some[3,8] suggest that there are even stronger ways to discover linguistic
and semantic structure: "linguistics by embodiment".  For humans, it is
easier to learn something by doing it, as opposed to just reading about
it.  Similarly, the embodiment hypothesis is that a software agent can
more easily learn semantic communication by having a virtual, 3D body
allowing linguistic behaviours to be correlated with visual and motor 
senses. This raises important theoretical and philosophical questions 
about "concepts" and "meaning": Is meaning confined to the evidence
attainable from a corpus[6], or is there a sublime aspect of meaning that
is accessible only by doing, feeling, sensing? In humans, internal
feelings and sensations are not directly accessible to outside observers,
and so such debates can be rendered moot by lack of evidence. The conceit
of corpus linguistics, that meaning is negotiated between speakers[4,5], 
and is located only in the discourse[6], has reigned unchallenged.  But
what happens when one has access to a robotic linguistic actor, whose 
internal states can be probed and examined, where the sublime and
ineffable can be probed and measured? This possiblity even turns 
corpus linguistics on its head: does it make sense to collect and analyze
a corpus of humans talking to automated bankers and pizza ordering
systems?[7] Surely, the fact that the software programming for the pizza
ordering system is available seems to obviate the need to sample its
utterances. And yet, a sampling of the confusions and digressions that
occur during chat between humans and robotic systems can be commericially
valuable: the efficiency and accuracy of these systems can be improved
by studying how communications breaks down.

The goal of the rest of this screed is to sketch not only how linguistic
analysis is performed automatically, but how deeply layered and 
abstract it has become. Two points emerge: collocation and corpus are
key elements, and that forward progress requires clever observation of 
subtle linguistic phenomena by human linguists.

Collocation from a Mathematical Point of View
----------------------------------------------
The starting point for automated linguistic learning is collocation.
One of the simplest analysis one can perform is to discover important 
bigrams by means of mutual information[9]. While a simple count of
probabilities might show which word pairs (or N-grams) occur frequently, 
mutual information gives a different view: it uncovers idiomatic
expressions and "set phrases": collections of words that are used 
together in a relatively invariant fashion. Examples of high mutual
information word pairs are 'Johns Hopkins' and 'jet propulsion', both
scoring an MI of about 15 in one dataset[10].  The most striking 
result from looking over lists of high-mutual-information word pairs
is that most are the names of conceptual entities.

Mutual information between word pairs provides the starting point for
a class of parsers such as the Minimum Spanning Tree Parser[11]. These
parsers are typically dependency parsers that discover dependency
relations using only "unsupervised" training. The key word here is 
"unsupervised": rather than providing the learning algorithm with 
linguist-annotated datasets, the parser "learns" merely by observing
text "au naturel", without annotations.  This is in contrast to many
other parsers built on statistical techniques, which require an
annotated corpus as input.  By eschewing annotation, it stays true 
to one of the core ideals of corpus linguistics: "trust the text".

Sometimes it is imagined that statistical techniques stop here: anyone
who has played with (or read about) Markov chains built from N-grams
know that such systems are adept at generating quasi-grammatical nonsense
text.  But this does not imply that machine learning techniques cannot
advance further: it merely means that a single level of analysis has
only limited power, and must be aided by further layers providing
additional structure and constraints.  So, for example, although a
naive Markov chain might allow nonsense word combinations, a higher
order structure discriminator would note that certain
frequently-generated patterns are rarely or never observed in an
actual corpus, and prohibit their generation (alternatively, balk at 
creating a parse when presented with such ungrammatical input).

Another important point is that the observation of structures in
corpora can be used to validate "mentalistic" theories of grammar.
The whole point of traditional, hand-built parsers was to demonstrate
that one could model certain high-frequency word occurances with a
relatively small set of rules -- that these rules can differentiate
between grammatical and ungrammatical text. Thus, for example, high 
mutual information word pairs help validate the "mentalistic" notion
of a "dependency parser".  But this is a two way street: theories 
based on practical experience can serve as a guide for the kinds of
structures and patterns that one might be able to automatically mine,
using unsupervised techniques, from a text corpus. Thus, for example, 
the "Meaning-Text Theory" developed by Igor Mel'cuk and others[12, 13]
provides a description of "set phrases", deep semantic structures, and 
of the interesting notion of "lexical functions". Perhaps one might be
able to discover these by automated means, and conversely, use these 
automatically discovered structures to perform more accurate parsing
and even analysis of semantic content.

Such attempts at automated, unsupervised learning of deeper structure is
already being done, albeit without any theoretical baggage in tow.
Prominent examples include Dekang Lin's work on the automatic discovery
of synonymous phrases ("Discovery of Inference Rules from Text" [14]), 
and Poon & Domingos extension of these ideas to the automated discovery
of synonymous relations of higher arity ("Unsupervised Semantic Parsing"[15]).
The earlier work by Lin applies a fairly straight-forward statistical
analysis to discover co-occuring phrases and thus deduce their synonymy.
However, it is important to note that Lin's analysis is
"straight-forward" only because a certain amount of heavy lifting was
already performed by parsing the input corpus.  That is, rather than
applying statistical techniques on raw N-grams, the text is first
analized with a dependency parser; it is the output of the parser that
is subject to statistical analysis.  Crudely speaking, "collcations
of parses" are found.

This provides an example of the kind of "layered" approach that seems 
to be necessary to tease out the deeper structures in text -- it is not 
possible to define a single, simple statistical measure that will 
magically reveal hidden structure in text. Indeed, the complexity of 
techniques can quickly escalate: although the work of Poon & Domingos[15]
can be seen as a kind of generalization of that of Lin, it employs a 
considerably more dense and intellectually challenging mathematical 
framework to do so.  Nonetheless, it takes as its core the idea of 
"trust the text" -- neither linguistics nor meaning are "engineered 
into the system" -- there are no ad-hoc English-language-specific 
tweaks in the code, and, indeed, almost no linguistic theory whatsoever, 
beyond the core requirement that dependency parsing provide the input.

Although the above tries to paint a continuous chain of analysis,
starting with a corpus, passing through mutual information to a
dependency parser, from which further relationships are teased out.
There are certanly other paths.  Phrase-structure-driven parsers can
also be learned by statistical techniques, although the *unsupervised*
discovery of such grammars is more mathematically challenging (i.e.
requiring a deeper understanding of topics in computer science). That
"large scale knowledge extraction" is possible from such grammars is
demonstrated by Michael and Valiant[16].

A major blot on the above arguments is that the nature of the driving
force behind the results is the urge to discover some practical, usable,
functional technique for automatically discovering and manipulating
"knowledge". This is a laudable goal, for doing so will presumably shed
light on the nature of "meaning" and "concepts", as well as be of
considerable economic benefit.  Yet, in the mad rush towards automated 
knowledge extraction and manipulation, finer and more subtle linguistic 
phenomena are left on the wayside. The overall attack is still 
brute-force: the knowledge to be extracted is often no deeper
than an analysis of the sentence "Aristotle is a man", and often
requires little more linguistic analysis than that needed to parse 
that sentence. The genres frequently analyzed are financial or biomedical 
texts; fine literature and poetry are roundly ignored.  I only hope that 
perhaps some of these techniques can be refocused on more subtle phenomena.

The trend towards ever-more complex analysis seems inexorable. But
this does not mean that every simple, fast, easy way of analyzing text 
has been discovered. Recent work from Google and NEC labs titled
"Polynomial Semantic Indexing"[17] demonstrates how a fairly simple, 
shallow mathematical technique, which assumes no syntactic structure in
the text whatsoever, can none-the-less discover considerable amounts of
semantic content. While the technique is hardly any deeper than that of
computing co-occurance probabilities or mutual information, it is notable
in that it requires a miniscule amount of CPU time, as compared to other
"obvious" approaches, or even the act of parsing text.

Conclusion
----------
In the conectionist view of semantics, "meaning" exists only in patterns
and relations and connections: meaning is expressed in linguistic
dialog.  In the case of embodied actors, meaning can be expressed in 
action; but again, the action is in reference to some thing. There is 
no atomic kernel to meaning, there is no "there", there. Rather, meaning 
is tied into the expression and articulation of structure. 

But what is meant by "structure" and "connections"? There are multiple
candidates for things that could be "structure" in linguistics. One
obvious candidate is "lexis" -- that grand lexicon in the sky, where all
words, phrases and idioms are defined maximally in relation to one
another.  But there is also the structure commonly known as "syntax" --
a set of rules governing the positional ordering of words.  Cognitive
linguistics posits that there are even deep structures of various sorts
(such as the "lexical functions" of Meaning-Text Theory). It would seem
that each of these different kinds of structures can be discerned to
some degree or another using automatic, unsupervised machine-learning
techniques, taking only a bare, naked corpus of text as input.  

To put it crudely, collocation need not always be done "by hand": let 
the computer do it, and look at what it does. Don't just look at what
words are next to each other, but look at what structures are next to
each other.  Future effort at the intersection of computational and 
corpus linguistics is in the discovery of more subtle structure and the
invention of new techniques to expose and manipulate it. Insofar as
current structures only capture an approximation of human discourse,
then clearly more work remains to be done.

Perhaps to the reader, this layering and interaction of different 
algorithms to explain linguistic phenomena is reminiscent of the use 
of epicycles on epicycles to correct for circular planetary orbits.  
Perhaps it is. But at this stage, it is still too early to tell.  
Certainly, the parsers of a few decades back were perhaps less than 
impressive. Language has a lot of structure, and capturing that 
structure with hand-coded rules proved to be an overwhelming task.  
Yet, the structure is there, unclear as it may often be.  What we
have now, that we didn't have back then, are the tools to raise the 
exploration to a "meta" level: the tools themselves find rules and 
structure in the corpus -- and now we can try to understand what 
sort of rules and structure they are finding, and why some kinds of 
structure are invisible to some kinds of tools, and what it is that 
makes one tool better than another. 

About the Author
----------------
Dr. Linas Vepstas is a relative newcomer to the field of lingustics, 
and thus expresses a naive view of the field, likely to be grating to
old hands. Worse, his doctorate is in physics and mathematics, and so is
doubly cursed by those who have come to linguistics as a discipline of
the humanities.  He hopes to make up for these sins with enthusiasm,
striving for some state of grace.


References
----------
[1] http://mailman.uib.no/public/corpora/2008-August/007102.html
[2] http://mailman.uib.no/public/corpora/2008-August/007112.html
[3] Francisco J. Varela, Evan Thompson, Eleanor Rosch,
    "The embodied mind:  Cognitive science and human experience"
    MIT Press, Cambridge, MA, USA. 1991. 
[4] http://mailman.uib.no/public/corpora/2008-August/007115.html
[5] http://mailman.uib.no/public/corpora/2008-August/007089.html
[6] http://mailman.uib.no/public/corpora/2008-August/007161.html
[7] http://mailman.uib.no/public/corpora/2008-August/007166.html
[8] Rolf Pfeifer, Christian Scheier, "Understanding Intelligence", 
    MIT Press, October 1999
[9] Deniz Yuret, ''Discovery of Linguistic Relations Using
    Lexical Attraction'' (1998) PhD Thesis
[10] Linas Vepstas, "Linas' collection of NLP data", 2008 
     http://gnucash.org/linas/nlp/
[11] R. McDonald, F. Pereira, etal, "Minimum-Spanning Tree Parser"
     2005
     http://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.html
[12] @article{Mel'cuk1987,
   author = {Igor A. Mel'cuk and Alain Polguere},
   title = {A Formal Lexicon in Meaning-Text Theory},
   journal = {Computational Linguistics},
   year = 1987,
   volume = {13},
   pages = {261-275},
}
[13] @proceedings{Steele1990,
   title = {Meaning-Text Theory: Linguistics, Lexicography, and Implications},
   year = 1990,
   editor = {James Steele},
   publisher = {University of Ottowa Press},
}
[14] @inproceedings{Lin2001,
   author = {Lin, Dekang and Pantel, Patrick},
   title = {DIRT: Discovery of Inference Rules from Text},
   booktitle = {Proceedings of the Seventh ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD'01)},
   pages = {323--328},
   year = 2001,
   publisher = {ACM Press},
   doi = {10.1145/502512.502559},
}
[15] @inproceedings{Poon2009,
   author = {Poon, Hoifung and Domingos, Pedro},
   title = {Unsupervised Semantic Parsing},
   booktitle = {Proceedings of the 2009 Conference on Empirical Methods
in Natural Language Processing},
   pages = {1--10},
   year = 2009,
   address = {Singapore},
   month = {August},
   publisher = {Association for Computational Linguistics},
   url = {http://www.aclweb.org/anthology/D/D09/D09-1001},
}
[16] Loizos Michael and Leslie G. Valiant, "A First Experimental 
     Demonstration of Massive Knowledge Infusion" Proc. 11th
     International Conference on Principles of Knowledge Representation 
     and Reasoning, Sept. 16-20, 2008, Sydney, Australia, 378-389.
[17] Bing Bai, Jason Weston, et al, "Polynomial Semantic Indexing"
     ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 22
     Proceedings of the 2009 Conference

[18] http://mailman.uib.no/public/corpora/2008-August/007096.html
[19] http://mailman.uib.no/public/corpora/2008-August/007116.html
