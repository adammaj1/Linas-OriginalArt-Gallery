#LyX 1.6.1 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\use_default_options false
\language english
\inputencoding auto
\font_roman times
\font_sans helvet
\font_typewriter courier
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\float_placement h
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 2
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
The Distribution of Word Pairs
\end_layout

\begin_layout Author
Linas Vepstas
\end_layout

\begin_layout Date
13 March 2009
\end_layout

\begin_layout Abstract
The short note presents some empircal data on the distribution of word pairs
 obtained from English text.
 Particular attention is paid to the distribution of mutual information.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The mutual information of word pairs observed in text is commonly used in
 statistical corpus linguistics for a variety of reasons: to help identify
 idioms and collocations, to create dependency parsers, such as minimum
 spanning tree parsers, and many other uses.
 This is because the mutual information of a word-pair co-occurance is a
 good indicator of how often two words 'go together'.
 However, little seems to have been written on the distribution of word
 pairs, or the distribution of mutual information.
 The current work is a short note reporting on the empirical distribution
 of word pairs seen in English text.
\end_layout

\begin_layout Standard
Mutual information is a measure of the relatedness of two words.
 For example “Northern Ireland” will have high mutual information, since
 the words “Northern” and “Ireland” are commonly used together.
 By contrast, “Ireland is” will have negative mutual information, mostly
 because the word “is” is used with many, many other words besides “Ireland”;
 there is no special relationship between these two words.
 High mutual information word pairs are typically noun phrases, often idioms
 and collocations, and almost always embody some concept (so, for example,
 “Northern Ireland” is the name of a place — the name of the conception
 of a particular country).
\end_layout

\begin_layout Section
Definitions
\end_layout

\begin_layout Standard
In what follows, a 
\begin_inset Quotes eld
\end_inset

pair
\begin_inset Quotes erd
\end_inset

 will always mean an 
\begin_inset Quotes eld
\end_inset

ordered word pair
\begin_inset Quotes erd
\end_inset

; an ordered pair is simply a pair 
\begin_inset Formula $(x,y)$
\end_inset

 where both words 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 occur in the same sentence, and the word 
\begin_inset Formula $x$
\end_inset

 occurs to the left of the word 
\begin_inset Formula $y$
\end_inset

 in that sentence.
 The pairs are taken to be ordered because, in English, word order mattters.
 Some of the graphs below show pairs of neighboring words; others show graphs
 of pairs simply taken from the same sentence (so that other words may intervene
 in the middle of the pair).
 
\end_layout

\begin_layout Standard
The frequency of an ordered word pair 
\begin_inset Formula $(x,y)$
\end_inset

 is defined as 
\begin_inset Formula \[
P(x,y)=\frac{N(x,y)}{N(*,*)}\]

\end_inset

where 
\begin_inset Formula $N(x,y)$
\end_inset

 is the number of times that the pair 
\begin_inset Formula $(x,y)$
\end_inset

 was observed, with word 
\begin_inset Formula $x$
\end_inset

 to the left of word 
\begin_inset Formula $y$
\end_inset

.
 The * represents 'any', so that 
\begin_inset Formula $N(*,*)$
\end_inset

 is the total number of word pairs observed.
\end_layout

\begin_layout Standard
The mutual information 
\begin_inset Formula $M(x,y)$
\end_inset

 associated with a word pair 
\begin_inset Formula $(x,y)$
\end_inset

 is defined as 
\begin_inset Formula \[
M(x,y)=log_{2}\frac{P(x,y)}{P(x,*)P(*,y)}\]

\end_inset

Here, 
\begin_inset Formula $P(x,*)$
\end_inset

 is the marginal probablity of observing a pair where the left word is 
\begin_inset Formula $x$
\end_inset

; similarly, 
\begin_inset Formula $P(*,y)$
\end_inset

 is the marginal probability of observing a pair where the right word is
 
\begin_inset Formula $y$
\end_inset

.
 
\end_layout

\begin_layout Standard
In the following, a selection of 925 thousand sentences consisting of 13.4
 million words were observed, from a corpus consisting of Voice of Ameria
 shows, Simple English Wikipedia, and a selction of books from Project Gutenberg
, including Tolstoy's 'War and Peace'.
\end_layout

\begin_layout Section
Unigram counts, Zipf's law
\end_layout

\begin_layout Standard
Individual words, ranked by thier frequency of occurrance, are known to
 be distributed according to the Zipf power law; this was noted by Zipf
 in the 1930's, and possibly by others even earlier.
 The power law is illustrated in figure
\begin_inset CommandInset ref
LatexCommand ref
reference "Flo:single-word-dist"

\end_inset

, and is genereated from the current corpus.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "Flo:single-word-dist"

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Single Word Distribution
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename single.ps
	width 100text%

\end_inset

 
\end_layout

\begin_layout Plain Layout
The graph above shows the frequency distribution of the most commony occuring
 words in the sample corpus, ranked by thier frequency of occurance.
 The first point corresponds to the beginning-of-sentence maker; the average
 sentence length of about 14.4 words corresponds to a frequency of 1/14.4=0.07.
 This is followed by the words 
\begin_inset Quotes eld
\end_inset

the, of, in to, and
\begin_inset Quotes erd
\end_inset

, and so on.
 The distribution follows Zipf's power law.
 The thicker red line represents the data, whereas the thinner green line
 represents the power law: 
\begin_inset Formula \[
P(k)=k^{-1.1}/\zeta(1.1)\]

\end_inset

 where 
\begin_inset Formula $k$
\end_inset

 is the rank or the word, and 
\begin_inset Formula $P(k)$
\end_inset

 is the freuency at which the 
\begin_inset Formula $k$
\end_inset

'th word is observed.
 
\end_layout

\begin_layout Plain Layout
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
generated with~/src/novamente/src/cerego/lexical-attr/src/count/graph/single.gplo
t and single.pl 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Distribution of Mutual Information
\end_layout

\begin_layout Standard
How is mutual information distributed in the English language? The graph
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:MI nearby pairs"

\end_inset

 shows the (unweighted) distribution of mutual information, while the graph
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Weighted-Mutual-Information"

\end_inset

 shows the weighted distribution.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Distribution of Mutual Information
\begin_inset CommandInset label
LatexCommand label
name "fig:MI nearby pairs"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename mi-nearby.ps
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
This graph shows the distribution of the mutual information of word pairs
 taken from the text corpus.
 The figure shows approximately 10.4 million distinct word pairs observed,
 with each distinct word pair counted exactly once.
\end_layout

\begin_layout Plain Layout
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
created with~/src/novamente/src/cerego/lexical-attr/src/count/graph/mi-nearby.ps
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Notable in the figure is a distinct triangular shape, with a blunted, lop-sidede
d nose, and 
\begin_inset Quotes eld
\end_inset

fat
\begin_inset Quotes erd
\end_inset

 tails extending to either side.
 The triangular sides appear to be log-linear over many orders of magnitude,
 and seem to have nearly equal but opposite slopes.
 
\end_layout

\begin_layout Standard
Triangular distributions are the hallmark of the sum of two uniformly distribute
d random variables: the convolution of two rectangles is a triangle.
 XXX pursue this idea.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Weighted Mutual Information
\begin_inset CommandInset label
LatexCommand label
name "fig:Weighted-Mutual-Information"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename mi-nearby-weighted.ps
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
This graph shows the weighted frequency of mutual information.
 Unlike the previous figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:MI nearby pairs"

\end_inset

, where each pair was counted with a weight of 1.0, here, each pair is counted
 with a weight of 
\begin_inset Formula $P(x,y)$
\end_inset

.
 That is, for a fixed value of mutual information, frequent pairs with that
 value of mutual information contribute more strongly than infrequent ones.
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Mutual Information Scatterplots
\end_layout

\begin_layout Standard
How does the mutual information
\end_layout

\begin_layout Standard
(show graph)
\end_layout

\begin_layout Standard
925K sentences 13.4M words
\end_layout

\begin_layout Standard
Here it is
\end_layout

\end_body
\end_document
