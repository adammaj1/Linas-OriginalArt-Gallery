#LyX 1.6.1 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass amsart
\use_default_options false
\begin_modules
theorems-ams
\end_modules
\language english
\inputencoding auto
\font_roman times
\font_sans helvet
\font_typewriter courier
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\float_placement h
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 2
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
The Distribution of English Language Word Pairs
\end_layout

\begin_layout Author
Linas Vepstas
\end_layout

\begin_layout Email
linasvepstas@gmail.com
\end_layout

\begin_layout Date
13 March 2009
\end_layout

\begin_layout Abstract
This short note presents some empircal data on the distribution of word
 pairs obtained from English text.
 Particular attention is paid to the distribution of mutual information.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The mutual information of word pairs observed in text is commonly used in
 statistical corpus linguistics for a variety of reasons: to help identify
 idioms and collocations, to create dependency parsers, such as minimum
 spanning tree parsers, and many other uses.
 This is because the mutual information of a word-pair co-occurance is a
 good indicator of how often two words 'go together'.
 However, little seems to have been written on the distribution of word
 pairs, or the distribution of mutual information.
 The current work is a short note reporting on the empirical distribution
 of word pairs seen in English text.
\end_layout

\begin_layout Standard
Mutual information is a measure of the relatedness of two words.
 For example “Northern Ireland” will have high mutual information, since
 the words “Northern” and “Ireland” are commonly used together.
 By contrast, “Ireland is” will have negative mutual information, mostly
 because the word “is” is used with many, many other words besides “Ireland”;
 there is no special relationship between these two words.
 High mutual information word pairs are typically noun phrases, often idioms
 and collocations, and almost always embody some concept (so, for example,
 “Northern Ireland” is the name of a place — the name of the conception
 of a particular country).
\end_layout

\begin_layout Section
Definitions
\end_layout

\begin_layout Standard
In what follows, a 
\begin_inset Quotes eld
\end_inset

pair
\begin_inset Quotes erd
\end_inset

 will always mean an 
\begin_inset Quotes eld
\end_inset

ordered word pair
\begin_inset Quotes erd
\end_inset

; an ordered pair is simply a pair 
\begin_inset Formula $(x,y)$
\end_inset

 where both words 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 occur in the same sentence, and the word 
\begin_inset Formula $x$
\end_inset

 occurs to the left of the word 
\begin_inset Formula $y$
\end_inset

 in that sentence.
 The pairs are taken to be ordered because, in English, word order mattters.
 Some of the graphs below show pairs of neighboring words; others show graphs
 of pairs simply taken from the same sentence (so that other words may intervene
 in the middle of the pair).
 
\end_layout

\begin_layout Standard
The frequency of an ordered word pair 
\begin_inset Formula $(x,y)$
\end_inset

 is defined as 
\begin_inset Formula \[
P(x,y)=\frac{N(x,y)}{N(*,*)}\]

\end_inset

where 
\begin_inset Formula $N(x,y)$
\end_inset

 is the number of times that the pair 
\begin_inset Formula $(x,y)$
\end_inset

 was observed, with word 
\begin_inset Formula $x$
\end_inset

 to the left of word 
\begin_inset Formula $y$
\end_inset

.
 The * represents 'any', so that 
\begin_inset Formula $N(*,*)$
\end_inset

 is the total number of word pairs observed.
\end_layout

\begin_layout Standard
The mutual information 
\begin_inset Formula $M(x,y)$
\end_inset

 associated with a word pair 
\begin_inset Formula $(x,y)$
\end_inset

 is defined as 
\begin_inset Formula \begin{equation}
M(x,y)=log_{2}\frac{P(x,y)}{P(x,*)P(*,y)}\label{eq:mutual info}\end{equation}

\end_inset

Here, 
\begin_inset Formula $P(x,*)$
\end_inset

 is the marginal probablity of observing a pair where the left word is 
\begin_inset Formula $x$
\end_inset

; similarly, 
\begin_inset Formula $P(*,y)$
\end_inset

 is the marginal probability of observing a pair where the right word is
 
\begin_inset Formula $y$
\end_inset

.
 
\end_layout

\begin_layout Standard
In the following, a selection of 925 thousand sentences consisting of 13.4
 million words were observed, from a corpus consisting of Voice of Ameria
 shows, Simple English Wikipedia, and a selction of books from Project Gutenberg
, including Tolstoy's 'War and Peace'.
\end_layout

\begin_layout Section
Unigram counts, Zipf's law
\end_layout

\begin_layout Standard
Individual words, ranked by thier frequency of occurrance, are known to
 be distributed according to the Zipf power law; this was noted by Zipf
 in the 1930's, and possibly by others even earlier.
 The Zipf distribution takes the form, for large 
\begin_inset Formula $N$
\end_inset

 (that is, for a large number 
\begin_inset Formula $N$
\end_inset

 of words) 
\begin_inset Formula \[
P_{s}(k)=\frac{1}{\zeta(s)}k^{-s}\]

\end_inset

 where 
\begin_inset Formula $s$
\end_inset

 is the characteristic power defining the distribution.
 Here, 
\begin_inset Formula $k$
\end_inset

 is the 
\begin_inset Formula $k$
\end_inset

'th ranked word, and 
\begin_inset Formula $P(k)$
\end_inset

 is the probability of observing that word.
 For large 
\begin_inset Formula $N$
\end_inset

, the overall normalization can be approximated by the Riemann zeta function
 
\begin_inset Formula $\zeta(s)$
\end_inset

.
\end_layout

\begin_layout Standard
The power law is illustrated in figure
\begin_inset CommandInset ref
LatexCommand ref
reference "Flo:single-word-dist"

\end_inset

, and is generated from the current corpus.
 As may be seen, power laws become straight lines in a log-log graph.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "Flo:single-word-dist"

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Single Word Distribution
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename single.ps
	width 100text%

\end_inset

 
\end_layout

\begin_layout Plain Layout
The graph above shows the frequency distribution of the most commony occuring
 words in the sample corpus, ranked by thier frequency of occurance.
 The first point corresponds to the beginning-of-sentence maker; the average
 sentence length of about 14.4 words corresponds to a frequency of 1/14.4=0.07.
 This is followed by the words 
\begin_inset Quotes eld
\end_inset

the, of, in to, and
\begin_inset Quotes erd
\end_inset

, and so on.
 The distribution follows Zipf's power law.
 The thicker red line represents the data, whereas the thinner green line
 represents the power law: 
\begin_inset Formula \[
P(k)=k^{-1.1}/\zeta(1.1)\]

\end_inset

 where 
\begin_inset Formula $k$
\end_inset

 is the rank or the word, and 
\begin_inset Formula $P(k)$
\end_inset

 is the frequency at which the 
\begin_inset Formula $k$
\end_inset

'th word is observed.
 Notice that after the first thousand words, the probability starts dropping
 at a greater rate than at first.
 This is bend is commonly seen such word frequency graphs; it is not covered
 by Zipf's distribution.
 To the author's best knowledge, there has not been any published analysis
 of this bend.
\end_layout

\begin_layout Plain Layout
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
generated with~/src/novamente/src/cerego/lexical-attr/src/count/graph/single.gplo
t and single.pl 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Distribution of Mutual Information
\end_layout

\begin_layout Standard
How is mutual information distributed in the English language? The graph
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:MI nearby pairs"

\end_inset

 shows the (unweighted) distribution histogram of mutual information, while
 the graph 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Weighted-Mutual-Information"

\end_inset

 shows the weighted distribution.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Histogram of Mutual Information
\begin_inset CommandInset label
LatexCommand label
name "fig:MI nearby pairs"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename mi-nearby.ps
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
This graph shows the distribution histogram of the mutual information of
 word pairs taken from the text corpus.
 The figure shows approximately 10.4 million distinct word pairs observed,
 with each distinct word pair counted exactly once.
 The sides of the distribution are clearly log-linear, and are easily fit
 by straight lines.
 Shown are two such fits, one proportional to 
\begin_inset Formula $e^{-0.28M}$
\end_inset

 and the other to 
\begin_inset Formula $e^{0.26M}$
\end_inset

 where 
\begin_inset Formula $M$
\end_inset

 is the mutual information.
\end_layout

\begin_layout Plain Layout
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
created with~/src/novamente/src/cerego/lexical-attr/src/count/graph/mi-nearby.ps
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The figures are constructed by means of 
\begin_inset Quotes eld
\end_inset

binning
\begin_inset Quotes erd
\end_inset

 or histograming.
 For a given value of mutual information 
\begin_inset Formula $M$
\end_inset

 and bin width 
\begin_inset Formula $\Delta M$
\end_inset

, the (unweighted) bin count is the total number of pairs 
\begin_inset Formula $(x,y)$
\end_inset

 with 
\begin_inset Formula $M<M(x,y)\le M+\Delta M$
\end_inset

.
 The weighted bin count is defined similarly, except that each pair contributes
 
\begin_inset Formula $P(x,y)$
\end_inset

 to the bin count.
\end_layout

\begin_layout Standard
Notable in the first figure is a distinct triangular shape, with a blunted,
 lop-sideded nose, and 
\begin_inset Quotes eld
\end_inset

fat
\begin_inset Quotes erd
\end_inset

 tails extending to either side.
 The triangular sides appear to be log-linear over many orders of magnitude,
 and seem to have nearly equal but opposite slopes.
 
\end_layout

\begin_layout Standard
Triangular distributions are the hallmark of the sum of two uniformly distribute
d random variables: the convolution of two rectangles is a triangle.
 Taking equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mutual info"

\end_inset

 and propagating the logarithm through, one obtains 
\begin_inset Formula \[
M(x,y)=\log_{2}P(x,y)-\log_{2}P(x,*)-\log_{2}P(*,y)\]

\end_inset

which can be seen to be a sum of three random variables.
\end_layout

\begin_layout Standard
XXX pursue this idea.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Weighted Histogram of Mutual Information 
\begin_inset CommandInset label
LatexCommand label
name "fig:Weighted-Mutual-Information"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename mi-nearby-weighted.ps
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
This graph shows the weighted frequency of mutual information.
 Unlike the previous figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:MI nearby pairs"

\end_inset

, where each pair was counted with a weight of 1.0, here, each pair is counted
 with a weight of 
\begin_inset Formula $P(x,y)$
\end_inset

.
 That is, for a fixed value of mutual information, frequent pairs with that
 value of mutual information contribute more strongly than infrequent ones.
\end_layout

\begin_layout Plain Layout
Several different log-linear behaviours are visible; these have been hand-fit
 with the straight lines illustrated; these are given by 
\begin_inset Formula $e^{-0.5M}$
\end_inset

, 
\begin_inset Formula $e^{-0.75M}$
\end_inset

 and 
\begin_inset Formula $e^{M}$
\end_inset

.
 The two slopes on the right hand side require no further scaling: they
 interecept 
\begin_inset Formula $P=1$
\end_inset

 at 
\begin_inset Formula $M=0$
\end_inset

.
 The appearence of such simple fractions in the fit is curious: changing
 the slopes by as little as 5% leads to a noticably poorer fit.
 Whether this is a pure coincidence, or the sign of something deeper, is
 unclear.
\end_layout

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
Generated with mi-nearby-weighted.gplot 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Linked Words
\end_layout

\begin_layout Standard
Instead of considering all possible word pairs in a sentence, one might
 instead limit consideration to words that are in some way related to one-anothe
r.
 To that end, the same corpus was parsed by means of the Link Grammar parser
 (need ref), to obtain pairs of words linked by Link Grammar linkages.
 
\end_layout

\begin_layout Standard
The basic insight of Link Grammar is that every word can be treated as a
 puzzle-piece, in that a word can only connect to other words that have
 matching connectors.
 The connector types are labelled; the natural output of parsing is a set
 of connected word pairs.
 It is an important coincidence to notice that, usually, the word pairs
 that the parser generates also have positive, and usually a large positive
 value of mutual information.
 This observation provides insight into why the Link Grammar 
\begin_inset Quotes eld
\end_inset

works
\begin_inset Quotes erd
\end_inset

 or is a linguistiacally successful theory of parsing: its rule set captures
 word associations.
\end_layout

\begin_layout Standard
The figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Linked-Word-Pairs"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Weighted-Linked-Words"

\end_inset

 reproduce the previous two graphs, except that this time, the set of word
 pairs is limited to those that have been identified by Link Grammar.
 This set consists of 2.37 million distinct word pairs.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Histogram of Linked Word Pairs
\begin_inset CommandInset label
LatexCommand label
name "fig:Linked-Word-Pairs"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename mi-pair.ps
	width 100text%
	BoundingBox 50bp 50bp 554bp 770bp

\end_inset


\end_layout

\begin_layout Plain Layout
This figure shows a historgram of word pairs that have been linked to one-anothe
r by means of the Link Grammar parser.
 The dataset contains approximately 2.37 million such word pairs; each is
 counted precisely once (thus, this is the 
\begin_inset Quotes eld
\end_inset

unweighted
\begin_inset Quotes erd
\end_inset

 distribution).
 Several log-linear regimes are visible.
 That on the left has exactly the same slope as in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "Flo:single-word-dist"

\end_inset

, namely 
\begin_inset Formula $e^{0.26M}$
\end_inset

.
 The average slope on the right is as before, 
\begin_inset Formula $e^{-0.28M}$
\end_inset

, but seems to be composed of two distinct regions, first with a shallower,
 and then a steeper slope of 
\begin_inset Formula $e^{-0.195M}$
\end_inset

 and 
\begin_inset Formula $e^{-0.48M}$
\end_inset

.
\end_layout

\begin_layout Plain Layout
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
built by mi-pair.gplot
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Weighted Linked Words
\begin_inset CommandInset label
LatexCommand label
name "fig:Weighted-Linked-Words"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename mi-pair-weighted.ps
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
This figure shows weighted linked word pairs; it is the analogue to figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Weighted-Mutual-Information"

\end_inset

, but for linked word pairs.
 That is, the historgram bin counts are weighted by the probability 
\begin_inset Formula $P(x,y)$
\end_inset

 of seeing the linked word pair 
\begin_inset Formula $(x,y)$
\end_inset

.
 Again, several sloped regions are visible; the linear fits are given by
 
\begin_inset Formula $e^{1.1M}$
\end_inset

, 
\begin_inset Formula $e^{-0.3M}$
\end_inset

 and 
\begin_inset Formula $e^{-0.85M}$
\end_inset

.
\end_layout

\begin_layout Plain Layout
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Created by ./mi-pair-weighted.gplot
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

 
\end_layout

\begin_layout Section
Mutual Information Scatterplots
\end_layout

\begin_layout Standard
The previous sections explored the histogram distribution of the mutual
 information of word pairs.
 Alternately, one might wish to understand how mutual information correlates
 with the probability of observing word pairs.
 A scatterplot showing such a correlation is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Word-Pair-Scatterplot"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Word Pair Scatterplot
\begin_inset CommandInset label
LatexCommand label
name "fig:Word-Pair-Scatterplot"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename mi-scatter-nearby-200K.ps
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
This figure shows a scatterplot of the probability of occurance 
\begin_inset Formula $P(x,y)$
\end_inset

 versus the mutual information 
\begin_inset Formula $M(x,y)$
\end_inset

 for word pairs 
\begin_inset Formula $(x,y)$
\end_inset

.
 Shown are 200 thousand pairs (out of the sample set of 10.4 million pairs).
 Rather than plotting the probability 
\begin_inset Formula $P(x,y)$
\end_inset

 on a logarithmic axis, this plot uses a linear axis, labelled with 
\begin_inset Formula $-log_{2}P(x,y)$
\end_inset

.
 This axis labelling gives a better feeling for the relative sizes of the
 horizontal and vertical scales.
 
\end_layout

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
Created with mi-scatter-nearby-200K.gplot and mi-scatter.pl
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
bib
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "/home/linas/src/fractal/paper/lang"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
