#LyX 1.6.1 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass IEEEtran
\use_default_options false
\language english
\inputencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\float_placement tbh
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 0
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Table-driven Word Sense Disambiguation
\end_layout

\begin_layout Author
Linas Vepstas
\begin_inset Foot
status open

\begin_layout Plain Layout
Linas Vepstas is with Novamente LLC; work was supported by Cerego/smart.fm
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
The age-old observation that sense and syntax are correlated is used to
 build a table-driven word-sense disambiguation algorithm.
 Because the sense is determined by a table lookup, it is extremely fast,
 making it suitable for practical, real-time semantic-web applications.
 By contrast, existing word-sense disambiguation (WSD) or semantic role
 labelling (SRL) algorithms are fairly CPU-intensive, requiring many seconds
 or even minutes to run on modern hardware.
 
\end_layout

\begin_layout Abstract
The lookup table is constructed by correlating natural language parse structures
 with word-sense assignments.
 The key observation is that certain parse structures resemble very fine-grained
 part-of-speech (POS) tags, and that this fine-grained tag can be used to
 construct a more fine-grained (but otherwise traditional) word-sense lexicon.
 Manually creating such a dictionary is overwhelming, and so the lexicon
 is constructed using automated techniques: a large body of text is tagged
 with senses using a standard WSD algorithm, and then tabulated with the
 result of NLP parses of the same text.
 The POS distinctions are much finer than the usual noun/verb/adj/adv distinctio
ns or even the Penn Treebank POS tags.
 These fine-grained tags allow the correlation between sense and syntax
 to be examined at a more detailed level than usual.
\end_layout

\begin_layout Keywords
Lexicon, NLP, parsing, word-sense disambiguation, WSD, SRL, Semantic role
 labelling, Link Grammar
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
ATTENTION: READ THIS NOTE FIRST:
\begin_inset Note Note
status open

\begin_layout Plain Layout
This paper is hard to finish due to extensive flaws in the tagged dataset.
 There are two major problems:
\end_layout

\begin_layout Plain Layout
* No record was kept of how many files were processed with sense tags, and
 how many processed for other things.
\end_layout

\begin_layout Plain Layout
* The page-rank algo that was implemented did not include the suggested
 
\begin_inset Quotes eld
\end_inset

most-frequent-sense
\begin_inset Quotes erd
\end_inset

 weighting that Mihalcea suggests.
 This significantly lowers the tagging accuracy of the input.
\end_layout

\begin_layout Plain Layout
* Most significantly, the data includes artifacts from 
\begin_inset Quotes eld
\end_inset

islands
\begin_inset Quotes erd
\end_inset

 -- network graphs that were disconnected and thus didn't get the page-rank
 treatment.
 These islands lead to multiple senses being tagged with equal weights,
 which shows up as ugly, un-natural peaks in the entropy vs.
 word-disjunct pair distribution.
 Yikes! This is the most serious problem, see below.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout
Minor-ish problems include:
\end_layout

\begin_layout Plain Layout
* (word,dj,sense) triples were collected even for sentences with unused
 words in the parse.
 This probably potentially adds noise to the data.
\end_layout

\begin_layout Plain Layout
* subscript tags have changed in the newest versions of link-grammar.
 This messes up database lookup.
\end_layout

\begin_layout Plain Layout
* link-grammar lists the - direction disjuncts in backward order, as compared
 to thier corresponding dict entries.
 Disjuncts need to be consistent between database contents, and link-grammar.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Plain Layout
A plan for how to fix this stuff, and proceed, is given in the lexat package
 README, in src/lexat/senseval/README
\end_layout

\begin_layout Plain Layout
Be sure to review other yellows sticky NOTES in this text.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
That there is a correlation between English-language syntax, and word senses,
 is obvious, and underlies the presentation of dictionaries: word senses
 are traditionally grouped according to part-of-speech (POS): noun, verb,
 adjective, adverb.
 Parts of speech are crude indicators of the allowed syntactic use of a
 word: so, in general, one cannot use a noun in place of a verb.
 Of course, there are exceptions: English grammar does allow the use of
 nouns as noun modifiers, the verbing of nouns by use of gerunds, and one
 can, to a limited extent, verbize one's nouns directly.
 But each of these grammatical operations alters the sense of a word as
 well as changing its POS category; the word sense is no longer the same,
 and most dictionaries supply distinct entries for each usage: senses and
 POS tags correlate.
\end_layout

\begin_layout Standard
Some dictionaries attempt a more fine-grained distinction of word senses:
 thus, for example, The American HeritageÂ® Dictionary of the English Language
\begin_inset CommandInset citation
LatexCommand cite
key "AmHeritage2000"

\end_inset

 frequently uses section headings of 
\emph on
v.tr.

\emph default
 and 
\emph on
v.intr.

\emph default
 to denote word senses that are used only with the transitive or intranstive
 form of a verb.
 Thus, upon witnessing a verb used in a certain sentence in a certain way,
 one can immediately narrow down the possible senses for that verb.
\end_layout

\begin_layout Standard
It is is natural to presume that the correlation between word senses and
 word usage might continue to a finer, more detailed level, if only one
 could make finer, more detailed syntactic observations, and had a finer,
 more detailed lexicon to work with.
 This is the primary thesis of this paper: such finer distinctions are possible
 and useful.
 To illustrate this, consider, for example, the verb 
\begin_inset Quotes eld
\end_inset


\emph on
to suffer
\emph default

\begin_inset Quotes erd
\end_inset

.
 The American Heritage Dictionary lists seven senses for this; other dictionarie
s list from 5 to 11 senses.
 The Princeton WordNet dictionary
\begin_inset CommandInset citation
LatexCommand cite
key "Wordnet1998"

\end_inset

 lists 11 meanings.
 Now consider the sentence:
\end_layout

\begin_layout Quotation

\emph on
\begin_inset Quotes eld
\end_inset

She suffered a fracture in the accident.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
A syntactic analysis shows that 
\begin_inset Quotes eld
\end_inset


\emph on
suffer
\emph default

\begin_inset Quotes erd
\end_inset

 is in the past tense, takes a singular pronomial subject, a singular direct
 object, and a modifying prepositional phrase.
 The corresponding WordNet sense key is 
\emph on
suffer%2:29:01::, (undergo (as of injuries and illnesses))
\emph default
.
 This example is syntactically quite different than those of other example
 sentences taken from WordNet:
\end_layout

\begin_layout Quotation

\emph on
\begin_inset Quotes eld
\end_inset

This author really suffers in translation.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
where the verb is in the present tense, has a singular subject, a prepositional
 modifier, but no object: it corresponds to sense 
\emph on
suffer%2:30:02::, (be set at a disadvantage)
\emph default
.
 Similarly one has
\end_layout

\begin_layout Quotation

\emph on
\begin_inset Quotes erd
\end_inset

Many saints suffered martyrdom.
\emph default

\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
which has a plural subject, is in past tense, and the object is an uncountable
 (mass) noun: this corresponds to the sense 
\emph on
suffer%2:39:01::, (undergo or be subjected to)
\emph default
.
 Each of these example sentences, taken from WordNet itself, shows a strong
 correlation between the syntactic structure of the sentence, and the designated
 word sense.
 
\end_layout

\begin_layout Standard
Most of this correlation can be erased by altering the example sentences,
 and creating new, grammatically correct sentences, by altering the tense,
 the count of the subject or object, the presence or absence of the object,
 or adding/removing a modifying perpositional phrase.
 But do native English speakers generate all possible sentence constructions
 with equal probability? Clearly not: idioms, the strict coupling of a few
 words, are a well-known liguistic phenomenon.
 This observation can be taken much farther: the Mel'cuk Meaning-Text Theory
\begin_inset CommandInset citation
LatexCommand cite
key "Mel'cuk1987,Steele1990"

\end_inset

 posits that the intended meaning of an expression strongly influences the
 structure of the sentence that expresses that meaning; conversely, that
 certain sentence patterns are used only with limited, well-defined subsets
 of nouns, verbs.
 Thus, the proper study of the correlation between sense and syntax, in
 order to be valid and correct, must become an exercise in corpus linguistics:
 the tabulation of the frequency of word usage as used by actual writers,
 as compared to the intended sense of the author.
 To the extent that such a correlation exists, it can be tabulated into
 a dictionary, and this dictionary can, in turn, be used to quickly guess
 at the intended meaning, given only the syntactic usage.
\end_layout

\begin_layout Standard
Discovering this correlation relies on large-scale computational linguistics
 techniques.
 Although one might be able to analyze, by hand, a few verbs or nouns, in
 the manner illustrated above, this is hardly practical to get a true sense
 of the prevelance of this correlation.
 In doing a small-scale analysis, one might discover only a few words that
 appear to be highly correlated, without being able to say much about the
 language, in general.
 What is wanted is a full lexicon that makes fine-grained POS distinctions.
 Its not practical to build such a lexicon by hand; it must be constructed
 in some automated way.
 The approach used here is to parse a large quantity of text, obtaining
 syntactic structure, and to tag the same text with word-senses, and so
 as to generate statistics correlating the two tag sets.
 
\end_layout

\begin_layout Standard
The 
\begin_inset Quotes eld
\end_inset

weak link
\begin_inset Quotes erd
\end_inset

 to this automated approach is sense tagging.
 The quantity of text hand-tagged by experts with word senses is not large
 enough to usefule for collecting a reasonable amount of statistics; thus
 sense-tagging must be performed automatically.
 Although automated sense tagging or 
\begin_inset Quotes eld
\end_inset

word sense disambiguation
\begin_inset Quotes erd
\end_inset

 (WSD) algorithms have been improving, the best simple, straight-forward
 systems remain marginal in precision and recall.
 This raises an interesting question: is it possible that, by correlating
 sense tags with syntactic usage, that the incorrect tags will statistically
 
\begin_inset Quotes eld
\end_inset

cancel out
\begin_inset Quotes erd
\end_inset

, while correct tags will reinforce? That is, is it possible that a lookup-table
 based WSD algorithm might have euqal or better accuracy than the underlying
 WSD system from which it was constructed? 
\end_layout

\begin_layout Standard
For parsing, the Link Grammar parser
\begin_inset CommandInset citation
LatexCommand cite
key "Sleator1991,Sleator1993"

\end_inset

 is used.
 The output of the Link Grammar parser are 
\begin_inset Quotes eld
\end_inset

linkages
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

disjuncts
\begin_inset Quotes erd
\end_inset

 that explicitly express how a word is connected to its vicinity.
 Each disjunct is essentially a linkage statement, such as 
\begin_inset Quotes eld
\end_inset

this is a verb with a singular subject on the left, a singular direct object
 on the right, and a modifying prepositional phrase
\begin_inset Quotes erd
\end_inset

, expressed in a compact notation (
\begin_inset Quotes eld
\end_inset


\emph on
Ss- Os+ MVp+
\emph default

\begin_inset Quotes erd
\end_inset

 for this example).
 These disjuncts are taken as the 
\begin_inset Quotes eld
\end_inset

fine-grained POS tags
\begin_inset Quotes erd
\end_inset

 refered to above.
 In certain ways, Link Grammar resembles dependency parsing, and indeed,
 it is straight-forward to convert a Link Grammar parse into a dependency
 parse
\begin_inset Foot
status open

\begin_layout Plain Layout
The RelEx semantic relationship extractor[[XXX need ref] accepts Link-Grammar
 parses as input, and, among other outputs, can generate dependency parses
 fully compatible with the well-known Stanford dependency parser.
\end_layout

\end_inset

.
 Unlike a dependency parse, Link Grammar does not explicitly indicate head
 words; but this is not required for the notion of fine-grained POS tags.
 Conversely, it is straight-forward to create a 
\begin_inset Quotes eld
\end_inset

fine-grained POS tag
\begin_inset Quotes erd
\end_inset

 from a dependency parse; thus, the results in this paper should be reproducible
 by employing other parsers with dependency output.
 It is less clear how to extract a 
\begin_inset Quotes eld
\end_inset

fine-grained POS
\begin_inset Quotes erd
\end_inset

 from a phrasal (constituency, or 
\begin_inset Quotes eld
\end_inset

phrase structure
\begin_inset Quotes erd
\end_inset

) parse: phrasal parses do not indicate the relationships and constraints
 between words; rather, the production rules of such parsers contain many
 non-terminal (non-word) symbols.
 Perhaps the sequence of production rules taken to arrive at a given word
 could serve to act as a 
\begin_inset Quotes eld
\end_inset

fine-grained POS tag
\begin_inset Quotes erd
\end_inset

 for that word.
 Understanding how different parsers change the the correlation between
 POS and sense tags may shed light into the nature of the syntactic/semantic
 connection.
 This question is not explored in this paper.
\end_layout

\begin_layout Standard
Word-sense tagging was performed using an implementation of the Mihalcea
 all-words word-sense disambiguation algorithm
\begin_inset CommandInset citation
LatexCommand cite
key "Mihalcea2004,Mihalcea2005,Mihalcea2007"

\end_inset

.
 This algorithm is reasonably accurate and is straightforward to implement.
 It requires the use of a word-sense similarity measure; several such measures
 available for WordNet.
 The output of the Mihalcea WSD is a ranked list of word-sense assignments
 (WordNet senses) for each word in a text.
 The final dicitionary created is then a tabulation of the frequency with
 which a given Link-Grammar disjunct was observed with a given WordNet sense
 key.
\end_layout

\begin_layout Standard
It is important to recognize that table-driven patten recognition is a biologica
lly plausible model for human cognition.
 Pattern recognition is a central precept of the connectionist philosophy
 of human cognition [xxx need ref].
 It seems unlikely that the human brain implements proceedural computational
 algorithms; rather, connectionism states that symbol manipulation is performed
 with neural-net style architectures.
 A table-lookup based pattern recognizer fits neatly into this picture:
 one might imagine a single neuron with a set of connections sensitive to
 a particular syntactic pattern; when that pattern is presented on the input,
 the neuron fires.
 In this sense, the table lookup of a pattern resembles a single-layer discrimin
atory neural net or perceptron; for a given input, only a small number of
 outputs (word senses) are suggested.
 This analogy indicates the power as well as the weakness of the tehnique.
 Table lookup can be made massively parallel: those neurons that know of
 a specific pattern respond, and all others are silent.
 This echoes the massively-parallel structure of the brain.
 But single-layer perceptrons are also notoriously limited: they can classify
 
\begin_inset Quotes eld
\end_inset

linerarly seperable
\begin_inset Quotes erd
\end_inset

 patterns, but no more.
 There is no doubt that language is far more complex than that.
 There is no reason to believe that table-driven WSD could ever function
 more accurately than a single-layer perceptron: its strength is speed,
 and a certain degree of 
\begin_inset Quotes eld
\end_inset

biological naturalness
\begin_inset Quotes erd
\end_inset

.
 One does not expect exquisite accuracy.
 In the connectionist philosophy, greater sophistication requires that the
 output of one layer be fed to the input of another: thus, for example,
 table-driven WSD might provide a-priori weighted word-sense suggestions
 for other, more refined semantic algorithms.
\end_layout

\begin_layout Standard
The mechanism used to construct the lexicon is also fairly connectionist
 in nature.
 The Mihalcea all-words WSD graph algorithm resembles a Markov chain, solved
 using the PageRank algorithm, and is thus fairly explicitly connectionist
 in and of itself.
 The Link-Grammar parser itself is currently implemented using a computational,
 not connectionist algorithm.
 However, the author beleives that it should be possible to re-implement
 the parser using the Viterbi algorithm.
 This could provide a significant speedup, especially for long sentences,
 as well as making the parser more biologically natural: the Viterbi algorithm
 is explicitly connectionist, maintaining only a finite history, with a
 finite number of connections to recent input (i.e.
 enouraging and maintaining mosly just short-distance connections between
 words).
\end_layout

\begin_layout Standard
Breif results summary ...
 
\end_layout

\begin_layout Standard
Outline of paper....
\end_layout

\begin_layout Standard
use 
\begin_inset Quotes eld
\end_inset

semantic role labelling
\begin_inset Quotes erd
\end_inset

 in a few more paras.
\end_layout

\begin_layout Standard
-- xxx also make the following important point: the semeval code (mihalcea
 1994) indicates that the 
\begin_inset Quotes eld
\end_inset

most gfrequent sense
\begin_inset Quotes erd
\end_inset

 -- i.e.
 MFS provides a very strong signal.
 So the work here is all about how disjuncts modify the MFS signal! 
\end_layout

\begin_layout Section
Previous Work
\end_layout

\begin_layout Standard
Perhaps the most developed of linguistic theories that attempt to describe
 how syntax gives rise to semantics is Igor MelâÄuk's 
\begin_inset Quotes eld
\end_inset

Meaning-Text Theory
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Steele1990,Mel'cuk1987"

\end_inset

.
 The theory describes a network of relationships between semantic lexemes
 and a set of functions and proceedures that relate this network to correspondin
g grammatically correct utterances.
 In particular, the theory attempts to explain why a 
\begin_inset Quotes eld
\end_inset

phraseme
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

set phrase
\begin_inset Quotes erd
\end_inset

, such as the phrase 
\begin_inset Quotes eld
\end_inset


\emph on
to give X a look
\emph default

\begin_inset Quotes erd
\end_inset

, constricts the kind of things that can occupy the slot 
\emph on
X
\emph default
.
 In this example, 
\emph on
X
\emph default
 typically names a process, a product, an idea or proposal.
 A key observation in MTT is that, semantically, 
\emph on
X
\emph default
 cannot be just 
\begin_inset Quotes eld
\end_inset

any old noun
\begin_inset Quotes erd
\end_inset

; even though pure syntactic analysis would allow this.
 To fill this slot in a semantically meaningful way, a noun would have to
 name or denote something worth pondering, reviewing, looking over.
 One of the acheivements of the theory is to precisely specify how such
 a restriction comes about and how it can be maintained in a computational
 way.
 
\end_layout

\begin_layout Standard
The structure of an MTT 
\begin_inset Quotes erd
\end_inset

Explanatory Combinatory Dictionary
\begin_inset Quotes erd
\end_inset

 is quite different than an ordinary dictinary: it gives precise and detailed
 instructions on how to convert lexical meaning into syntactic expression.
 In this sense, it correlates syntax and semantics.
 By contrast, the work described in this paper shows how to automatically
 create a short-cut or abridged form of such an ECD: for any given word-sense,
 one has a table of allowed syntactic structures in which that word sense
 has been observed (and a frequency count of how often that word-sense was
 used in this particular way).
 The MTT ECD is hand-constructed by linguists after hard, patient work.
 The approach described here obtains relationships automatically, with an
 unsupervised learning algorithm.
 
\end_layout

\begin_layout Standard
There are other approaches for the unsupervised learning of syntactic/semantic
 correlations.
 These include Dekang Lin's work on automatic synonym discovery
\begin_inset CommandInset citation
LatexCommand cite
key "Lin1998,Lin2001"

\end_inset

, the work of Domingos and others on applying Markov Logic Networks (MLN)
\begin_inset CommandInset citation
LatexCommand cite
key "Domingos2006"

\end_inset

 to semantics
\begin_inset CommandInset citation
LatexCommand cite
key "Poon2009,Meza-Ruiz2009"

\end_inset

.
 Each of these approaches also have one strength that the current approach
 does not: they potentially discover much narrower classes of words that
 can appear in juxtaposition to each other.
 That is, the current Link Grammar word classes are, for the most part,
 broad, syntactic categories, and not narrow, semantic classes.
 So, for example, the Link Grammar link 
\emph on
Os+
\emph default
, which is short-hand for the statement 
\begin_inset Quotes eld
\end_inset


\emph on
this word takes a singular direct object on the right
\emph default

\begin_inset Quotes erd
\end_inset

, places no particular restriction on what that object might be -- it can
 be any noun.
 This is in contrast to the 
\begin_inset Quotes eld
\end_inset


\emph on
give X a look
\emph default

\begin_inset Quotes erd
\end_inset

 example, where X is syntactically free, but semantically constrained.
\end_layout

\begin_layout Standard
Lin describes how to automatically build a thesaurus in 
\begin_inset CommandInset citation
LatexCommand cite
key "Lin1998"

\end_inset

, by examing the frequency with which words are used in similar dependency
 relations.
 Specifically, he creates a dependency parse, having relations of the form
 
\begin_inset Formula $r(w_{1},w_{2})$
\end_inset

 with 
\begin_inset Formula $r$
\end_inset

 a relation (such as 
\emph on
subj
\emph default
, 
\emph on
obj
\emph default
, 
\emph on
etc
\emph default
.), and 
\begin_inset Formula $w_{1}$
\end_inset

 and 
\begin_inset Formula $w_{2}$
\end_inset

 are words.
 By examing co-occurance statistics for such relations in a large corpus,
 he provides a number of similarity measures which are able to successfully
 identify synonymous words.
 This result is quite remarkable.
 It can be criticized in two ways: by collecting statistics on just individual
 relations, it does not look at the broader syntactic context of a word.
 That is, for example, the meaning of a word may depend on its having both
 a direct object, and a prepositional relation, both within the same sentence.
 This is partly corrected in a later work
\begin_inset CommandInset citation
LatexCommand cite
key "Lin2001"

\end_inset

, which groups together dependency paths to discover synonymmous phrases.
 A similar solution is explored in the Markov logic work described below.
 Either approach provides for the automatic discovery of significant parts
 of an entry of the ECD dictionary of MTT theory.
 A second criticism is that the work does not suggest how to discover that
 a given word may have multiple meanings: it does not specify how to group
 synonyms into synonym sets.
 As such, there is no direct way to extend this work to perform sense labelling
 on individual words in a sentence: there is no way to look at a word in
 a sentence and discover the intended lexeme.
\end_layout

\begin_layout Standard
A global approach to automatic synonymous phrase discovery is offered by
 applying the theory of Markov logic networks to the extraction of semantic
 content from a dependency parse
\begin_inset CommandInset citation
LatexCommand cite
key "Poon2009"

\end_inset

.
 The approach used extracts, tabulates and clusters sentence patterns according
 to common substructures, thus automatically discovering synonymous expressions
 by performing a global sentence analysis.
 In certain ways, it can be thought of as a generalization of Lin's DIRT
\begin_inset CommandInset citation
LatexCommand cite
key "Lin2001"

\end_inset

.
 The system is remarkable in that it is completely unsupervised (does not
 require a training corpus), and requires very few 
\emph on
a priori 
\emph default
built in rules.
 Although it significantly outperforms other question answering systems,
 this result can also be taken as a critique: it does not explicitly provide
 markup or tagged output; but rather performs question answering by pattern
 matching.
\end_layout

\begin_layout Standard
A contrasting application of Markov logic nets is that of Meza-Ruiz, 
\emph on
et al
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "Meza-Ruiz2009"

\end_inset

.
 This system uses MLN to simultaneously provide a variety of tags, and,
 in particular, to identify the head-word of a sentence, disambiguate the
 semantic frame that it participates in, and provide a semantic role tag
 (essentially, a word-sense) for the head-word.
 Unfortunately, the system uses a large number of hand-crafted relations,
 frames and rules to specify the logic network.
 However, once trained, the weighted network can be quickly evaluated to
 obtain tags for any given sentence, typically in fractions of a second
 for contemporary computers.
\end_layout

\begin_layout Standard
It is an interesting exercise to contrast the above-descibed global sentence
 analysis systems to the current work in dependency parsing, such as minimum
 spanning tree approaches
\begin_inset CommandInset citation
LatexCommand cite
key "McDonald2006"

\end_inset

 or greedy state machines
\begin_inset CommandInset citation
LatexCommand cite
key "Nivre2006"

\end_inset

, where global structure is essentially ignored, except to constrain the
 parse to include all of the words in a sentence.
 In essence, syntax is a local constraint on word juxtapositions; semantics
 is a global anlysis of word relationships.
 This fits well into the theoretical framework of Meaning-Text Theory; that
 the current work lies in a middle ground is perhaps no accident.
\end_layout

\begin_layout Section
Methodology
\end_layout

\begin_layout Standard
This section provides a breif review of the ingredients for creating the
 table of POS-sense tags: the Link Grammar parser, the Mihalcea WSD algorithm.
 
\end_layout

\begin_layout Standard
The theory of Link Grammar
\begin_inset CommandInset citation
LatexCommand cite
key "Sleator1991,Sleator1993"

\end_inset

 is built upon the concept that allowed 
\begin_inset Quotes eld
\end_inset

connections
\begin_inset Quotes erd
\end_inset

 between words are determined by a set of 
\begin_inset Quotes eld
\end_inset

connectors
\begin_inset Quotes erd
\end_inset

 attached to the words.
 Words with connectors on them can be thought of being like puzzle pieces:
 they can only be correctly assembled when mating pieces match.
 The link-grammar lexicon contains lists of words together with the connectors
 that they carry.
 Consider then the sentence 
\begin_inset Quotes eld
\end_inset


\emph on
Mary loves beer
\emph default

\begin_inset Quotes erd
\end_inset

.
 The Link Grammar dictionary might contain (for example) entires such as
 these:
\end_layout

\begin_layout Quotation

\family typewriter
beer: O-
\end_layout

\begin_layout Quotation

\family typewriter
loves: S- & O+
\end_layout

\begin_layout Quotation

\family typewriter
Mary: S+
\end_layout

\begin_layout Standard
Here, 
\family typewriter
O-
\family default
 and 
\family typewriter
S+
\family default
 are 
\begin_inset Quotes eld
\end_inset

connectors
\begin_inset Quotes erd
\end_inset

, and 
\family typewriter
S
\family default
 and 
\family typewriter
O
\family default
 are the 
\begin_inset Quotes eld
\end_inset

link types
\begin_inset Quotes erd
\end_inset

 of the connectors.
 The above is simplified for illustration: in practice, words can carry
 a large variety of different connectors.
 The example sentence, as parsed by the Link Grammar parser, it is rendered
 as 
\end_layout

\begin_layout Quotation

\family typewriter
+--S--+--O--+ 
\end_layout

\begin_layout Quotation

\family typewriter
| 
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 |
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 | 
\end_layout

\begin_layout Quotation

\family typewriter
Mary  loves  beer  
\end_layout

\begin_layout Standard
This parse is obtained by noting that 
\family typewriter
S
\family default
 connectors can connect only to other 
\family typewriter
S
\family default
 connectors, and, specifically 
\family typewriter
+
\family default
 denotes a connection to the right, and 
\family typewriter
-
\family default
 a connection to the left.
 Thus, 
\family typewriter
S+
\family default
 can only connect to 
\family typewriter
S-
\family default
 to form an 
\family typewriter
S
\family default
 link; likewise for 
\family typewriter
O+
\family default
 and 
\family typewriter
O-
\family default
.
 An expression such as 
\family typewriter
S+ & O-
\family default
 will be called a 
\begin_inset Quotes eld
\end_inset

disjunct
\begin_inset Quotes erd
\end_inset

 in the remainder of the text; this is because such an expression is usually
 disjoined with many others that a word can have.
 The expression 
\family typewriter
S+ & O-
\family default
 implicitly encodes a verb-like connection pattern for the word 
\begin_inset Quotes eld
\end_inset


\emph on
loves
\emph default

\begin_inset Quotes erd
\end_inset

 -- it demands a subject on the left, and an object on the right.
\end_layout

\begin_layout Standard
The actual Link Grammar dictionary is only slightly more complicated than
 what the above suggests.
 In practice, there are over 100 different link types.
 Many link-types also have subtypes, which are optionally matched using
 a form of wild-card matching.
 Some connectors, such as those to modifiers, are optional.
 Thus, a realistic example from the current version of the parser:
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\family typewriter
\size small
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

+------------Ou------------+
\end_layout

\begin_layout Standard

\family typewriter
\size small
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

+-----Ss----+ 
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

+--------A---------+ 
\end_layout

\begin_layout Standard

\family typewriter
\size small
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

|
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 +--Em--+ 
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

+-Xc-+ 
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

+----A---+ 
\end_layout

\begin_layout Standard

\family typewriter
\size small
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

|
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

|
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

 | 
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

| 
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

|
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

| 
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

| 
\end_layout

\begin_layout Standard

\family typewriter
\size small
Mary really loves.v dark.a , heavy.a beer.n 
\family default
\size default

\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Here, the 
\begin_inset Quotes eld
\end_inset

subscripts
\begin_inset Quotes erd
\end_inset

 
\family typewriter
.f
\family default
, 
\family typewriter
.v
\family default
, 
\family typewriter
.a
\family default
, 
\family typewriter
.n
\family default
 are non-functional tags that help simplify the organization of the dictionary,
 although they also hint at the word function: 
\family typewriter
.v
\family default
 for verbs, 
\family typewriter
.n
\family default
 for nouns, 
\emph on
etc
\emph default
.
 The link type 
\family typewriter
Ss
\family default
 indicates that the subject is singular, and 
\family typewriter
Ou
\family default
 that the object is uncountable.
 The dictionary entry 
\family typewriter
loves.v
\family default
 consists of 40 different disjuncts, some of which are:
\end_layout

\begin_layout Quotation

\family typewriter
({@E-} & Ss- & O+) or
\end_layout

\begin_layout Quotation

\family typewriter
({@E-} & Ss- & TO+) or
\end_layout

\begin_layout Quotation

\family typewriter
({@E-} & Ss- & Pg+)
\end_layout

\begin_layout Standard
Here, 
\family typewriter
{@E-}
\family default
 indicates that 
\begin_inset Quotes eld
\end_inset


\emph on
loves
\emph default

\begin_inset Quotes erd
\end_inset

 can take one or more optional emphasis modifiers, while 
\family typewriter
TO+
\family default
 would link to the word 
\begin_inset Quotes eld
\end_inset


\emph on
to
\emph default

\begin_inset Quotes erd
\end_inset

 (
\begin_inset Quotes eld
\end_inset


\emph on
Mary loves to dance
\emph default

\begin_inset Quotes erd
\end_inset

), and 
\family typewriter
Pg+
\family default
 would link to gerunds (
\begin_inset Quotes eld
\end_inset


\emph on
Mary loves dancing
\emph default

\begin_inset Quotes erd
\end_inset

).
 The disjunction 
\family typewriter
or
\family default
 indicates that only one of these disjuncts will be used in a parse.
 
\end_layout

\begin_layout Standard
Upon completion of parsing, we see that the word 
\begin_inset Quotes eld
\end_inset


\emph on
loves
\emph default

\begin_inset Quotes erd
\end_inset

 was de facto assigned only one disjunct: namely 
\family typewriter
Em- Ss- Ou+
\family default
.
 Here, we drop the ampersand, for brevity; it is implicit.
 This disjunct is is what will play the role of the 
\begin_inset Quotes eld
\end_inset

fine-grained part of speech
\begin_inset Quotes erd
\end_inset

 in the statistical tabulations of parsed text.
\end_layout

\begin_layout Standard
Word-sense tagging was performed by a simple implementation of the Mihalcea
 WSD algorithm
\begin_inset CommandInset citation
LatexCommand cite
key "Mihalcea2005,Mihalcea2007"

\end_inset

.
 In this implementation, every word in a sentence is tagged with all possible
 word-senses appropriate for the assigned coarse part-of-speech, taken from
 the WordNet 3.0 dictionary (WordNet defines only four parts of speech: noun,
 verb, adjective and adverb).
 A clique graph is created, by taking each sense to be a vertex, and with
 edges running between all possible sense pairs.
 The edges are assigned numeric weights, based on the similarity of the
 word-senses at each end of the edge.
 Word-sense similarity was computed using the perl Sense::Similarity package
 [xxx need ref], and using the xx similarity measure between verbs, and
 the xx measure between nouns, as suggested in 
\begin_inset CommandInset citation
LatexCommand cite
key "Mihalcea2007"

\end_inset

 to obtain the best scores.
 Because these two similarity measures run over a different set of numeric
 ranges, they must be rescaled to be directly comparable.
 The weights were renormalized so as to xxx.
 The clique can be thought of as a matrix, which can be renormalized to
 define a Markov chain.
 One common way to find an approximate solution to such a Markov chain is
 the employ the PageRank algorithm.
 The result of doing so is a vector of probabilities, assigning a score
 to each possible word sense.
 The algorithm works due to a simple observation: word-senses used in a
 sentence tend to be closely related; the use of a Markov chain spreads
 around this notion of 
\begin_inset Quotes eld
\end_inset

relatedness
\begin_inset Quotes erd
\end_inset

 in an equitable manner.
 The end result is a ranked, weighted list of WordNet 3.0 senses for each
 word.
 Taken together with the disjunct tag obtained from parsing the sentence,
 one can now tabulate statistics.
\end_layout

\begin_layout Standard
Tabulation was performed by simple counting, accumulating results into an
 SQL database storing (word, word-sense, disjunct) triples, weighting the
 count by parse confidence and by the assigned word-sense confidence.
 The Link Grammar parser will produce multiple parses for a sentence when
 it finds the grammatical structure of the sentence to be ambiguous.
 It will rank these parses according to a confidence measure considting
 of four integers: linkage-length, disjunct-cost, and-cost, unsed-word cost.
 The linkage-length simply counts the total length of the links in a parse.
 The disjunct-cost is incurred when a 
\begin_inset Quotes eld
\end_inset

costly
\begin_inset Quotes erd
\end_inset

 disjunct is used to obtain a parse.
 The and-cost refers to imbalanced branches of sentences in conjunctions;
 unused words can occur when the parser cannot comprehend a sentence.
 These four numeric scores are combined into a single floating-point parse-ranki
ng score for the sentence with the 
\emph on
ad-hoc
\emph default
 formula:
\begin_inset Formula \begin{eqnarray*}
\mathrm{SCORE}= & \exp & -(0.012*\mathrm{LEN}+0.06*\mathrm{DIS}+\\
 &  & \quad\quad0.2*\mathrm{AND}+0.4*\mathrm{UNUSED})\end{eqnarray*}

\end_inset

This formula always gives a value of less than 1.0; it typically assigns
 scores in the range of 0.9 for the confident parses of shorter sentences,
 and can give scores as low as 0.1 or less for parses that were problematic.
 If there was more than one parse for a sentence, all of the highest-ranked
 parses were considered, up to a total of four.
 For each alternative parse, this parse score was used as a multiplier to
 weight the word-sense score.
 These weighted scores were accumulated into the (word, word-sense, disjunct)
 triples table.
\end_layout

\begin_layout Standard
Notice several ad-hoc assumptions -- these adhocs should be explored:
\end_layout

\begin_layout Standard
-- parse ranking formula
\end_layout

\begin_layout Standard
-- re-ranking of word-sense scores.
\end_layout

\begin_layout Section
Corpus
\end_layout

\begin_layout Standard
IMPORTANT: READ THIS NOTE: 
\begin_inset Note Note
status open

\begin_layout Plain Layout
The overall article/word count below is wrong.
 Of the 41K articles mentioned, not all were sense-tagged.
 I am no longer sure what fraction really was sense-tagged.
 Other numbers reported below seem odd or inconsistent across the different
 database sets.
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
The corpus that was analyzed for this work consists of about 41 thousand
 (41K) articles starting with the letters A-E from a May 2008 dump of the
 English-language Wikipedia.
 On average, there were 22 sentences per article, 22 words per sentence,
 and 2.8 distinct parses per sentence.
 Including multiplicities for the multiple parses, a total of 58 million
 words were observed.
 A total of 230K distinct words were observed, of which 117K were observed
 at least four times.
 This number is large for several reasons: the English language Wikipedia
 includes many foreign words, as well as a large number of names of entities
 (people, places, 
\emph on
etc
\emph default
.).
 In addition, the parser identifies numbers (times, dates, measurements)
 as unique words, thus further inflating the raw word count.
 By comparison, the Link Grammar parse dictionaries contain 76K words, counting
 inflected forms (plurals, tenses, 
\emph on
etc
\emph default
.).
 After lemmatization, which is required in order for a word to be found
 in WordNet, only 15K distinct word lemmas were observed.
 By comparison, WordNet contains 155K entries, of which 117K are nouns,
 many of these being entity names or noun phrases.
\end_layout

\begin_layout Standard
These words were tagged with 46.5K unique, distinct disjuncts.
 These wre composed of 385 unique link types, not counting idiom links.
 These 385 links are subtypes of the 89 primary link types; as explained
 above, subtypes are typically used to enforce number or tense agreement,
 or other types of agreement restrictions.
 Notable is tht Link Grammar defines 103 different major link types; not
 all link types were used in this corpus! The unused types deal primarily
 with questions, some fairly idiomaitc comparative usages, and certain types
 of numeric expressions involving fractions.
 Perhaps this is not surising: one doesn't expect questions or judgemental
 statements in Wikipedia articles.
 There were 45 idiom links observed, again a surprise, as the Link Grammar
 dictionaries contain considerably more idiomatic phrases.
 Idiom links are unique, automatically-generated link types to used to connect
 the words in an idiomatic phrase; they have no particular significance,
 other than to maintain a consistent approach to the implementation of the
 parser.
 
\end_layout

\begin_layout Standard
A disjunct is then some combination of these 385+45=430 different link types,
 together with a +/- direction indicator on each, to form a connector.
 Disjuncts may contain repeated connectors; thus, for example, if a noun
 had appeared with two adjective modifiers to its left, it will include
 
\begin_inset Quotes eld
\end_inset


\family typewriter
A- A-
\family default

\begin_inset Quotes erd
\end_inset

 in its disjunct to signify these two connections.
 So for example, 
\begin_inset Quotes eld
\end_inset


\family typewriter
A- A- Ss+
\family default

\begin_inset Quotes erd
\end_inset

 indicates a noun with two adjectival modifiers, used as the subject of
 a sentence.
 Ignoring disjuncts containing idiom connectors, a total of 46.1K unique
 disjuncts were observed -- a tad less than before, but indicating that
 idiomatic phrases make a minimal impact on the total number of possible
 word connections.
 In total, 227K unique word-disjunct pairs were observed; however, many
 of these were observed only infrequently; only 56.6K were observed at least
 five times.
\end_layout

\begin_layout Standard
The words were tagged with 19.7K unique different word senses.
 This is only a small fraction of the 207K senses contained in WordNet 3.0.
 Curiously, there were fewer senses identified and used than there were
 different grammatical usages (
\emph on
i.e
\emph default
.
 disjuncts).
 There were a total of 568K (word,disjunct,sense) triples observed.
 Again, many of these were seen very infrequently: only 76K triples were
 observed more than five times.
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
XX Need intro?
\end_layout

\begin_layout Standard
A basic premise of this paper is that correlations between syntactic use
 and word-sense exist.
 This section quickly presents a few examples that were discovered that
 sustain this view.
\end_layout

\begin_layout Standard
Given a table of (word, disjunct, sense) triples, with frequency counts,
 there are several quantities of interest.
 Define 
\begin_inset Formula $P(w,d,s)$
\end_inset

 to be the probability of observing the (word, disjunct, sense) triple 
\begin_inset Formula $(w,d,s)$
\end_inset

.
 The conditional probability 
\begin_inset Formula \[
P(s|w,d)=\frac{P(w,d,s)}{P(w,d,*)}\]

\end_inset

gives the probility of seeing the sense 
\begin_inset Formula $s$
\end_inset

 given a fixed 
\begin_inset Formula $w,d$
\end_inset

.
 If this is close to 1, this indicates that a a certain word sense is being
 used almost uniquly/exclusively in a certain syntactic context.
 By contrast, if this is close to zero, this indicates that the given sense
 is almost never used with the disjunct.
\end_layout

\begin_layout Standard
An alternative way of observing relationships between sense and syntax is
 by means of the entropy 
\begin_inset Formula \[
H(w,d)=-\sum_{s}\, P(s|w,d)\;\log_{2}P(s|w,d)\]

\end_inset

For the case where there is only one sense 
\begin_inset Formula $s$
\end_inset

 observed associated with 
\begin_inset Formula $w,d$
\end_inset

, one has 
\begin_inset Formula $P(s|w,d)=1$
\end_inset

 and so 
\begin_inset Formula $H(w,d)=0$
\end_inset

.
 In general, if there are only a small number of senses that are used with
 given disjunct-word pair, or if one sense dominates over all others, then
 the value of 
\begin_inset Formula $H(w,d)$
\end_inset

 will be small.
 Low-entropy disjunct-word pairs indicate a strong association of sense
 and syntax.
\end_layout

\begin_layout Standard
Consider, for example, the verb '
\emph on
suffered
\emph default
', together with the disjunct '
\family typewriter
Ss- Os+ MVp+
\family default
', which indicated that 'suffered' was used with a singular subject, a singular
 direct object and a prepositional modifier.
 This word, disjunct pair was observed 176 times.
 It was tagged with eight different senses (out of 11 possible in WordNet).
 It was tagged wtih the sense 
\emph on
suffer%2:29:01::
\emph default
 in 138 of those uses, or 138/176=78% of the time.
 The total entropy for this triple was 1.274, which is considerably higher
 than the average of 0.777 for the entire dataset.
 The average number of senses observed per 
\begin_inset Formula $(w,d)$
\end_inset

 pair was 2.373, but the median was just 1: of the 227K total observed 
\begin_inset Formula $(w,d)$
\end_inset

 pairs, 89K, or 39%, were assigned just one sense.
 
\end_layout

\begin_layout Standard
The distribution of sense assignments is shown in figure
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Distribution-of-word-sense"

\end_inset

.
 This figure just shows a histogram or bincount: how many of the 
\begin_inset Formula $(w,d)$
\end_inset

 pairs were assigned a given number of senses.
 It is not clear what mathematical formula might define this distribution.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Distribution of word-sense assignments
\begin_inset CommandInset label
LatexCommand label
name "fig:Distribution-of-word-sense"

\end_inset


\end_layout

\end_inset


\begin_inset Graphics
	filename sense-bincount.ps
	width 100col%

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The distribution of entropy vs.
 
\begin_inset Formula $(w,d)$
\end_inset

 is shown in the figure below.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Distribution of Entropy
\begin_inset CommandInset label
LatexCommand label
name "fig:Distribution-of-Entropy"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename entropy-bincount.ps
	width 100col%

\end_inset

 
\end_layout

\begin_layout Plain Layout
Note the prominent peaks in the graph above.
 This is due to a bug in the tagging algo: where there were islands in the
 page rank algo (i.e.
 regions disconnected from the main graph) these islands weren't properly
 tagged, and instead, senses were given equal probability assignments: i.e.
 1/2, 1/2, or 1/3,1/3,1/3 etc.
 which gives rise to large, prominent peaks in the entropy graph.
 The sharp drop to one side, and decay to the other, is easily explained:
 in many of these cases, there are small tie-breaker votes from other sources.
 These tie-brakers skew the totals to be e.g.
 51%-49%, which has a slightly lower entropy than 50-50.
 Argh !!!!!
\end_layout

\begin_layout Plain Layout
The above suggests that the entire dataset should be completely redone.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This was observe
\end_layout

\begin_layout Standard
-- what is the total entropy? what would max ent be?
\end_layout

\begin_layout Standard
-- how many have low ent?
\end_layout

\begin_layout Standard
--what about results by POS category?? e.g.
 does this work better for verbs, or for nouns? wahat about adj/adv? (i.e.
 accuracy?)
\end_layout

\begin_layout Standard
Total count is 593003.630924276 for 537985 items Done updating the probs
 Done updating the entropy of 226679 disjuncts Avg sense cnt=2.37333409799761
 avg entropy=0.777430272279844;
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement htbp
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\family sans
A single column figure goes here
\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Captions go 
\emph on
under
\emph default
 the figure
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float table
placement htbp
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Table captions go 
\emph on
above
\emph default
 the table
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="2">
<features>
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
delete
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
this
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
example
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
table
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
One of the primary applications of the WSD effort is to provide the Link
 Grammar parser with a set of dictionaries theat provide parse-ranking,
 and provide parse-time syntactical word-sense disambiguation.
 See the file renorm-stats/README for instructions on how to prepare these
 files.
 
\end_layout

\begin_layout Standard
other conclusions
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "/home/linas/src/fractal/paper/lang"
options "plain"

\end_inset


\end_layout

\begin_layout Biography
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

Linas Vepstas
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 All about me.
 I do many things, like NLP, math, programming,etc.
\end_layout

\begin_layout --Separator--

\end_layout

\end_body
\end_document
